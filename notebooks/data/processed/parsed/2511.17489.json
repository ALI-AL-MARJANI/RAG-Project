{
  "id": "2511.17489",
  "content": "5\n2\n0\n2\n\nv\no\nN\n1\n2\n\n]\n\nG\nL\n.\ns\nc\n[\n\n1\nv\n9\n8\n4\n7\n1\n.\n1\n1\n5\n2\n:\nv\ni\nX\nr\na\n\n1\u201330\n\nHarnessing Data from Clustered LQR Systems: Personalized and\nCollaborative Policy Optimization\n\nVinay Kanakeri\nDepartment of Electrical and Computer Engineering, North Carolina State University\n\nVKANAKE@NCSU.EDU\n\nShivam Bajaj\nThe Elmore Family School of Electrical and Computer Engineering, Purdue University\n\nBAJAJ41@PURDUE.EDU\n\nAshwin Verma\nThe Elmore Family School of Electrical and Computer Engineering, Purdue University\n\nVERMA240@PURDUE.EDU\n\nVijay Gupta\nThe Elmore Family School of Electrical and Computer Engineering, Purdue University\n\nGUPTA869@PURDUE.EDU\n\nAritra Mitra\nDepartment of Electrical and Computer Engineering, North Carolina State University\n\nAMITRA2@NCSU.EDU\n\nAbstract\nIt is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it\nhas been proposed that the learning algorithm utilize data from \u2018approximately similar\u2019 processes.\nHowever, since the process models are unknown, identifying which other processes are similar poses\na challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic\nRegulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding\nto a copy of a linear process to be controlled. The agents\u2019 local processes can be partitioned into\nclusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination\nand zeroth-order policy optimization, we propose a new algorithm that performs simultaneous\nclustering and learning to output a personalized policy (controller) for each cluster. Under a suitable\nnotion of cluster separation that captures differences in closed-loop performance across systems,\nwe prove that our approach guarantees correct clustering with high probability. Furthermore, we\nshow that the sub-optimality gap of the policy learned for each cluster scales inversely with the\nsize of the cluster, with no additional bias, unlike in prior works on collaborative learning-based\ncontrol. Our work is the first to reveal how clustering can be used in data-driven control to learn\npersonalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality\ndue to inclusion of data from dissimilar processes. From a distributed implementation perspective,\nour method is attractive as it incurs only a mild logarithmic communication overhead.\nKeywords: Policy gradients for LQR; Collaborative Learning; Transfer/Multi-Task Learning.\n\n1. Introduction\n\nThe last decade or so has seen a surge of interest in model-free data-driven control (Hu et al., 2023),\nwhere control laws (policies) are learned directly from data, bypassing the need to estimate the\nsystem model as an intermediate step. Although such a framework is promising, it relies on the\navailability of adequate data to learn high-precision policies. Unfortunately, however, data from\nphysical processes (such as real-world robotic environments) could be scarce and/or difficult to\ncollect. Drawing inspiration from popular paradigms such as federated and meta-learning, some\nrecent papers (Zhang et al., 2023; Wang et al., 2023a; Toso et al., 2024) have attempted to mitigate\nthis challenge by exploring the idea of combining information generated by multiple environments,\nwhere each environment represents a dynamical system with an associated cost performance metric\n\n\u00a9 V. Kanakeri, S. Bajaj, A. Verma, V. Gupta & A. Mitra.\n\n \n \n \n \n \n \n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nthat captures a task or a goal. The unifying theme in such papers is to learn a single common policy\nthat performs well across all environments by minimizing an average-cost performance metric. When\nenvironments differ considerably in their tasks, such a single common policy might incur highly\nsub-optimal performance on any given environment. More fundamentally, when environments differ\nin dynamics, even the existence of a common stabilizing policy is unclear and difficult to verify in the\nabsence of models. Departing from the approach of learning a single common policy, in this paper,\nwe ask: (When) is it possible to learn personalized policies in a sample-efficient way by leveraging\ndata generated by potentially non-identical dynamical processes?\n\nTo formalize our study, we consider a scenario involving multiple agents that can be partitioned\ninto distinct clusters. We assume that all agents within a given cluster interact with the same physical\nenvironment modeled as a linear time-variant (LTI) system with unknown dynamics; furthermore,\nall agents within a cluster share the same quadratic cost function. However, the dynamics and cost\nfunctions across clusters can be arbitrarily different. Thus, our setting captures both similarities\nand differences in dynamics and tasks. As is common in collaborative and federated learning, we\nallow agents to exchange information via a central aggregator. Concretely, our problem of interest\nis to learn a personalized policy for each cluster that enjoys the benefits of collaboration, i.e., we\nwish to show that such a policy can be learned faster (relative to a single-agent setting) by using the\ncollective samples available within the cluster. However, this is challenging, as we explain below.\n\nChallenges. To make our setting realistic, we assume that the cluster structure is unknown a\npriori. Since the system models associated with the clusters are also unknown, it becomes difficult\nto decide how information should be exchanged between agents. In particular, care needs to be\ntaken to avoid misclustering, since transfer of information across clusters with arbitrarily different\nLTI systems can lead to the learning of destabilizing policies; thus, in our setting, more data can\npotentially hurt if not used judiciously. Additional subtleties arise as the agents in our setting access\nonly noisy zeroth-order information for both clustering and learning policies; we discuss them in\nSections 2 and 3. In light of these challenges, the main contributions of this paper are as follows.\n\u2022 Problem Formulation. While clustering has been explored in federated learning (FL) for\nstatic supervised learning tasks (Ghosh et al., 2020), our work provides the first principled study of\nclustering in the context of model-free data-driven control, and shows how such a formalism can\nenable learning personalized policies in a sample-efficient manner. As part of our formulation, we\nidentify a dissimilarity metric \u2206 (see (3)) that captures differences in optimal costs between clusters.\nOur results reveal that a larger value of \u2206 leads to a faster separation of clusters.\n\n\u2022 Novel Algorithm. The primary contribution of this paper is the development of a novel\nmodel-free Personalized and Collaborative Policy Optimization (PO) algorithm (Algorithm 1) called\nPCPO that combines ideas from sequential elimination in multi-armed bandits (Even-Dar et al.,\n2006) and policy gradient algorithms in reinforcement learning (RL) (Agarwal et al., 2021). The lack\nof prior knowledge of the cluster-separation gap \u2206 motivates the need for a sequential elimination\nstrategy to identify the clusters. Moreover, since it is non-trivial to decide when to stop clustering and\nstart collaborating, we propose an epoch-based approach that involves clustering and collaboration\nin every epoch by requiring agents to maintain two separate sequences of policies: local policies\nused purely for sequential clustering and global policies for collaboration. Another key feature of\nour algorithm is that it only incurs a mild communication cost that scales logarithmically with the\nnumber of agents and samples, making it particularly appealing for distributed implementation.\n\n\u2022 Collaborative Gains. In Theorem 1, we prove that with high probability, our approach leads to\ncorrect clustering, despite the absence of prior knowledge of models, cluster structure, and separation\n\n2\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\ngap \u2206. This result also reveals that more heterogeneity can actually aid the learning process in that\na larger \u2206 incurs fewer noisy function evaluations for correct clustering. Building on Theorem 1,\nour main result in Theorem 2 proves that by using PCPO, each agent can learn a near-optimal\npersonalized policy for its own system with a sub-optimality gap that scales inversely with the\nnumber of agents in its cluster. In other words, PCPO prevents negative transfer of information across\nclusters, while ensuring sample-complexity reductions via collaboration within each cluster. To our\nknowledge, this is the first result to show how data from heterogeneous dynamical systems admitting\na cluster structure can be harnessed to expedite the learning of personalized policies.\n\nSince this is a preliminary investigation of clustering in data-driven control, we restrict our\nattention to the canonical Linear Quadratic Regulator (LQR) formalism. That said, we anticipate that\nour general algorithmic template can be used in other supervised or RL problems.\n\nRelated Work. To put our contributions into perspective, we discuss relevant literature below.\n\u2022 Policy Gradient for LQR. We build on the rich set of results on policy gradient methods for\nthe LQR problem (Fazel et al., 2018; Malik et al., 2020; Gravell et al., 2020; Zhang et al., 2021;\nMohammadi et al., 2019, 2020; Hu et al., 2023; Moghaddam et al., 2025). Generalizing these results\nfrom the single system setting to our clustered multi-system formulation introduces various nuances\nand challenges (outlined in Sections 2 and 3) that we address in this paper.\n\n\u2022 Personalized Federated Learning. We draw inspiration from the work on clustering in\nFL (Ghosh et al., 2020, 2022; Sattler et al., 2020) that aims to learn personalized models for groups of\nagents that are similar in terms of their data distributions. Despite cosmetic similarities, the specifics\nof our setting differ significantly in that we focus on control of dynamical systems where stability\nplays a crucial role; no such stability concerns arise in the FL papers above on supervised learning.\n\u2022 Collaborative System Identification. Our paper is related to a growing body of work that seeks\nto leverage data from multiple dynamical systems to achieve statistical gains in estimation accuracy.\nIn this context, several papers (Wang et al., 2023b; Toso et al., 2023; Chen et al., 2023; Modi et al.,\n2024; Rui and Dahleh, 2025; Tupe et al., 2025; Xin et al., 2025) have explored collaborative system\nidentification by combining trajectory data from multiple systems that share structural similarities.\nIn particular, Toso et al. (2023) and Rui and Dahleh (2025) assume a cluster structure like us. While\nthe above papers focus on using collective data for an open-loop estimation problem, namely system-\nidentification, our work focuses instead on data-efficient closed-loop control by directly learning\npolicies. As such, our notion of heterogeneity captures differences in closed-loop performance across\nclusters as opposed to similarity metrics imposed on open-loop system matrices in the papers above.\n\u2022 Meta, Multi-Task, and Transfer Learning in Control. Under the umbrella framework of\nmeta and transfer learning, various recent papers (Wang et al., 2023a; Toso et al., 2024; Aravind et al.,\n2024; Stamouli et al., 2025) have used PO methods to study how information from multiple LTI\nsystems can be aggregated to learn policies that adapt across similar systems. Our formulation, which\nseeks to find a personalized policy for every system, departs fundamentally from this line of work\nwhich instead aims to learn a common policy for all systems. In this regard, we note that the closely\nrelated papers of Wang et al. (2023a) and Toso et al. (2024) need to assume that all the systems are\nsufficiently similar to admit a common stabilizing set. Even under this restrictive assumption, the\nresults in these papers indicate that the sub-optimality gap exhibits an additive heterogeneity-induced\nbias term that might negate the speedups from collaboration. In contrast, our work does not require\na common stabilizing policy to exist for systems across clusters. Furthermore, our personalization\napproach completely eliminates heterogeneity-induced biases. We also note that our approach incurs\na logarithmic (in agents and samples) communication cost as opposed to the linear cost in Wang\n\n3\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\net al. (2023a). Finally, complementary to our clustering-based approach, ideas from representation\nlearning (Zhang et al., 2023; Guo et al., 2023; Lee et al., 2025) and domain randomization (Fujinami\net al., 2025) have also been recently used to improve data-efficiency in dynamic control tasks.\n\n2. Problem Formulation\n\nWe consider a setting with N agents partitioned into H disjoint clusters {Mj}j\u2208[H]. With each\ncluster j \u2208 [H], we associate a tuple Sj = (Aj, Bj, Qj, Rj), comprising a system matrix Aj \u2208 Rn\u00d7n,\na control input matrix Bj \u2208 Rn\u00d7m, and two positive definite matrices Qj \u2208 Rn\u00d7n, Rj \u2208 Rm\u00d7m that\ndefine the LQR cost function for cluster j. Each agent in cluster j interacts with the same instance\nof the LQR problem specified by Sj, and aims to find a linear policy of the form ut = \u2212Kxt that\nminimizes the following infinite-horizon discounted cost:\n\nCj(K) = E\n\n(cid:34) \u221e\n(cid:88)\n\nt=0\n\n\u03b3t (cid:16)\n\nt Qjxt + u\u22a4\nx\u22a4\n\nt Rjut\n\n(cid:35)\n\n(cid:17)\n\nsubject to xt+1 = Ajxt + Bjut + zt,\n\n(1)\n\nwhere x0 = 0 and xt, ut, and zt are the state, control input (action), and exogenous process noise,\nrespectively, at time t, \u03b3 \u2208 (0, 1) is a discount factor, and K is a control gain matrix. We make the\nstandard assumption that the pair (Aj, Bj) is controllable for every j \u2208 [H]. Following Malik et al.\n(2020), we assume that zt is sampled independently from a distribution D, such that:\n\nE[zt] = 0, E[ztz\u22a4\n\nt ] = I, and \u2225zt\u22252\n\n2 \u2264 B, \u2200t,\n\n(2)\n\nwhere B > 0 is some positive constant. For the LQR problem described in (1), it is well known (Bert-\nsekas, 2015) that the optimal control law is a linear feedback policy of the form ut = \u2212K\u2217\nj xt, where\nK\u2217\nj is the optimal control gain matrix for cluster j. When Sj is known, each agent in Mj can obtain\nK\u2217\nj by solving the discrete-time algebraic Riccati equation (DARE) (Anderson and Moore, 2007).\nHowever, our interest is in the learning scenario where the system matrices {(Aj, Bj)}j\u2208[H] are\nunknown to the agents. Even in this setting, it is known that policy optimization (PO) algorithms that\ntreat the control gain as the optimization variable converge to the optimal policy (Fazel et al., 2018;\nMalik et al., 2020). The implementation of such algorithms relies on noisy trajectory rollouts to\ncompute estimates of policy gradients.1 Specifically, given T independent rollouts from the tuple Sj,\neach agent within Mj can generate a gain \u02c6K such that with high probability, Cj( \u02c6K) \u2212 Cj(K\u2217\nj ) \u2264\n\u02dcO(1/\nT ) (Malik et al., 2020). Our goal is to investigate whether this sample-complexity bound can\nbe improved by leveraging the cluster structure in our problem.\n\n\u221a\n\nTo achieve potential gains in sample-complexity via collaboration, we allow the agents to\ncommunicate via a central server, and make the following assumption that is common in the literature\non collaborative/federated learning (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017).\n\nAssumption 1 The noise processes across agents are statistically independent, i.e., for all i1, i2 \u2208\n[N ] such that i1 \u0338= i2, the noise stochastic processes {z(i1)\n} are mutually independent.\nHere, with a slight overload of notation, we use {z(i)\nt } to denote the noise process for agent i \u2208 [N ].\n\n} and {z(i2)\n\nt\n\nt\n\nAlthough the above assumption suggests that exchange of information between agents can accelerate\nthe learning of an optimal policy, collaboration is complicated by the heterogeneity among clusters,\ndue to the difference in system dynamics (Aj, Bj) and in task objectives (Qj, Rj). To capture such\n\n1. The notion of a rollout will be made precise later in this section.\n\n4\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nheterogeneity across clusters, we take inspiration from the notion of \u201ccluster separation gaps\" in\nsupervised learning (Ghosh et al., 2022; Su et al., 2022; Sattler et al., 2020), and introduce\n\n\u2206 :=\n\nmin\nj1,j2\u2208[H]:j1\u0338=j2\n\n|Cj1(K\u2217\n\nj1) \u2212 Cj2(K\u2217\n\nj2)|.\n\n(3)\n\nWe assume a non-zero separation across clusters, i.e., \u2206 > 0. While one can certainly formu-\nlate alternative notions of heterogeneity, we will show (in Theorem 2) that our metric of cluster-\ndissimilarity, as captured by \u2206 in (3), can be suitably exploited to separate clusters and learn\npersonalized policies for each cluster. In particular, our results reveal that heterogeneity can be\nhelpful: a larger value of \u2206 leads to faster cluster separation using fewer rollouts. With the above\nideas in place, we are now ready to formally state our problem of interest. For each agent i \u2208 [N ],\nlet us use \u03c3(i) to represent the index of the cluster to which it belongs.\n\nProblem 1 (Clustered LQR Problem) Let \u03b4 \u2208 (0, 1) be a given failure probability. Suppose\nevery agent i \u2208 [N ] has access to T independent rollouts from its corresponding system S\u03c3(i).\nDevelop an algorithm that returns { \u02c6Ki}i\u2208[N ] such that with probability at least 1 \u2212 \u03b4, the\nfollowing is true \u2200i \u2208 [N ]:\n\nC\u03c3(i)( \u02c6Ki) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i)) \u2264 \u02dcO\n\n\uf8eb\n\n\uf8ed\n\n1\n\n(cid:113)\n\n|M\u03c3(i)|T\n\n\uf8f6\n\n\uf8f8 .\n\nIn simple words, our goal is to come up with an algorithm that generates a personalized control\npolicy for every agent that benefits from the collective information available within that agent\u2019s\ncluster. This is quite non-trivial due to the following technical challenges.\n\n\u2022 In our setting, the system dynamics and the cluster identities are both unknown a priori. Thus,\n\nour problem requires learning the cluster identities and optimal policies simultaneously.\n\n\u2022 The clustering process is complicated by two main issues. First, the information used for\nclustering is based on noisy function evaluations that are insufficient for estimating the system models,\nruling out system-identification-based approaches in Toso et al. (2023) and Rui and Dahleh (2025).\nThus, we need to develop a model-free clustering algorithm. Second, the minimum separation gap \u2206\nin (3) is assumed to be unknown, ruling out the possibility of simple one-shot clustering approaches.\n\u2022 Unlike supervised learning problems in Ghosh et al. (2020) and Su et al. (2022) where\nmisclustering only introduces a bias due to heterogeneity, the price of misclustering can be more\nsevere in our control setting. In particular, since the system tuples across clusters are allowed to be\narbitrarily different, transfer of information across clusters can lead to destabilizing policies.\n\nIn the next section, we will develop the PCPO algorithm that addresses the above challenges\nand solves Problem 1, while incurring only a logarithmic (with respect to the number of agents and\nrollouts) communication cost. In preparation for the next section, we now define the notion of a\nrollout and a zeroth-order gradient estimator. Given a policy K, a rollout for an agent i \u2208 Mj yields\na noisy sample of the infinite-horizon trajectory cost, defined as:\n\nCj(K; Z (i)) =\n\n\u03b3t (cid:16)\n\n\u221e\n(cid:88)\n\nt=0\n\nt Qjxt + u\u22a4\nx\u22a4\n\nt Rjut\n\n(cid:17)\n\n, where xt+1 = Ajxt + Bjut + zt, ut = \u2212Kxt, (4)\n\n5\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nx0 = 0 and Z (i) = {z(i)\nt }. We will interpret each rollout as a sample. Using such noisy function\nevaluations for a policy K run by an agent i \u2208 Mj, we define the M -minibatched zeroth-order\ngradient estimator with a smoothing radius r, as follows (Fazel et al., 2018; Malik et al., 2020):\n\ngi(K) :=\n\n1\nM\n\nM\n(cid:88)\n\nk=1\n\nCj(K + rUk; Z (i)\nk )\n\n(cid:19)\n\n(cid:18) D\nr\n\nUk,\n\n(5)\n\nwhere D = mn, Uk is drawn independently from a uniform distribution over matrices with unit\nFrobenius norm, and Z (i)\nk are independent copies of Z (i) for all k \u2208 [M ]. In the sequel, for an\nagent i \u2208 Mj, we use the shorthand ZOi(K, M, r) to refer to the M -minibatched zeroth-order\ngradient estimator at policy K with smoothing radius r, as defined in (5). We use the notation c(p,_)\nto denote problem-parameter-dependent constants and provide their expressions in Appendix A of\nKanakeri et al. (2025). We make the standard assumption (Fazel et al., 2018; Malik et al., 2020)\nthat each agent i has access to an initial controller K(0)\nthat lies within its respective stabilizing set:\n{K \u2208 Rm\u00d7n : \u03c1(A\u03c3(i) \u2212 B\u03c3(i)K) < 1}, where \u03c1(X) is the spectral radius of a matrix X \u2208 Rn\u00d7n.\n\ni\n\n3. Description of the Algorithm\n\nIn this section, we present our proposed algorithm, Personalized and Collaborative Policy Opti-\nmization (PCPO) (Algorithm 1), which effectively addresses Problem 1 by carefully accounting for\nits inherent challenges. Since the cluster separation gap \u2206 is unknown, we propose a sequential\nelimination strategy to identify the correct clusters. While the idea of sequential elimination has\nbeen explored in the context of multi-armed bandits (Even-Dar et al., 2006), we show that a similar\napproach can be used to effectively cluster LTI systems that satisfy the heterogeneity metric defined\nin (3). The algorithm proceeds in epochs (indexed by l) of increasing duration, where in each epoch,\neach agent i updates two sequences: a local sequence, {X (l)\ni }l\u22650.\nThe local sequence is updated using the zeroth-order policy optimization algorithm in Fazel et al.\n(2018); Malik et al. (2020), and is used exclusively for clustering the agents. The global sequence is\nupdated by aggregating the gradient estimates from all the agents within an appropriately defined\nneighborhood set. After each epoch, the neighborhood sets are updated based on the concentration\nof the estimated cost around the optimal cost. As the number of rollouts increases across epochs, this\nconcentration becomes tighter, hence pruning out misclustered agents over successive epochs. The\nvarious components of the algorithm are succinctly captured in Figure 1. In Section 4, we show that\nthe neighborhood sets eventually converge to the correct clusters with high probability, after which\nthe global sequence enjoys the collaborative gains without any heterogeneity bias. In what follows,\nwe elaborate on the rationale behind the design of the various components of PCPO.\n\ni }l\u22650, and a global sequence, { \u02c6K(l)\n\nBuilding intuition. Correctly identifying the agents\u2019 clusters is crucial to reap any potential ben-\nefits from collaboration, as collaborating with agents from a different cluster can lead to destabilizing\npolicies. In this regard, the major difficulty arises from the fact that the cluster separation gap \u2206\nin (3) is unknown a priori. To appreciate the associated challenges, as a thought experiment, let us\nconsider a simpler case where \u2206 is known. Under this scenario, each agent can locally run policy\noptimization to obtain a policy in a sufficiently close neighborhood of the optimal policy. Then, each\nagent can evaluate the cost at this policy and ensure that it is concentrated around the optimal cost.\nIf the neighborhood radius, which depends on \u2206, is carefully chosen, it is easy to see that such a\none-shot approach leads to correct clustering. However, when \u2206 is unknown, one-shot clustering\nmay no longer work, motivating the sequential clustering idea in our proposed PCPO Algorithm.\n\n6\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nFigure 1: Illustration of the epoch-based structure of PCPO, where each epoch involves three key\n\nsteps: local policy optimization (PO), cost estimation, and global PO.\n\ni\n\nSequential elimination. In each epoch l of PCPO, for every agent i \u2208 [N ], the server maintains\na neighborhood set N (l)\nas an estimate of the true cluster M\u03c3(i). All such neighborhood sets are\ninitialized from the set of all agents and sequentially pruned over epochs. For pruning, we start with\nan initial estimate of \u2206, denoted by \u22060, and update it by halving its value at the beginning of each\nepoch l to obtain \u2206l (Line 3 of Algo. 1). Our goal is to ensure that for all agents, the estimated cost\nin epoch l is in the \u2206l/4 neighborhood of its optimal cost. We achieve this in a two-step process,\nwhere we first perform local policy optimization to obtain a policy that is \u2206l/8-suboptimal (Line 4).\nThe localPO subroutine performs Ml-minibatched policy optimization for Rl iterations starting\nwith a controller X (l\u22121)\n. In every iteration t \u2208 [Rl], X(i,t), the t-th sub-iterate of localPO, is\nupdated as X(i,t+1) \u2190 X(i,t) \u2212 \u03b7gi(X(i,t)), where gi(X(i,t)) = ZOi(X(i,t), Ml, r(loc)\n). Then, we\nestimate the cost at the policy X (l)\ni obtained from localPO with an error tolerance of \u2206l/8 using\nMl rollouts (Line 5). Having achieved the desired cost-estimation accuracy of \u2206l/4 for all agents,\nwe prune the neighborhood sets according to (7) in Line 13. Eventually, as \u2206l \u2264 \u2206, which happens\nin O(log(\u22060/\u2206)) epochs, correct clustering takes place, as elaborated in the next section.\n\ni\n\nl\n\nLocal and Global Sequences. To motivate the need for maintaining two sequences in PCPO, let\nus again consider the case where \u2206 is known, where it would suffice for the agents to only maintain a\nsingle sequence to run local PO until clustering, as discussed earlier in the one-shot clustering scheme.\nAfter the clusters are identified, the same sequence can be used for collaboration. In our setting,\nhowever, although the neighborhood sets eventually converge to the correct clusters in logarithmic\nnumber of epochs with respect to 1/\u2206, the number of such epochs cannot be determined a priori with-\nout knowledge of \u2206, making it difficult to decide when to initiate collaboration. Furthermore, with a\nsingle sequence, collaborating with misclustered agents can lead to an undesirable scenario where the\nsequence used to cluster is itself contaminated due to misclustering. PCPO carefully navigates this\ndifficulty by maintaining two sequences of policies at each agent. The local sequence, {X (l)\ni }l\u22650, is\nused purely for clustering, and the global sequence, { \u02c6K(l)\ni }l\u22650, is updated by aggregating gradients\nfrom agents within the neighborhood set N (l\u22121)\n\nfrom the previous epoch l \u2212 1; see (6) in Line 9.\n\ni\n\nLogarithmic communication. Since both local and global PO are performed for Rl iterations\nwith Ml rollouts per iteration, the overall sample complexity per epoch is Tl = 2RlMl + Ml, where\nthe additional Ml rollouts are due to the cost estimation step. Each iteration of global PO proceeds as\nfollows. First, for every agent, the server combines the minibatched zeroth-order gradient estimates\nas per (6). Then, the agents update the iterates using the averaged gradient with an appropriately\nchosen but fixed step size \u03b7 (Line 10). Since the above essentially incurs O(Rl) communication steps\n\n7\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nAlgorithm 1 Personalized and Collaborative Policy Optimization (PCPO)\n1: Initialization: \u22060; \u2200i \u2208 [N ], \u02c6K(0)\n2: For l = 1, 2, . . . ,\n\ni \u2190 K(0)\n\ni \u2190 K(0)\n\n, N (0)\n\n, X (0)\n\ni \u2190 [N ].\n\ni\n\ni\n\nAt Each Agent i: \u2206l \u2190 \u2206l\u22121\n2\n\n2l2 , \u03b7 \u2190 c(p,1), Rl \u2190 c(p,2) log\n\n(cid:17)\n\n(cid:16) c(p,3)N\n\u22062\nl\n\nMl \u2190\n\nc(p,4)\n\u22062\nl\n\nlog\n\n(cid:16) 8DN Rl\n\u03b4l\n\n(cid:17)\n\n, \u02dcrl \u2190\n\n(cid:114)\n\nlog\n\n(cid:16) 8DN Rl\n\u03b4l\n\n(cid:17)(cid:19)1/2\n\n, r(loc)\n\nl \u2190 min{c(p,6), \u02dcrl}.\n\n, \u03b4l \u2190 \u03b4\n(cid:18) c(p,5)\u221a\n\nMl\n\nLocal Policy Optimization: X (l)\nCost estimation: \u02c6C\u03c3(i)(X (l)\nInitialize Y (0)\n\ni \u2190 \u02c6K(l\u22121)\nFor k = 0, 1, . . . , Rl \u2212 1\n\ni \u2190 localPO(X (l\u22121)\nj=1 C\u03c3(i)(X (l)\n\ni ) \u2190 1\nMl\nand set r(global)\n(i,l) \u2190 min\n\n(cid:80)Ml\n\n(cid:26)\n\ni\n\ni\n\ni\n\n, Ml, Rl, r(loc)\n, Z (i)\nj ).\n\nl\n\n).\n\n(cid:27)\n\nc(p,6),\n\n\u02dcrl\n|N (l\u22121)\ni\n\n|1/4\n\nAt Each Agent i: Transmit gi(Y (k)\n) to the Server.\nAt Server: Compute and transmit the averaged gradient estimate as follows:\n\n) \u2190 ZOi(Y (k)\n\n, Ml, r(global)\n\n(i,l)\n\ni\n\ni\n\n\u25b7 For collaborative PO\n\nGi \u2190\n\n1\n|N (l\u22121)\ni\n\n|\n\n(cid:88)\n\ngj(Y (k)\nj\n\n).\n\nj\u2208N (l\u22121)\ni\n\n(6)\n\nAt Each Agent i: Y (k+1)\n\ni\n\n\u2190 Y (k)\n\ni \u2212 \u03b7Gi.\n\n\u25b7 Global policy update via collaboration\n\nEnd For\n. Transmit X (l)\nAt Each Agent i: Update \u02c6K(l)\ni\nAt Server: Update the neighborhood set as follows:\n\ni \u2190 Y (Rl)\n\ni\n\n, \u02c6C\u03c3(i)(X (l)\n\ni ) to the server.\n\n\u25b7 Sequential elimination\n\nN (l)\n\ni \u2190 {j \u2208 N (l\u22121)\n\ni\n\n(cid:12)\n\u02c6C\u03c3(j)(X (l)\n(cid:12)\n(cid:12)\n\n(cid:12)\nj ) \u2212 \u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12) \u2264 \u2206l/2}\ni )\n\n:\n\n(7)\n\nIf N (l)\ni\n\n\u0338= N (l\u22121)\ni\n\nfor some i \u2208 [N ]:\nFor all agents i \u2208 [N ], update and transmit \u02c6K(l)\ni\n\n\u25b7 Reinitialization to ensure stability\n\nas follows:\n\n\u02c6K(l)\n\ni \u2190 argmin\n:j\u2208N (l)\n{X (l)\ni }\nj\n\n{ \u02c6C\u03c3(j)(X (l)\n\nj ) : j \u2208 N (l)\n\ni }.\n\n(8)\n\n3:\n\n4:\n\n5:\n\n6:\n\n7:\n\n8:\n\n9:\n\n10:\n\n11:\n\n12:\n\n13:\n\n14:\n\n15:\n\n16: End For\n\nper epoch, the total communication complexity of PCPO is O(Rl \u00d7 Number of Epochs). Based on\nour choice of parameters, both objects in the above product are logarithmic in the number of agents\nN , the gap 1/\u2206, and the number of total rollouts per agent, namely T .\n\nNote on reinitialization. Since the server averages gradients in every epoch based on the\nneighborhood set, agents inevitably collaborate across clusters until correct clustering is achieved.\nThis can lead to destabilizing policies in the global sequence. To mitigate this, at the end of each\nepoch, we reinitialize the global policy sequences for all agents whenever any neighborhood set is\nupdated, as specified in (8). In the next section, we establish in Theorem 1 that the neighborhood\nsets eventually converge to the correct clusters and cease to update. Consequently, reinitialization\n\n8\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nensures that the global sequences of agents within the same cluster evolve identically and achieve\ncollaborative gains once the correct clusters are identified.\n\n4. Main Results\n\nOur main results concern the two key components of the PCPO algorithm: (i) identifying the\ncorrect clusters via sequential elimination, and (ii) performing collaborative policy optimization with\nlogarithmic communication. The following theorem captures the clustering component.\n\nTheorem 1 (Clustering with sequential elimination) Define L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2}.\nGiven a failure probability \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4/2, the following statements\nconcerning the neighborhood sets from the PCPO algorithm hold for every agent i \u2208 [N ]:\n\n1. In every epoch l, we have M\u03c3(i) \u2286 N (l)\ni\n\n.\n\n2. For any epoch l such that l \u2265 L, we have M\u03c3(i) = N (l)\ni\n\n.\n\nDiscussion. The key technical contribution of Theorem 1 lies in showing that the sequential\nelimination strategy successfully lets the neighborhood sets converge to the correct clusters with high\nprobability. In particular, we show that the true clusters are included in the neighborhood sets for all\nagents in each epoch, and there exists an epoch L after which the neighborhood sets have converged\nto the true clusters and remain fixed for all subsequent epochs (l \u2265 L). Later in this section, we\nprovide a proof sketch and discuss how these claims follow from the local policy optimization and\nthe cost estimation step, relying on our notion of the cluster separation gap as defined in (3). The\nfollowing theorem captures the key result concerning the collaborative optimization part in PCPO.\n\nTheorem 2 (Collaborative Policy Optimization) Let the failure probability be \u03b4 \u2208 (0, 1). Define\nL = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2} and let \u00afL denote the last epoch. If the number of rollouts per\nagent satisfies T \u2265 \u02dcO(1/\u22062), and Assumption 1 holds, then \u00afL > L and \u02c6K( \u00afL)\nsatisfies the following\nwith probability at least 1 \u2212 \u03b4 for every agent i \u2208 [N ]:\n\ni\n\nC\u03c3(i)( \u02c6K( \u00afL)\n\ni\n\n) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i)) \u2264 O\n\n\uf8ed\n\n\uf8eb\n\n(cid:113)\n\nc(p,7)\n(cid:113)\n\nlog (cid:0) 8DN T\n\n\u03b4\n\n\uf8f6\n\n(cid:1)\n\n\uf8f8 .\n\n(9)\n\nT |M\u03c3(i)|\n\n\u221a\n\nDiscussion. It was shown in Malik et al. (2020) that zeroth-order policy optimization provides a\n\u02dcO(1/\nT ) suboptimal policy using T rollouts for a single system LQR problem. In contrast, Wang\net al. (2023a) showed collaborative gains for a federated LQR setting while incurring additive bias\nterms that depend on the heterogeneity gap. Moreover, the results in Wang et al. (2023a) apply only\nto systems with bounded heterogeneity. Theorem 2 bridges this gap by showing that collaborative\n|M\u03c3(i)|-factor speedup for each agent i in (9)) can be achieved without\ngains (as evidenced by the\nany additive bias through careful cluster identification and collaboration exclusively within clusters.\nFurthermore, these gains hold for systems that can be arbitrarily different, as long as their optimal\ncosts are separated according to (3).\n\n(cid:113)\n\nCorollary 3 (Logarithmic communication complexity) The PCPO algorithm guarantees a loga-\nrithmic communication complexity with respect to the total number of rollouts T , the number of\nagents N , and the inverse of the separation gap 1/\u2206.\n\n9\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nIn the following, we provide proof sketches for both Theorem 1 and Theorem 2, while deferring\nthe detailed proofs to Kanakeri et al. (2025). The statements made in the proof sketches are proba-\nbilistic in nature. However, to keep the exposition simpler, we omit specifying the success/failure\nprobability of the statements and refer the readers to Kanakeri et al. (2025) for such details.\n\nProof sketch for Theorem 1. We start by showing that the estimated cost for each agent is in the\n\n\u2206l/4 neighborhood of its optimal cost, i.e., in each epoch l, for each agent i, we prove that\n\n(10)\n\n| \u02c6C\u03c3(i)(X (l)\n\n\u03c3(i))| \u2264 \u2206l/4.\n\u03c3(i)) = C\u03c3(j)(K\u2217\n\ni ) \u2212 C\u03c3(i)(K\u2217\nNote that for any agent j \u2208 M\u03c3(i), since C\u03c3(i)(K\u2217\n\u03c3(j)), in light of (10), it is\napparent that such an agent will pass the requirement in (7), and hence, never be eliminated from the\nneighborhood sets of agent i. This explains the first claim of Theorem 1. As for the second claim, note\nthat for any j /\u2208 M\u03c3(i), in light of our dissimilarity metric \u2206 in (3), |C\u03c3(i)(K\u2217\n\u03c3(j))| \u2265\n\u2206. Now for an epoch l such that \u2206l \u2264 \u2206/2, the above inequality can be combined with that in (10)\nto see that | \u02c6C\u03c3(i)(X (l)\nj )| \u2265 3\u2206/4, violating the requirement for inclusion in (7). It\nremains to establish (10), which follows from two guarantees: (i) | \u02c6C\u03c3(i)(X (l)\ni )| \u2264\n\u2206l/8, and (ii) |C\u03c3(i)(X (l)\n\u03c3(i))| \u2264 \u2206l/8. The second guarantee follows from an analysis\nof the local PO sub-routine in Line 4 of Algo. 1, drawing on Malik et al. (2020); the first follows\nfrom analyzing the cost estimation step in Line 5 based on a simple Hoeffding bound.\n\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\n\ni ) \u2212 C\u03c3(i)(X (l)\n\n\u03c3(i)) \u2212 C\u03c3(j)(K\u2217\n\ni ) \u2212 C\u03c3(i)(K\u2217\n\nProof sketch for Theorem 2. We prove Theorem 2 by conditioning on the event where the claims\nmade in Theorem 1 hold. As agents collaborate exclusively within their respective clusters and the\nreinitialization step synchronizes their global sequences after correct clustering, the gradient estimate\nobtained by averaging (see (6)) estimates of agents within a cluster enjoys a variance reduction effect\nunder Assumption 1. Using this, and the fact that the LQR cost satisfies a \u03d5-smoothness and \u00b5-PL\ncondition locally (Fazel et al., 2018; Malik et al., 2020), we establish that in each iteration k of the\nfinal epoch \u00afL, the following recursion holds with probability 1 \u2212 \u03b4\u2032:\n\uf8eb\n\n\uf8f6\n\nSk+1 \u2264\n\n(cid:16)\n\n1 \u2212\n\n(cid:17)\n\n\u03b7\u00b5\n4\n\nSk + 3\u03b7\n\n\uf8ec\n(p,8)D2\nc2\n\uf8ec\n\uf8ec\n\uf8ec\n(r(global)\n)2|M\u03c3(i)|M \u00afL\n\uf8ec\n(i, \u00afL)\n\uf8ed\n(cid:124)\n\n(cid:123)(cid:122)\ns1\n\nlog\n\n(cid:19)\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:125)\n\n+ \u03d52(r(global)\n\n(cid:124)\n\n(i, \u00afL)\n(cid:123)(cid:122)\ns2\n\n\uf8f7\n\uf8f7\n)2\n\uf8f7\n\uf8f7\n\uf8f7\n(cid:125)\n\uf8f8\n\n,\n\n(11)\n\ni\n\n) \u2212 C\u03c3(i)(K\u2217\n\nwhere Sk := C\u03c3(i)(Y (k)\n\u03c3(i)). The term s1 is due to the concentration of the minibatched\ngradient estimate around the gradient of a smoothed cost defined in Appendix A of Kanakeri et al.\n(2025); this is the term that benefits from collaboration. The term s2 captures the bias that arises\nwhen estimating gradients from noisy function evaluations. To ensure that this bias term does not\nnegate the collaborative speedup in s1, we choose the smoothing radius r(global)\nto minimize the\nsum s1 + s2. Unrolling the recursion for Rl iterations (with the choice of Rl in PCPO) provides\nthe per epoch convergence with rate \u02dcO\n. Finally, using the fact that Ml increases\n|M\u03c3(i)|M \u00afL\nexponentially with epochs, we establish that M \u00afL = \u02dc\u2126(T ).\n\n(i, \u00afL)\n\n(cid:113)\n\n1/\n\n(cid:17)\n\n(cid:16)\n\n5. Conclusion\n\nWe developed a novel clustering-based approach for learning personalized control policies using data\nfrom heterogeneous dynamical processes. As future work, we will explore (i) alternative measures of\ndissimilarity across systems, (ii) more general dynamical processes, and (iii) online settings.\n\n10\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nReferences\n\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy\ngradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning\nResearch, 22(98):1\u201376, 2021.\n\nBrian DO Anderson and John B Moore. Optimal control: linear quadratic methods. Courier\n\nCorporation, 2007.\n\nAshwin Aravind, Mohammad Taha Toghani, and C\u00e9sar A Uribe. A moreau envelope approach for\nLQR meta-policy estimation. In 2024 IEEE 63rd Conference on Decision and Control (CDC),\npages 415\u2013420. IEEE, 2024.\n\nDimitri P Bertsekas. Dynamic programming and optimal control 4th edition, volume ii. Athena\n\nScientific, 2015.\n\nYiting Chen, Ana M Ospina, Fabio Pasqualetti, and Emiliano Dall\u2019Anese. Multi-task system\nidentification of similar linear time-invariant dynamical systems. In 2023 62nd IEEE Conference\non Decision and Control (CDC), pages 7342\u20137349. IEEE, 2023.\n\nEyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and\nstopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of\nmachine learning research, 7(6), 2006.\n\nMaryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient\nmethods for the linear quadratic regulator. In Int. Conf. on Machine Learning, pages 1467\u20131476.\nPMLR, 2018.\n\nTesshu Fujinami, Bruce D Lee, Nikolai Matni, and George J Pappas. Domain randomization is\n\nsample efficient for linear quadratic control. arXiv preprint arXiv:2502.12310, 2025.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\nclustered federated learning. Advances in neural information processing systems, 33:19586\u201319597,\n2020.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\nclustered federated learning. IEEE Transactions on Information Theory, 68(12):8076\u20138091, 2022.\n\nBenjamin Gravell, Peyman Mohajerin Esfahani, and Tyler Summers. Learning optimal controllers\nfor linear systems with multiplicative noise via policy gradient. IEEE Transactions on Automatic\nControl, 66(11):5283\u20135298, 2020.\n\nTaosha Guo, Abed AlRahman Al Makdah, Vishaal Krishnan, and Fabio Pasqualetti. Imitation and\n\ntransfer learning for lqg control. IEEE Control Systems Letters, 7:2149\u20132154, 2023.\n\nBin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer Ba\u00b8sar. Toward a\ntheoretical foundation of policy optimization for learning control policies. Annual Review of\nControl, Robotics, and Autonomous Systems, 6(1):123\u2013158, 2023.\n\n11\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nChi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note\narXiv preprint\n\non concentration inequalities for random vectors with subgaussian norm.\narXiv:1902.03736, 2019.\n\nVinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, and Aritra Mitra. Harnessing data from\nclustered LQR systems: Personalized and collaborative policy optimization. arXiv preprint, 2025.\n\nJakub Kone\u02c7cn`y, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and\nDave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv\npreprint arXiv:1610.05492, 2016.\n\nBruce D Lee, Leonardo F Toso, Thomas T Zhang, James Anderson, and Nikolai Matni. Regret\nanalysis of multi-task representation learning for linear-quadratic adaptive control. In Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 39, pages 18062\u201318070, 2025.\n\nDhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter L Bartlett, and Martin J\nWainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic\nsystems. Journal of Machine Learning Research, 21(21):1\u201351, 2020.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In Artificial Intelli-\ngence and Statistics, pages 1273\u20131282. PMLR, 2017.\n\nAditya Modi, Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Joint\n\nlearning of linear time-invariant dynamical systems. Automatica, 164:111635, 2024.\n\nAmirreza Neshaei Moghaddam, Alex Olshevsky, and Bahman Gharesifard. Sample complexity\nof the linear quadratic regulator: A reinforcement learning lens. Journal of Machine Learning\nResearch, 26(151):1\u201350, 2025.\n\nHesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanovi\u00b4c. Global\nexponential convergence of gradient methods over the nonconvex landscape of the linear quadratic\nregulator. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 7474\u20137479.\nIEEE, 2019.\n\nHesameddin Mohammadi, Mahdi Soltanolkotabi, and Mihailo R Jovanovi\u00b4c. On the linear conver-\ngence of random search for discrete-time LQR. IEEE Control Systems Letters, 5(3):989\u2013994,\n2020.\n\nMaryann Rui and Munther A Dahleh. Learning clusters of partially observed linear dynamical\n\nsystems. In 2025 American Control Conference (ACC), pages 3545\u20133550. IEEE, 2025.\n\nFelix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered federated learning: Model-\nagnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural\nnetworks and learning systems, 32(8):3710\u20133722, 2020.\n\nCharis Stamouli, Leonardo F Toso, Anastasios Tsiamis, George J Pappas, and James Anderson.\n\nPolicy gradient bounds in multitask LQR. arXiv preprint arXiv:2509.19266, 2025.\n\n12\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nLili Su, Jiaming Xu, and Pengkun Yang. Global convergence of federated learning for mixed\n\nregression. Advances in Neural Information Processing Systems, 35:29889\u201329902, 2022.\n\nLeonardo F Toso, Han Wang, and James Anderson. Learning personalized models with clustered\nsystem identification. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages\n7162\u20137169. IEEE, 2023.\n\nLeonardo Felipe Toso, Donglin Zhan, James Anderson, and Han Wang. Meta-learning linear\nIn 6th Annual\n\nquadratic regulators: a policy gradient maml approach for model-free LQR.\nLearning for Dynamics & Control Conference, pages 902\u2013915. PMLR, 2024.\n\nOmkar Tupe, Max Hartman, Lav R Varshney, and Saurav Prakash. Federated nonlinear system\n\nidentification. arXiv preprint arXiv:2508.15025, 2025.\n\nHan Wang, Leonardo F Toso, Aritra Mitra, and James Anderson. Model-free learning with het-\nerogeneous dynamical systems: A federated LQR approach. arXiv preprint arXiv:2308.11743,\n2023a.\n\nHan Wang, Leonardo Felipe Toso, and James Anderson. Fedsysid: A federated approach to\nsample-efficient system identification. In Learning for Dynamics and Control Conference, pages\n1308\u20131320. PMLR, 2023b.\n\nLei Xin, Lintao Ye, George Chiu, and Shreyas Sundaram. Learning dynamical systems by leveraging\n\ndata from similar systems. IEEE Transactions on Automatic Control, 2025.\n\nKaiqing Zhang, Bin Hu, and Tamer Basar. Policy optimization for h2 linear control with h\u221e\nrobustness guarantee: Implicit regularization and global convergence. SIAM Journal on Control\nand Optimization, 59(6):4081\u20134109, 2021.\n\nThomas T Zhang, Katie Kang, Bruce D Lee, Claire Tomlin, Sergey Levine, Stephen Tu, and Nikolai\nMatni. Multi-task imitation learning for linear dynamical systems. In Learning for Dynamics and\nControl Conference, pages 586\u2013599. PMLR, 2023.\n\n13\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nAppendix A. Properties of the LQR problem\n\nNotation. For matrices A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n, we use \u2225A\u2225 to denote the Frobenius norm of\nA which is defined as \u2225A\u2225 = (cid:112)trace(A\u22a4A). We use \u27e8A, B\u27e9 to denote the Frobenius inner-product\ndefined as \u27e8A, B\u27e9 = trace(A\u22a4B).\n\nIn this section, we discuss some of the properties of the LQR cost that were established in Fazel\net al. (2018); Malik et al. (2020). In particular, the LQR cost in (1) is locally Lipschitz, locally\nsmooth, and enjoys a gradient-domination property over the set of stabilizing controllers. These key\nproperties aid in the convergence analysis of the model-free policy gradient algorithm, and we use\nthem in the proofs of Theorem 1 in Appendix B and Theorem 2 in Appendix C. However, to show\nthe convergence, it is crucial to ensure that the policy gradient iterates always lie within a restricted\nsubset of the stabilizing set with high probability. In Malik et al. (2020), such a restricted set is\nchosen based on the initial suboptimality gap. In our setting, given access to a set of initial stabilizing\ncontrollers for all agents, {K(0)\ni }i\u2208[N ], and our initial guess for the cluster separation gap, \u22060, we\ndefine \u02dc\u22060 := max{maxi\u2208[N ](C\u03c3(i)(K(0)\n\u03c3(i))), \u22060}, and the restricted sets as follows\nfor all j \u2208 [H]:\n\n) \u2212 C\u03c3(i)(K\u2217\n\ni\n\n(12)\n\nj ) \u2264 10 \u02dc\u22060}.\n\nj := {K \u2208 Rm\u00d7n : Cj(K) \u2212 Cj(K\u2217\nG0\nProperties of the LQR cost. For each system j \u2208 [H], on the restricted domain G0\n\nj , Malik et al.\n(2020) showed that the local properties hold uniformly, i.e., \u2203\u03d5j > 0, \u03bbj > 0, \u03c1j > 0, such that the\nLQR cost in (1) is (\u03bbj, \u03c1j)-locally Lipschitz and (\u03d5j, \u03c1j)-locally smooth for all policies K \u2208 G0\nj .\nFurthermore, it is known that the LQR cost satisfies the PL (gradient-domination) condition for all\npolicies in the stabilizing set (see Lemma 3 of Malik et al. (2020)). Denoting the parameter for the\nPL condition for system j by \u00b5j > 0, we define \u00b5 := min{\u00b51, \u00b52, . . . , \u00b5H }. Similarly, defining\n\u03d5 := max{\u03d51, \u03d52, . . . , \u03d5H }, \u03bb := max{\u03bb1, \u03bb2, . . . , \u03bbH }, and \u03c1 := min{\u03c11, \u03c12, . . . , \u03c1H }, we can\nensure that for every system j \u2208 [H], the cost in (1) is (\u03bb, \u03c1)-locally Lipschitz and (\u03d5, \u03c1)-locally\nsmooth for all policies in their respective restricted sets G0\nj , and \u00b5-PL in their respective stabilizing\nsets. The following lemmas from Malik et al. (2020) capture these properties.\n\nLemma 4 (LQR cost is locally Lipschitz). For any system j \u2208 [H], given a pair of policies\n(K, K\u2032) \u2208 (G0\n\nj \u00d7 G0\n\nj ), if \u2225K \u2212 K\u2032\u2225 \u2264 \u03c1, we have\n(cid:12)Cj(K) \u2212 Cj(K\u2032)(cid:12)\n(cid:12)\n\n(cid:12) \u2264 \u03bb\u2225K \u2212 K\u2032\u2225.\n\nLemma 5 (LQR cost has locally Lipschitz gradients.) For any system j \u2208 [H], given a pair of\npolicies (K, K\u2032) \u2208 (G0\n\nj \u00d7 G0\n\nj ), if \u2225K \u2212 K\u2032\u2225 \u2264 \u03c1, we have\n(cid:13)\u2207Cj(K) \u2212 \u2207Cj(K\u2032)(cid:13)\n(cid:13)\n\n(cid:13) \u2264 \u03d5\u2225K \u2212 K\u2032\u2225.\n\nLemma 6 (LQR cost satisfies PL.) For any system j \u2208 [H], given a stable policy K, we have\n\n\u2225\u2207Cj(K)\u22252 \u2265 \u00b5(Cj(K) \u2212 Cj(K\u2217\n\nj )).\n\nSmoothed cost and the properties of the gradient estimate. The smoothed cost with a radius\nr for a system j \u2208 [H] is defined as Cj,r(K) := E[Cj(K + rv)], where v is uniformly distributed\nover all matrices in Rm\u00d7n with the Frobenius norm of at most 1. It is shown in Fazel et al. (2018);\nMalik et al. (2020) that the zeroth-order gradient estimate gi(\u00b7) as defined in (5) for an agent i is an\n\n14\n\n(13)\n\n(14)\n\n(15)\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nunbiased estimator of the gradient of the smoothed cost. In particular, for all systems j \u2208 [H] and all\nagents i \u2208 Mj, we have the following properties for all K \u2208 G0\n\nj and r \u2208 (0, \u03c1):\n\nE[gi(K)] = \u2207C\u03c3(i),r(K)\n\n\u2225\u2207Cj,r(K) \u2212 \u2207Cj(K)\u2225 \u2264 \u03d5r.\n\n(16)\n\n(17)\n\nj , \u2200r \u2208 (0, \u03c1) and all U \u2208 Rm\u00d7n with \u2225U \u2225 = 1. In other words, there exists G(j)\n\nFurthermore, it is known that the noisy rollout cost Cj(K + rU ; Z (i)) is uniformly bounded\n\u221e \u2265 0 such\n\u221e .) Let\n\u221e }. Hence, for any agent i \u2208 [N ], we have the following\n\n\u221e (see Lemma 11 of Malik et al. (2020) for the expression of G(j)\n\n\u2200K \u2208 G0\nthat Cj(K + rU ; Z (i)) \u2264 G(j)\nus define G\u221e := max{G(1)\n\u2200K \u2208 G0\n\nj , \u2200r \u2208 (0, \u03c1) and all U \u2208 Rm\u00d7n with \u2225U \u2225 = 1:\n\n\u221e , . . . , G(H)\n\n\u221e , G(2)\n\nC\u03c3(i)(K + rU ; Z (i)) \u2264 G\u221e.\n\n(18)\n\nWe then have the following concentration result that will prove useful in establishing \u201cvariance-\n\nreduction\" effects.\n\nLemma 7 (Concentration of the zeroth-order gradient estimates). For any system j \u2208 [H], given\na policy K \u2208 G0\nj , a smoothing radius r \u2208 (0, \u03c1) and a failure probability \u03b4\u2032 \u2208 (0, 1), the following\nholds for the M -minibatched zeroth-order gradient estimate of an agent i \u2208 Mj with probability at\nleast 1 \u2212 \u03b4\u2032:\n\n\u2225gi(K) \u2212 \u2207Cj,r(K)\u2225 \u2264\n\n(cid:16)\n\nD + \u03d5 \u03c12\nG\u221e + \u03bb \u03c1\n\u221a\nM\n\nD\n\nr\n\n(cid:17)\n\n(cid:115)\n\nD\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:19)\n.\n\n(19)\n\n(cid:13)\n(cid:13)\nk=1(g(i,k)(K) \u2212 \u2207Cj,r(K))\nProof We have \u2225gi(K) \u2212 \u2207Cj,r(K)\u2225 =\n(cid:13), where we denoted\nthe k-th component of the minibatch as g(i,k)(K). Recall from (5) that this k-th component takes the\nform\n\n(cid:80)M\n\n(cid:13)\n(cid:13)\n(cid:13)\n\n1\nM\n\ng(i,k)(K) = Cj(K + rUk; Z (i)\nk )\n\n(cid:19)\n\n(cid:18) D\nr\n\nUk.\n\nHence, we have \u2225g(i,k)(K)\u2225 \u2264 D\nWe then have\n\nr G\u221e due to (18) as \u2225Uk\u2225 = 1. Let us define c(p,8) :=\n\n(cid:16)\n\nG\u221e + \u03bb \u03c1\n\nD + \u03d5 \u03c12\n\nD\n\n(cid:17)\n\n.\n\n\u2225g(i,k)(K) \u2212 \u2207Cj,r(K)\u2225 = \u2225g(i,k)(K) \u2212 \u2207Cj,r(K) + \u2207Cj(K) \u2212 \u2207Cj(K)\u2225\n\n(a)\n\u2264 \u2225g(i,k)(K)\u2225 + \u2225\u2207Cj,r(K) \u2212 \u2207Cj(K)\u2225 + \u2225\u2207Cj(K)\u2225\n(b)\n\u2264\n\nD\nr\nD\nr\n\nD\nr\nD\nr\n\n=\n\n(c)\n\u2264\n\n=\n\nG\u221e + \u03d5r + \u03bb\n(cid:18)\n\nG\u221e +\n\n(cid:18)\n\nG\u221e +\n\nr2\nD\n\u03c12\nD\n\n\u03d5 +\n\n\u03d5 +\n\n(cid:19)\n\n\u03bb\n\nr\nD\n\n(cid:19)\n\n\u03bb\n\n\u03c1\nD\n\nc(p,8).\n\n15\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nTable 1: Relevant notation and definitions\n\nNotation\nMl\nRl\n\u03b7\nrl\n\u2206l\nN (l)\ni\nX (l)\ni\n\u02c6K(l)\ni\n\nDefinition\nMinibatch size used to estimate zeroth-order gradients in the l-th epoch.\nNumber of steps/iterations of the local and global policy optimization in the l-th epoch.\nStep size for both local and global policy optimization.\nSmoothing radius used in the zeroth-order gradient estimates in the l-th epoch.\nEstimate of \u2206 used to cluster the agents in the lth epoch.\nNeighborhood set corresponding to the i-th agent in the l-th epoch.\nLocal policy for the i-th agent in the l-th epoch.\nGlobal policy for the i-th agent in the l-th epoch.\n\nIn the above, (a) follows from the triangle inequality, and (b) follows from the uniform-boundedness\nof the noisy rollout together with (5), the bounded bias property as shown in (17), and the local-\nLipschitz property in Lemma 4. Finally, (c) follows from using r < \u03c1. Hence, g(i,k)(K) \u2212 \u2207Cj,r(K)\nhas a bounded norm and therefore belongs to a class of norm sub-Gaussian random matrices (Jin\net al., 2019). Furthermore, it has zero mean due to (16). Therefore, the concentration result follows\nfrom a direct application of Corollary 7 from Jin et al. (2019) which provides a Hoeffding-type\ninequality for norm sub-Gaussian random matrices.\n\nCorollary 8 (Concentration of the collaborative zeroth-order gradient estimates.) Suppose As-\nsumption 1 holds. For any system j \u2208 [H], given a policy K \u2208 G0\nj , a smoothing radius r \u2208 (0, \u03c1),\ndefine the collaborative zeroth-order gradient estimate as Gj(K) = 1\ngi(K), where\ngi(K) is the M -minibatched gradient estimate from agent i \u2208 Mj. Let \u03b4\u2032 \u2208 (0, 1). The following\nholds with probability at least 1 \u2212 \u03b4\u2032:\n\ni\u2208Mj\n\n|Mj |\n\n(cid:80)\n\n(cid:16)\n\n\u2225Gj(K) \u2212 \u2207Cj,r(K)\u2225 \u2264\n\nG\u221e + \u03bb \u03c1\n\nD + \u03d5 \u03c12\nr(cid:112)|Mj|M\n\nD\n\n(cid:17)\n\n(cid:115)\n\nD\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:19)\n.\n\n(20)\n\nProof Under Assumption 1, we note that the noise processes for all agents in Mj are independent.\nThe proof then follows from Lemma 7 as the collaborative zeroth-order gradient estimate can be\ninterpreted as a gradient estimate for system j with a |Mj|-fold increased minibatch size.\n\nNote on the problem dependent constants. In Malik et al. (2020), the values of the constants\n\u03bbj, \u03d5j, \u03c1j, G(j)\n\u221e are first derived locally in terms of the local cost Cj(K), and then, the global\nparameters are obtained by noting that the local cost is uniformly bounded over the restricted domain\nas shown in Lemma 9 of Malik et al. (2020). Since we have a different definition of the restricted\ndomain, the values of our parameters vary from the ones provided in Malik et al. (2020). That said,\nthe global parameters in our setting can be derived exactly in the same way as in Malik et al. (2020)\nby bounding the local cost as Cj(K) \u2264 10 \u02dc\u22060 + Cj(K\u2217\n\nj ).\n\nFor convenience, we compile all the relevant notation in Table 1.\nIn the main text, we used the notation c(p,_) to denote the problem-parameter-dependent constants\n\nwhich are defined in the following.\n\n16\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\n(cid:18)\n\nc(p,8) =\n\nG\u221e + \u03bb\n\n\u03c1\nD\n\n+ \u03d5\n\n(cid:19)\n\n\u03c12\nD\n\nc(p,9) =\n\n12c(p,8)\n\u00b5\n\n(cid:18)\n\nmax\n\n(cid:26)\n\n(cid:112)\u03d5,\n\n(cid:27)(cid:19)2\n\n1\n\u03c1\n\n(p,9)D2, c2\n(cid:33)\n\n(p,8)D2\u22062\n\n0, 36G2\n\u221e\n\nc(p,10) = max\n\n\u22062\n\n0, 256c2\n\n(cid:110)\n\nc(p,11) =\n\nc(p,12) =\n\n(cid:32)\n\n(cid:1)\n\n\u22062\n0\nc(p,10) log (cid:0) 8DN\n(cid:32)\n(cid:32)\n4\n\u03b7\u00b5\n\nlog\n\n\u03b4\nc(p,10)N \u02dc\u22062\n0\n\u22062\n0\n(cid:113)\n\nc(p,13) = 4 max{1, c(p,9)}\n\n\uf8f1\n\uf8f2\n\n\uf8f3\n\n8\n\u00b5\n\n,\n\n1\n4\u03d5\n\n,\n\nc(p,1) = min\n\nc(p,2) =\n\n4\n\u03b7\u00b5\n\n(cid:33)\n\n(cid:33)\n\n+ log(4)\n\nc(p,12) log(c(p,11)T )\n\uf8fc\n\uf8fd\n\n\u03c1\n\n\u03bb + 2 max\n\n(cid:110)\u221a\n\n\u03d5, 1\n\u03c1\n\n(cid:111)\n\n\uf8fe\n\n(cid:111)\n\n(21)\n\nc(p,3) = \u02dc\u22060 max{16, 10c(p,10)}\nc(p,4) = c(p,10)\n\nc(p,5) =\n\nc(p,8)D\n\u03d5\n\nc(p,6) = \u03c1\nc(p,7) = Dc(p,13)\n\nBased on the above definitions of the problem-parameter-dependent constants, we provide the\n\nvalues used for the hyperparameters in the l-th epoch of the PCPO algorithm in Table 2.\n\n17\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nTable 2: Hyperparameters with their values in the lth epoch\n\nHyperparameters\n\nValues\n\n\u2206l\n\n\u03b4l\n\n\u03b7\n\nRl\n\nMl\n\n\u02dcrl\n\nr(loc)\nl\n\nr(global)\nl\n\n\u22060\n2l\n\n\u03b4\n2l2\n\nc(p,1)\n\nc(p,2) log\n\nc(p,4)\n\u22062\nl\n(cid:114)\n\nlog\n\nlog\n\n(cid:18) c(p,5)\u221a\n\nMl\n\n(cid:17)\n\n(cid:17)\n\n(cid:16) c(p,3)N\n\u22062\nl\n(cid:16) 8DN Rl\n\u03b4l\n(cid:16) 8DN Rl\n\u03b4l\n\n(cid:17)(cid:19)1/2\n\nmin{c(p,6), \u02dcrl}\n(cid:26)\n\n(cid:27)\n\nmin\n\nc(p,6),\n\n\u02dcrl\n|N (l\u22121)\ni\n\n|1/4\n\nAppendix B. Proof of Theorem 1\n\nIn this section, we provide the proof of Theorem 1 which concerns the clustering aspect of the PCPO\nalgorithm. In particular, we show that, with high probability, the true clusters are included in the\nneighborhood sets for all agents in each epoch, and if epoch l \u2265 L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2},\nthe neighborhood sets are identical to the clusters. More specifically, we show that for all agents\ni \u2208 [N ], with probability at least 1 \u2212 \u03b4/2, M\u03c3(i) \u2286 N (l)\nin every epoch l, and if l \u2265 L, then\ni\nM\u03c3(i) = N (l)\ni\n\n.\n\nTo prove both claims, it suffices to show that with high probability, the estimated cost at a locally\noptimized policy is concentrated in the \u2206l/4-neighborhood of the optimal cost in every epoch l for\nall agents i \u2208 [N ]. To see this, consider a \u201cgood\u201d event that occurs with probability 1 \u2212 \u03b4/2 where\nthe following holds for all agents i \u2208 [N ] in every epoch l (we will prove that such an event exists\nlater in this section):\n\n| \u02c6C\u03c3(i)(X (l)\n\ni ) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i))| \u2264 \u2206l/4.\n\n(22)\n\nOn this \u201cgood\u201d event, in what follows, we show that the first claim of Theorem 1 holds. Accordingly,\nfix an agent i and consider an agent j \u2208 M\u03c3(i). We now show by induction that j belongs to N (l)\ni\nin every epoch l. For the base case of induction, note that since we initialize the neighborhood sets\n. Next, for an epoch l \u2212 1 \u2265 1, let us assume that j \u2208 N (l\u22121)\nwith all agents, j \u2208 N (0)\n. Since\n\u03c3(i)) = C\u03c3(j)(K\u2217\nC\u03c3(i)(K\u2217\n\u03c3(j)) as a consequence of j \u2208 M\u03c3(i), in the l-th epoch under the \u201cgood\u201d\n\ni\n\ni\n\n18\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nevent where (22) holds, we have\n\n| \u02c6C\u03c3(i)(X (l)\n\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\n\nj )| \u2264 | \u02c6C\u03c3(i)(X (l)\n\ni ) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i))| + | \u02c6C\u03c3(j)(X (l)\n\nj ) \u2212 C\u03c3(j)(K\u2217\n\n\u03c3(j))|\n\n\u2264 \u2206l/4 + \u2206l/4 = \u2206l/2,\n\nbased on the neighborhood set update rule in (7). Hence, by induction,\n\nimplying that j \u2208 N (l)\nj \u2208 N (l)\n\ni\n\ni\n\nin every epoch, therefore establishing the first claim of Theorem 1.\n\nNext, we show that on the \u201cgood\u201d event where (22) holds for all agents in every epoch, the\nsecond claim of Theorem 1 is also true. We prove this claim via contradiction. To proceed, suppose\nthat there exist an epoch l \u2265 L, an agent i, and an agent j /\u2208 M\u03c3(i) such that j \u2208 N (l)\n. Then, we\ni\nhave the following in light of the heterogeneity metric defined in (3):\n\n\u2206 \u2264\n\n\u2264\n\n\u2264\n\n(cid:12)\n(cid:12)C\u03c3(i)(K\u2217\n(cid:12)\n(cid:12)\n(cid:12)C\u03c3(i)(K\u2217\n(cid:12)\n(cid:12)\n(cid:12)C\u03c3(i)(K\u2217\n(cid:12)\n\n(cid:12)\n(cid:12)\n\u03c3(j))\n(cid:12)\ni ) + \u02c6C\u03c3(j)(X (l)\n(cid:12)\n\u02c6C\u03c3(j)(X (l)\n(cid:12)\n(cid:12)\n\n\u03c3(i)) \u2212 C\u03c3(j)(K\u2217\n\u03c3(i)) \u2212 \u02c6C\u03c3(i)(X (l)\n(cid:12)\n\u03c3(i)) \u2212 \u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12) +\ni )\n(cid:12)\n\u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12)\n\n(cid:12)\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\n(cid:12)\nj )\n(cid:12)\n(cid:12)\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\n(cid:12)\n(cid:12) ,\nj )\n\n\u02c6C\u03c3(i)(X (l)\n\n(a)\n\u2264 \u2206l/4 + \u2206l/4 +\n\n(b)\n\u2264 \u2206/4 +\n\n(cid:12)\n(cid:12)\n(cid:12)\n\nj ) \u2212 C\u03c3(j)(K\u2217\n\nj ) \u2212 C\u03c3(j)(K\u2217\n\n\u03c3(j)) + \u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12)\n(cid:12)\n\n(cid:12)\n(cid:12)\n(cid:12) +\n\n\u03c3(j))\n\n\u02c6C\u03c3(i)(X (l)\n\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\nj )\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\n(cid:12)\nj )\n(cid:12)\n\n(cid:12)\n(cid:12)\n(cid:12)\n\n\u02c6C\u03c3(i)(X (l)\n\nwhere (a) holds due to (22), and (b) follows as \u2206l \u2264 \u2206/2 since l \u2265 L. The above set of inequalities\n(cid:12)\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\n(cid:12)\n(cid:12) \u2265 (3/4)\u2206 \u2265 (3/2)\u2206l, contradicting our assumption that\nj )\nimply that\n(cid:12)\n(cid:12)\n(cid:12) \u2264 \u2206l/2. Therefore, N (l)\ni ) \u2212 \u02c6C\u03c3(j)(X (l)\n\u02c6C\u03c3(i)(X (l)\nj \u2208 N (l)\n(cid:12)\n(cid:12)\ni = M\u03c3(i) for all\nj )\n(cid:12)\nl \u2265 L, establishing the second claim of Theorem 1.\n\ni which requires that\n\nNow, it remains to prove that the \u201cgood\u201d event where (22) holds for all agents in every epoch\n\noccurs with probability at least 1 \u2212 \u03b4/2. To do so, for an agent i \u2208 [N ] in epoch l, we have\n\n(cid:12)\n(cid:12)\n(cid:12)\n\n\u02c6C\u03c3(i)(X (l)\n\ni ) \u2212 C\u03c3(i)(K\u2217\n\n(cid:12)\n(cid:12)\n(cid:12) \u2264\n\u03c3(i))\n\n(cid:12)\n(cid:12)C\u03c3(i)(X (l)\n(cid:12)\n\ni ) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i))\n\n(cid:12)\n(cid:12)\n(cid:12) +\n\n(cid:12)\n(cid:12)\n(cid:12)\n\n\u02c6C\u03c3(i)(X (l)\n\n(cid:12)\ni ) \u2212 C\u03c3(i)(X (l)\n(cid:12)\n(cid:12) .\ni )\n\nTherefore, to show (22), it suffices to show the following two guarantees for all agents in every\nepoch:\n\n(a)\n\n(b)\n\n(cid:12)\n(cid:12)C\u03c3(i)(X (l)\n(cid:12)\n(cid:12)\n\u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12)\n\ni ) \u2212 C\u03c3(i)(K\u2217\n\u03c3(i))\ni ) \u2212 C\u03c3(i)(X (l)\ni )\n\n(cid:12)\n(cid:12)\n(cid:12) \u2264 \u2206l/8\n(cid:12)\n(cid:12)\n(cid:12) \u2264 \u2206l/8.\n\n(23)\n\nNow, let us establish the claims in (23) via induction across epochs. Let us assume that for a fixed\nepoch l \u2212 1 \u2265 1, the claims in (23) hold for all agents i \u2208 [N ] in every epoch k \u2264 {1, 2, . . . , l \u2212 1},\nwith probability at least (1 \u2212 (cid:80)l\u22121\n\u03b4j\n2 ). Denoting this event as El\u22121, in the following, we show that\nj=1\nthe claims in (23) hold in epoch l, \u2200i \u2208 [N ] with probability at least (1 \u2212 \u03b4l\n2 ) conditioned on the\nevent El\u22121.\n\nIn the following, fixing an agent i \u2208 [N ], we show: (a)\n\n(cid:12)\n(cid:12)C\u03c3(i)(X (l)\n(cid:12)\nwith probability at least 1\u2212\u03b4l/(4N ) conditioned on the event El\u22121, and (b)\n\ni ) \u2212 C\u03c3(i)(K\u2217\n(cid:12)\n\u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12)\n\n(cid:12)\n(cid:12)\n(cid:12) \u2264 \u2206l/8\n\u03c3(i))\n(cid:12)\ni ) \u2212 C\u03c3(i)(X (l)\n(cid:12)\n(cid:12) \u2264\ni )\n\n19\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\n\u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N ) conditioned on the intersection of the events El\u22121 and the\none where item (a) in (23) holds. The following lemma provides the convergence of the local policy\noptimization sub-routine in epoch l which aids in establishing claim (a) from (23).\n\nLemma 9 (Local Policy Optimization.) For any agent i \u2208 Mj, given a policy K0 \u2208 G0\nj , let KR be\nthe output of the localPO(K0, M, R, r) subroutine with step size \u03b7. Then, for any \u03b4\u2032 \u2208 (0, 1/R),\nwith probability at least 1 \u2212 \u03b4\u2032R, KR \u2208 G0\n\nj and we have the following:\n\nCj(KR) \u2212 Cj(K\u2217\n\nj ) \u2264\n\n(cid:16)\n\n1 \u2212\n\n(cid:17)R\n\n\u03b7\u00b5\n4\n\n(Cj(K0) \u2212 Cj(K\u2217\n\nj )) +\n\n(cid:32)\n\nc(p,9)D\n\u221a\nM\n\n(cid:115)\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:19)(cid:33)\n\n,\n\n(24)\n\nwhen \u03b7 = c(p,1), M \u2265\n\nc(p,4)\n\u22062\n0\n\nlog(2D/\u03b4\u2032), r = min{\u03c1, \u02dcr}, where \u02dcr =\n\n(cid:18) c(p,5)\u221a\n\nM\n\n(cid:113)\n\nlog (cid:0) 2D\n\u03b4\u2032\n\n(cid:19)1/2\n\n(cid:1)\n\n.\n\nThe proof of Lemma 9 is provided in Appendix B.1. We use Lemma 9 to analyze the local policy\n(cid:12)\ni ) \u2212 C\u03c3(i)(K\u2217\n(cid:12)\noptimization step in line 4 of the PCPO algorithm that helps in establishing\n(cid:12) \u2264\n\u03c3(i))\n\u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N ). More precisely, the settings for the hyperparameters\n(\u03b7, Ml, r(loc)\n, Rl) from Table 2 meet the requirement for the corresponding hyperparameters in\nLemma 9. Furthermore, conditioned on the event El\u22121, claim (a) in (23) implies that X (l\u22121)\n\u03c3(i).\nHence, the following holds due to Lemma 9 with probability at least 1\u2212\u03b4\u2032Rl for some \u03b4\u2032 \u2208 (0, 1/Rl):\n\n(cid:12)\n(cid:12)C\u03c3(i)(X (l)\n(cid:12)\n\n\u2208 G0\n\ni\n\nl\n\nC\u03c3(i)(X (l)\n\ni )\u2212C\u03c3(i)(K\u2217\n\n\u03c3(i)) \u2264\n\n1 \u2212\n\n(cid:17)Rl\n\n\u03b7\u00b5\n4\n\n(cid:16)\n\n(cid:124)\n\n(C\u03c3(i)(X (l\u22121)\n\ni\n(cid:123)(cid:122)\ns1\n\n) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i)))\n(cid:125)\n\n+\n\n(cid:32)\n\n(cid:124)\n\nc(p,9)D\n\u221a\nMl\n\n(cid:115)\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:123)(cid:122)\ns2\n\n(cid:19)(cid:33)\n\n.\n\n(cid:125)\n\ni\n\n(cid:17)\n\n(cid:17)\n\n(cid:16) 16 \u02dc\u22060\n\u2206l\n\n) \u2212 C\u03c3(i)(K\u2217\n\nensures s1 \u2264 \u2206l/16. Similarly, setting Ml =\n\nNote that (C\u03c3(i)(X (l\u22121)\nevent El\u22121 where claim (a) of (23) holds. Hence, from Table 2, setting Rl = c(p,2) log\n\n\u03c3(i))) \u2264 \u2206l\u22121/8 \u2264 \u22060 \u2264 \u02dc\u22060 as a result of conditioning on the\n\u2265\n\n(cid:16) c(p,3)N\n\u22062\nl\nlog (cid:0) 2D\n\u03b4\u2032\n\u03c3(i)) \u2264\n\u03c3(i) with probability at least (1 \u2212 \u03b4l/(4N )), based on Lemma 9. Let us\n\n4\n\u03b7\u00b5 log\nensures s2 \u2264 \u2206l/16. Finally, setting \u03b4\u2032 = \u03b4l/(4N Rl) provides us with C\u03c3(i)(X (l)\n\u2206l/8, and hence X (l)\ni \u2208 G0\ndenote this event by \u02dcE(l,1).\n(cid:12)\n(cid:12)\ni ) \u2212 C\u03c3(i)(X (l)\n\u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12)\n(cid:12) \u2264 \u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N )\ni )\nNow, we show that\n(cid:12)\nj ) and E[C\u03c3(i)(X (l)\nconditioned on the event \u02dcE(l,1). We have \u02c6C\u03c3(i)(X (l)\ni ) = 1\nMl\n, Z (i)\nC\u03c3(i)(X (l)\ni \u2208 G0\nj ) \u2264 G\u221e due to\n(18). Using Hoeffding\u2019s inequality, the following holds for all s \u2265 0:\n\n(cid:80)Ml\nj=1 C\u03c3(i)(X (l)\n, Z (i)\nj , we have C\u03c3(i)(X (l)\n\ni ). Furthermore, since on event \u02dcE(l,1), X (l)\n\n(p,9)D2\n\u22062\nl\ni )\u2212C\u03c3(i)(K\u2217\n\nlog (cid:0) 2D\n\u03b4\u2032\n\nc(p,4)\n\u22062\nl\n\n(cid:1) \u2265\n\n162c2\n\n(cid:1)\n\ni\n\ni\n\ni\n\n, Z (i)\n\nj )] =\n\nP\n\n\uf8eb\n\n\uf8ed\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\n1\nMl\n\nMl(cid:88)\n\nj=1\n\nC\u03c3(i)(X (l)\ni\n\n, Z (i)\n\nj ) \u2212 C\u03c3(i)(X (l)\ni )\n\n\uf8f6\n\n\u2265 s\n\n\uf8f8 \u2264 2 exp\n\n(cid:18) \u22122s2Ml\nG2\n\u221e\n\n(cid:19)\n\n.\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\nSetting s = \u2206l/8, and requiring the failure probability on the R.H.S to be lesser than \u03b4l/(4N ) leads\nto the requirement: Ml \u2265 36G2\nwhich is satisfied by our choice of Ml from Table 2.\n\u22062\nl\n\n(cid:16) 8N\n\u03b4l\n\nlog\n\n(cid:17)\n\n\u221e\n\n20\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\n(cid:12)\n\u02c6C\u03c3(i)(X (l)\n(cid:12)\nHence,\n(cid:12)\nthis event as \u02dcE(l,2).\n\ni ) \u2212 C\u03c3(i)(X (l)\ni )\n\n(cid:12)\n(cid:12)\n(cid:12) \u2264 \u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N ). Let us denote\n\nNow, to show the two claims in (23), let us define an event \u02dcEl = \u02dcE(l,1) \u2229 \u02dcE(l,2). Then,\nP( \u02dcEl|El\u22121) = P( \u02dcE(l,2)| \u02dcE(l,1), El\u22121)P( \u02dcE(l,1)|El\u22121) \u2265 (1 \u2212 \u03b4l/(4N ))(1 \u2212 \u03b4l/(4N )) \u2265 1 \u2212 \u03b4l/(2N ).\n(cid:12)\nTherefore, on event \u02dcEl, both the guarantees: (a)\n(cid:12)\n(cid:12) \u2264 \u2206l/8 and (b)\n(cid:12)\n(cid:12)\n\u02c6C\u03c3(i)(X (l)\n(cid:12)\n(cid:12)\n(cid:12) \u2264 \u2206l/8 hold with probability at least 1 \u2212 \u03b4l/(2N ). Union bounding\n(cid:12)\nover all the agents, with probability at least 1 \u2212 \u03b4l/2, both claims in (23) hold for all agents i \u2208 [N ]\non the event \u02dcEl after conditioning on the event El\u22121.\n\ni ) \u2212 C\u03c3(i)(X (l)\ni )\n\n(cid:12)\n(cid:12)C\u03c3(i)(X (l)\n(cid:12)\n\ni ) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i))\n\nFinally, defining an event El = \u02dcEl \u2229 El\u22121, we have,\n\nP(El) = P( \u02dcEl|El\u22121)P(El\u22121) \u2265 (1 \u2212 \u03b4l/2)\n\n\uf8ed1 \u2212\n\n\uf8eb\n\nl\u22121\n(cid:88)\n\nj=1\n\n\u03b4j\n2\n\n\uf8f6\n\n\uf8eb\n\n\uf8f8 \u2265\n\n\uf8ed1 \u2212\n\n\uf8f6\n\n\uf8f8 .\n\nl\n(cid:88)\n\nj=1\n\n\u03b4j\n2\n\nSince \u03b4l = \u03b4/(2l2), we have (cid:80)l\n\nj=1\n\nB.1. Proof of Lemma 9\n\n\u03b4j\n\n2 = (cid:80)l\n\nj=1\n\n\u03b4\n4j2 \u2264 \u03b4/2. This completes the proof of Theorem 1.\n\nIn this section, we prove Lemma 9 which provides the convergence of the localPO subroutine. Fix\na system j \u2208 [H] and let Kt denote the controller in the t-th iteration of localPO \u2200t = 0, 1, . . . , R.\nNote that the localPO sub-routine proceeds as follows: starting with a controller K0 \u2208 G0\nj , in\nevery iteration t, Kt is updated as Kt+1 = Kt \u2212 \u03b7g(Kt), where g(Kt) = ZO(Kt, M, r) is the\nM -minibatched zeroth-order gradient estimate with a smoothing radius r. We prove the statement\nvia induction. Given the base case K0 \u2208 G0\nj and \u03b4\u2032 \u2208 (0, 1/R), let us assume that in the t-th iteration\nthe following holds for all \u03c4 \u2208 {1, 2, . . . , t} with probability at least (1 \u2212 \u03b4\u2032t) :\n\nK\u03c4 \u2208 G0\nj\n\nCj(K\u03c4 ) \u2212 Cj(K\u2217\n\nj ) \u2264\n\n(cid:16)\n\n1 \u2212\n\n(cid:17)\n\n\u03b7\u00b5\n4\n\n(Cj(K\u03c4 \u22121) \u2212 Cj(K\u2217\n\nj )) +\n\n(cid:32)\n\n\u03b7\u00b5\n4\n\nc(p,9)D\n\u221a\nM\n\n(cid:115)\n\nlog\n\n(cid:19)(cid:33)\n\n.\n\n(cid:18) 2D\n\u03b4\u2032\n\n(25)\n\nLet us denote the event where both the claims in (25) hold by Et. Now, conditioned on the event Et,\nin the following, we will show that with probability at least 1 \u2212 \u03b4\u2032, Kt+1 \u2208 G0\n\nj and\n\nCj(Kt+1) \u2212 Cj(K\u2217\n\nj ) \u2264\n\n(cid:16)\n\n1 \u2212\n\n(cid:17)\n\n\u03b7\u00b5\n4\n\n(Cj(Kt) \u2212 Cj(K\u2217\n\nj )) +\n\n(cid:32)\n\n\u03b7\u00b5\n4\n\nc(p,9)D\n\u221a\nM\n\n(cid:115)\n\nlog\n\n(cid:19)(cid:33)\n\n.\n\n(cid:18) 2D\n\u03b4\u2032\n\nIn what follows, we omit the subscript notation j for convenience. Conditioned on the event Et,\n\nwe begin by analyzing the one-step progress in the (t + 1)-th iteration of localPO.\n\nFrom Lemma 7, as the event Et ensures that Kt \u2208 G0, we have \u2225g(Kt) \u2212 \u2207Cr(Kt)\u2225 \u2264\n(cid:1) with probability at least (1 \u2212 \u03b4\u2032). Let us denote this event by \u02dcEt. Define et :=\n\n(cid:113)\n\nlog (cid:0) 2D\n\u03b4\u2032\n\nc(p,8)D\n\u221a\nM\nr\n\n21\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\ng(Kt) \u2212 \u2207C(Kt). Conditioned on the event \u02dcEt \u2229 Et, we have\n\n\u2225et\u2225 = \u2225g(Kt) \u2212 \u2207Cr(Kt) + \u2207Cr(Kt) \u2212 \u2207C(Kt)\u2225\n\n(a)\n\u2264 \u2225g(Kt) \u2212 \u2207Cr(Kt)\u2225 + \u2225\u2207Cr(Kt) \u2212 \u2207C(Kt)\u2225\n\n(b)\n\u2264\n\nc(p,8)D\n\u221a\nM\nr\n\n(cid:115)\n\nlog\n\n(cid:19)\n\n(cid:18) 2D\n\u03b4\u2032\n\n+ \u03d5r,\n\n(26)\n\nwhere (a) follows from the triangle inequality and the (b) due to the event \u02dcEt \u2229 Et and (17). Let us\n(cid:18) c(p,8)D\n\n(cid:19)1/2\n\n(cid:113)\n\n(cid:113)\n\n, r = min{\u03c1, \u02dcr}, and define\n\nlog (cid:0) 2D\n\u03b4\u2032\n\n(cid:1)\n\nlog (cid:0) 2D\n\u03b4\u2032\n\ndefine cp =\n\nc(p,8)D\n\u221a\nM\nr\n(cid:113)\nlog (cid:0) 2D\nZ :=\n\u03b4\u2032\nfollowing sequence of bounds on cp :\n\nc(p,8)D\n\u221a\nM\n\n\u03d5\n(cid:1). Based on the choice M \u2265\n\n(cid:1) + \u03d5r. Set \u02dcr =\n\n\u221a\n\nM\n\nc(p,4)\n\u22062\n0\n\nlog(2D/\u03b4\u2032), we have Z \u2264 1, yielding the\n\ncp =\n\nZ\nr\n\n+ \u03d5r\n(cid:26) Z\n\u02dcr\n(cid:40)\n\n\u2264 max\n\n+ \u03d5\u02dcr,\n\n(a)\n\u2264 max\n\n2(cid:112)Z\u03d5,\n\n(cid:40)\n\n2(cid:112)Z\u03d5,\n\n= max\n\n(cid:27)\n\n+ \u03d5\u03c1\n\n(cid:41)\n\n+ \u03d5\u02dcr\n\n(cid:41)\n\n+ (cid:112)Z\u03d5\n\nZ\n\u03c1\n\u221a\n\nZ\n\u03c1\n\u221a\n\nZ\n\u03c1\n\n\u221a\n\n\u2264 2\n\nZ max\n\n(cid:26)\n\n(cid:112)\u03d5,\n\n(b)\n\u2264 2 max\n\n(cid:26)\n\n(cid:112)\u03d5,\n\n(cid:27)\n\n1\n\u03c1\n\n(cid:27)\n\n1\n\u03c1\n\n,\n\n(27)\n\n(28)\n\nwhere (a) and (b) follow from Z \u2264 1. Based on the above, we have\n\n\u03b7\u2225g(Kt)\u2225 \u2264 \u03b7(\u2225\u2207C(Kt)\u2225) + \u2225et\u2225\n\n(a)\n\u2264 \u03b7(\u03bb + cp)\n(cid:18)\n(b)\n\u2264 \u03b7\n\n\u03bb + 2 max\n\n(cid:26)\n\n(cid:112)\u03d5,\n\n(cid:27)(cid:19)\n\n,\n\n1\n\u03c1\n\nwhere (a) follows from Lemma 4 and (b) follows from (28). Setting the RHS \u2264 \u03c1 leads to\n(cid:111) which is satisfied by setting \u03b7 = c(p,1). This ensures that\nthe requirement \u03b7 \u2264\n\n\u03c1\n(cid:110)\u221a\n\n\u03bb+2 max\n\n\u03d5, 1\n\u03c1\n\n22\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\n\u2225Kt+1 \u2212 Kt\u2225 \u2264 \u03c1. Using the local smoothness property (Lemma 5), we then have\n\nC(Kt+1) \u2212 C(Kt) \u2264 \u27e8\u2207C(Kt), Kt+1 \u2212 Kt\u27e9 +\n\n\u2225Kt+1 \u2212 Kt\u22252\n\n= \u2212\u03b7\u27e8\u2207C(Kt), g(Kt)\u27e9 +\n\n= \u2212\u03b7\u27e8\u2207C(Kt), \u2207C(Kt) + et\u27e9 +\n\n\u2225\u2207C(Kt) + et\u22252\n\n\u03d5\n2\n\u03d5\u03b72\n2\n\n\u2225g(Kt)\u22252\n\u03d5\u03b72\n2\n\n(a)\n\u2264 \u2212\u03b7\u2225\u2207C(Kt)\u22252 \u2212 \u03b7\u27e8\u2207C(Kt), et\u27e9 + \u03d5\u03b72\u2225\u2207C(Kt)\u22252 + \u03d5\u03b72\u2225et\u22252\n(b)\n\u2264 \u2212\u03b7(1 \u2212 \u03d5\u03b7)\u2225\u2207C(Kt)\u22252 +\n\n\u2225et\u22252 + \u03d5\u03b72\u2225et\u22252\n\n\u03b7\n2\n(1 \u2212 2\u03d5\u03b7)\u2225\u2207C(Kt)\u22252 +\n\n\u03b7\n2\n(1 + 2\u03d5\u03b7)\u2225et\u22252\n\n\u2225\u2207C(Kt)\u22252 +\n\u03b7\n2\n\n= \u2212\n\n(c)\n\u2264 \u2212\n\n\u03b7\n2\n\u03b7\n4\n\n\u2225\u2207C(Kt)\u22252 +\n\n3\u03b7\n4\n\nc2\np.\n\nIn the above, we used \u2225A + B\u22252 \u2264 2\u2225A\u22252 + 2\u2225B\u22252 in (a), and \u22122\u27e8A, B\u27e9 \u2264 \u2225A\u22252 + \u2225B\u22252 in (b)\nwhere A and B are any matrices in Rm\u00d7n. In (c), we used \u03b7 \u2264 1/(4\u03d5) (satisfied by our choice\n\u03b7 = c(p,1)) and \u2225et\u2225 \u2264 cp. Denoting the suboptimality gap as St = C(Kt) \u2212 C(K\u2217), and using the\nPL condition (15) in the above, we obtain the following with probability at least 1 \u2212 \u03b4\u2032:\n\nSt+1 \u2264\n\n\u2264\n\n(cid:16)\n\n(cid:16)\n\n1 \u2212\n\n1 \u2212\n\n(cid:17)\n\n(cid:17)\n\n\u03b7\u00b5\n4\n\u03b7\u00b5\n4\n\nSt +\n\nSt +\n\n3\u03b7\n4\n\u03b7\u00b5\n4\n\nc2\np\n(cid:18) 3\n\u00b5\n\n(cid:19)\n\n.\n\nc2\np\n\n(29)\n\nOn event Et, since we have Kt \u2208 G0\n\nj , St \u2264 10 \u02dc\u22060. Furthermore, due to (27), we have\n\n3\n\u00b5\n\nc2\np \u2264\n\n12\n\u00b5\n\n(cid:18)\n\n(cid:26)\n\n(cid:112)\u03d5,\n\nmax\n\n1\n\u03c1\n\n(cid:27)(cid:19)2 c(p,8)D\n\u221a\nM\n\n(cid:115)\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:19)\n.\n\n(cid:16)\n\n(cid:110)\u221a\n\n(cid:111)(cid:17)2\n\n12c(p,8)\n\u00b5\n\nmax\n\nDefining c(p,9) :=\nby M \u2265\nbased on (29), we have St+1 \u2264 (cid:0)1 \u2212 \u03b7\u00b5\nevent \u02dcEt \u2229 Et, Kt+1 \u2208 G0\n\nc(p,4)\n\u22062\n0\n\nj and\n\n4\n\n\u03d5, 1\n\u03c1\n\nand setting M \u2265\n\n(p,9)D2\nc2\n\u22062\n0\n\nlog (cid:0) 2D\n(cid:1), which is satisfied\n\u03b4\u2032\np \u2264 \u22060 \u2264 10 \u02dc\u22060. Hence,\n\u00b5 c2\n4 10 \u02dc\u22060 \u2264 10 \u02dc\u22060. Therefore, conditioned on the\n\n(cid:1) 10 \u02dc\u22060 + \u03b7\u00b5\n\nlog(2D/\u03b4\u2032) from the statement of Lemma 9, ensures that 3\n\nSt+1 \u2264\n\n(cid:16)\n\n1 \u2212\n\n(cid:17)\n\n\u03b7\u00b5\n4\n\nSt +\n\n\u03b7\u00b5\n4\n\n(cid:32)\n\nc(p,9)D\n\u221a\nM\n\n(cid:115)\n\nlog\n\n(cid:19)(cid:33)\n\n.\n\n(cid:18) 2D\n\u03b4\u2032\n\nNow, let us define Et+1 := \u02dcEt \u2229 Et. We have P( \u02dcEt \u2229 Et) = P( \u02dcEt|Et)P(Et) \u2265 (1 \u2212 \u03b4\u2032)(1 \u2212 \u03b4\u2032t) \u2265\n1 \u2212 \u03b4\u2032(t + 1). This completes the induction step. To prove the statement of Lemma 9, since\n\u03b7 = c(p,1) \u2264 8/\u00b5, for any R \u2265 1, we can unroll the recursion on the event ER which occurs with\n\n23\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nprobability 1 \u2212 \u03b4\u2032R. Doing so, we obtain the following which completes the proof:\n\nSR \u2264\n\n(cid:16)\n\n1 \u2212\n\n(cid:16)\n\n\u2264\n\n1 \u2212\n\n\u03b7\u00b5\n4\n\n\u03b7\u00b5\n4\n\n(cid:17)R\n\nS0 +\n\nR\u22121\n(cid:88)\n\n(cid:16)\n\n1 \u2212\n\n(cid:17)R\n\nS0 +\n\nk=0\n(cid:32)\n\nc(p,9)D\n\u221a\nM\n\n\u03b7\u00b5\n4\n(cid:115)\n\n(cid:17)k \u03b7\u00b5\n4\n\n(cid:32)\n\nc(p,9)D\n\u221a\nM\n\n(cid:115)\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:19)(cid:33)\n\n(cid:19)(cid:33)\n\n.\n\n(cid:18) 2D\n\u03b4\u2032\n\n(30)\n\nlog\n\n24\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nAppendix C. Proof of Theorem 2\n\nWe prove Theorem 2 by conditioning on the event where the claims in (23) hold for all agents in\nevery epoch. In Appendix B, we showed that such an event occurs with probability at least 1 \u2212 \u03b4/2\nand let us denote it by EThm1. Furthermore, under this event, the claims of Theorem 1 hold as shown\nin Appendix B. In particular, the true clusters are always contained in the neighborhood sets and\ncorrect clustering takes place at the latest during the L-th epoch, ensuring that the agents collaborate\nsolely within their own clusters from the (L + 1)-th epoch onward. With that in mind, we consider\nthe following approach to prove Theorem 2. First, we take for granted that the last epoch occurs\nafter the correct clustering takes place, i.e, \u00afL > L, and later show that this is indeed true if the total\nnumber of rollouts T \u2265 \u02dcO(1/\u22062). Next, we show that for any l > L (note that at least one such\nepoch exists in light of \u00afL > L,) the policy at the start of the collaborative policy optimization remains\nin the corresponding restricted domain with high probability, i.e, \u02c6K(l\u22121)\n\u03c3(i). Then, we focus\non the last epoch \u00afL and provide the convergence guarantee, and finally conclude by analyzing the\nnumber of rollouts needed to ensure \u00afL > L.\n\n\u2208 G0\n\nConditioned on the event EThm1, we follow an induction based argument to show that \u02c6K(l\u22121)\n\u2208\n\u03c3(i) for all agents i \u2208 [N ] in every epoch l > L. Let us define \u02dcL as the first epoch where correct\nG0\nclustering takes place. Due to Theorem 1, since the correct clustering takes place at the latest during\nthe Lth epoch, \u02dcL \u2264 L, and moreover, since the neighborhood sets are sequentially pruned with\nno new agents getting added to the neighborhood sets, we have M\u03c3(i) = N (l)\nfor all agents in\ni\nevery epoch l \u2265 \u02dcL. Furthermore, \u02dcL being the first epoch where correct clustering takes place,\ni = M\u03c3(i) for some agent i \u2208 [N ], hence causing reinitializtion as shown in (8). After\nN\nthis reinitialization, the global sequences for all the agents are updated by collaborating within their\nrespective clusters, and hence the global sequences for two agents within a cluster evolve identically\nin light of (6). In other words, for all agents i, j, if M\u03c3(i) = M\u03c3(j), then for all l \u2265 \u02dcL, we have the\nfollowing on the event EThm1:\n\n\u0338= N \u02dcL\n\n\u02dcL\u22121\ni\n\ni\n\ni\n\ni = \u02c6K(l)\n\u02c6K(l)\ni = N (l)\nM\u03c3(i) = N (l)\n\nj\n\nj = M\u03c3(j)\n\n(31)\n\ni\n\nTaking this into account, we show that \u02c6K(l\u22121)\n\u03c3(i) for all agents i \u2208 [N ] in every epoch l > \u02dcL\n\u2208 G0\nvia induction across epochs. For the base case l = \u02dcL + 1, as a consequence of reinitialization during\nthe \u02dcLth epoch, and since X ( \u02dcL)\ni \u2208 G0\n\u03c3(i) for all agents i \u2208 [N ] as a result of conditioning on the event\nEThm1, we have \u02c6K( \u02dcL)\n\u03c3(i) for all agents i \u2208 [N ]. Let us assume that for an epoch l \u2265 \u02dcL + 1,\ni\nwith probability at least (1 \u2212 (cid:80)l\u22121\n\u03b4j\n4 ), we have \u02c6K(t\u22121)\n\u2208 G0\n\u03c3(i) for all agents i \u2208 [N ] and for\nj=1\nall t \u2208 { \u02dcL + 1, \u02dcL + 2, . . . , l}. With a slight abuse of notation, let us denote this event by El\u22121.\nNext, conditioned on the event EThm1 \u2229 El\u22121, we show that \u02c6K(l)\n\u03c3(i) for all agents i \u2208 [N ] with\nprobability at least 1 \u2212 \u03b4l/4.\n\ni \u2208 G0\n\n\u2208 G0\n\ni\n\nIn what follows we fix an agent i \u2208 [N ] and omit the notation i and \u03c3(i) for convenience. Given\nthat K(l\u22121) \u2208 G0 on the event EThm1 \u2229 El\u22121, we focus on analyzing the iterates {Y (k)}0\u2264k<Rl in\nthe lth epoch. The iterates are updated as follows: Y (k+1) = Y (k) \u2212 \u03b7G(Y (k)) with Y (0) = \u02c6K(l\u22121),\nwhere we used G(Y (k)) to denote the averaged zeroth-order gradient estimate as shown in (6) in the\nkth iteration. Note that in the light of Assumption 1, the averaged gradient estimate is an unbiased\n\n25\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nestimate of \u2207Cr(Y (k)) with an effective minibatch size of Ml|M|. Therefore, we follow an approach\nsimilar to the one from the proof of Lemma 9 in Appendix B.1 to analyze the one-step progress and\nto show that Y (Rl) = \u02c6Kl \u2208 G0. More specifically, we follow an induction based approach across\niterations and establish one-step recursion similar to (25) and finally unroll the recursion to obtain\nsomething similar to (30). However, a key difference arises from the fact that the second term in the\nRHS of both (25) and (30) will now enjoy a variance reduction effect due to collaboration in light of\nAssumption 1 as shown in Corollary 8.\n\nIn particular, following the induction approach from the proof of Lemma 9 in Appendix B.1,\nin the kth iteration, we have the following concentration with probability at least (1 \u2212 \u03b4\u2032) after\nconditioning on the event where the previous iterations satisfy similar guarantees as in (25):\n\n\u2225G(Y (k)) \u2212 \u2207Cr(Y (k))\u2225 \u2264\n\nc(p,8)D\nr(cid:112)Ml|M|\n\n(cid:115)\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:19)\n.\n\nConditioned on the event where the gradient estimate is concentrated as above, and defining ek :=\n(cid:1) + \u03d5r following the arguments up to\nG(Y (k)) \u2212 \u2207C(Y (k)), we have \u2225ek\u2225 \u2264\n\nc(p,8)D\n\u221a\n\n(cid:113)\n\nlog (cid:0) 2D\n\u03b4\u2032\n\n(26). Now, let us define cp =\n\nc(p,8)D\n\u221a\n\nMl|M|\n\nr\n\nr\n(cid:113)\n\nMl|M|\nlog (cid:0) 2D\n\u03b4\u2032\n\n(cid:1) + \u03d5r. Setting \u02dcr =\n\n(cid:19)1/2\n\n(cid:18) c(p,8)D\nMl\n\n\u221a\n\n\u03d5\n\n(cid:113)\n\n(cid:1)\n\nlog (cid:0) 2D\n\u03b4\u2032\n(cid:113)\nlog (cid:0) 2D\n\u03b4\u2032\n\nc(p,8)D\n\u221a\nMl\n\n\u02dcr\n\nr = min{\u03c1,\nwhich is ensured by our setting for Ml in Table 2.\n\n|M|1/4 }, we obtain the following bound on cp provided Z :=\n\ncp =\n\nZ\nr(cid:112)|M|\n\n+ \u03d5r\n\n(cid:26) Z\n\n\u02dcr|M|1/4\n\u221a\n\n+ \u03d5\n\n\u02dcr\n|M|1/4\n\u221a\n\n,\n\nZ\n\u03c1|M|1/2\n\n(cid:27)\n\n+ \u03d5\u03c1\n\n(cid:41)\n\n\u02dcr\n|M|1/4\n\n+ \u03d5\n\n\u2264 max\n\n\u2264 max\n\n(cid:40)\n2\n\n\u221a\n\nZ\n|M|1/4\n(cid:26)\n\n\u2264 2 max\n\n\u2264 2\n\nmax\n\n,\n\nZ\u03d5\n|M|1/4\n(cid:26)\n\nZ\n\u03c1|M|1/4\n(cid:27)\n\n(cid:112)\u03d5,\n\n1\n\u03c1\n\n(cid:112)\u03d5,\n\n(cid:27)\n\n.\n\n1\n\u03c1\n\nand\n\n(cid:1) \u2264 1\n\n(32)\n\n(33)\n\nBased on the above, we choose \u03b7 = c(p,1) \u2264\n\n(cid:111) to ensure that \u2225Y (k+1) \u2212 Y (k)\u2225 \u2264 \u03c1.\n\n\u03bb+2 max\nDefining Sk = C(Y (k)) \u2212 C(K\u2217) and following the analysis from Appendix B.1 up to (29) and\nusing the bound on cp from (32), we obtain\n\n\u03c1\n(cid:110)\u221a\n\n\u03d5, 1\n\u03c1\n\nSk+1 \u2264\n\n(cid:32)\n\n1 \u2212\n\n(cid:16)\n\n(cid:124)\n\n(cid:17)\n\n\u03b7\u00b5\n4\n(cid:123)(cid:122)\ns1\n\n+\n\nSk\n(cid:125)\n\n\u03b7\u00b5\n4\n(cid:124)\n\n(cid:115)\n\nlog\n\nc(p,9)D\n(cid:112)Ml|M|\n(cid:123)(cid:122)\ns2\n\n(cid:18) 2D\n\u03b4\u2032\n\n(cid:19)(cid:33)\n\n,\n\n(cid:125)\n\nwith probability at least 1 \u2212 \u03b4\u2032. Note that since |M| \u2265 1, the term s2 is not greater than the\n4 10 \u02dc\u22060. Meanwhile, the term s1 \u2264\ncorresponding term from (25), and hence s2 \u2264 \u03b7\u00b5\n\n4 \u22060 \u2264 \u03b7\u00b5\n\n26\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\n(cid:1) 10 \u02dc\u22060 as we have conditioned on the event where Y (k) \u2208 G0 similar to the proof of\n(cid:0)1 \u2212 \u03b7\u00b5\n4\nLemma 9. This ensures that Y (k+1) \u2208 G0. Now, unrolling the recursion, we have with probability at\nleast 1 \u2212 \u03b4\u2032Rl, Y (Rl) = \u02c6K(l) \u2208 G0 and the following:\n\nC( \u02c6K(l)) \u2212 C(K\u2217) \u2264\n\n(cid:16)\n\n1 \u2212\n\n(cid:17)Rl\n\n\u03b7\u00b5\n4\n\n(C( \u02c6K(l\u22121)) \u2212 C(K\u2217)) +\n\n(cid:32)\n\nc(p,9)D\n(cid:112)Ml|M|\n\n(cid:19)(cid:33)\n\n(cid:115)\n\nlog\n\n(cid:18) 2D\n\u03b4\u2032\n\n.\n\n(34)\n\nSetting \u03b4\u2032 = \u03b4l/(4RlN ) and applying an union bound over all agents, we have the above guarantee\nfor all agents with probability at least 1 \u2212 \u03b4l/4.\n\nLet us denote this event by \u02dcEl. Defining El = \u02dcEl \u2229 El\u22121, we have the following:\n\nP(El|EThm1) = P( \u02dcEl|El\u22121, EThm1)P(El\u22121|EThm1) \u2265 (1 \u2212 \u03b4l/4)\n\n\uf8ed1 \u2212\n\n\uf8eb\n\nl\u22121\n(cid:88)\n\nj=1\n\n\u03b4j\n4\n\n\uf8f6\n\n\uf8eb\n\n\uf8f8 \u2265\n\n\uf8ed1 \u2212\n\n\uf8f6\n\n\uf8f8 .\n\nl\n(cid:88)\n\nj=1\n\n\u03b4j\n4\n\nThis completes the induction argument. Hence, we have established that \u02c6K(l\u22121)\ni\n(cid:16)\n(34) holds for all agents i \u2208 [N ] in every epoch l \u2265 \u02dcL with probability at least\n\n\u2208 G0\n1 \u2212 (cid:80)l\n\n\u03c3(i) and that\n(cid:17)\n\u03b4j\n4\n\nj=1\n\n.\n\nNext, we analyze the final convergence guarantee in the last epoch \u00afL. Conditioned on the event\nE \u00afL\u22121 \u2229 EThm1, we obtain (34) as shown in the following with probability at least 1 \u2212 \u03b4 \u00afL/4 for all\nagents i \u2208 [N ]:\n\nC\u03c3(i)( \u02c6K( \u00afL)\n\ni\n\n) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i)) \u2264\n\n1 \u2212\n\n(cid:17)R \u00afL\n\n\u03b7\u00b5\n4\n\n(cid:16)\n\n(cid:124)\n\n) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i)))\n(cid:125)\n\n(C\u03c3(i)( \u02c6K( \u00afL\u22121)\ni\n(cid:123)(cid:122)\ns1\n(cid:18) 8DN R \u00afL\n\u03b4 \u00afL\n\nlog\n\n(cid:115)\n\n(cid:123)(cid:122)\ns2\n\n(cid:19)(cid:33)\n\n.\n\n(cid:125)\n\n+\n\n(cid:32)\n\n(cid:124)\n\nc(p,9)D\n(cid:112)M \u00afL|M|\n\nIn the above, the settings of R \u00afL and M \u00afL from Table 2 ensures the following:\n4\n\u03b7\u00b5 log\n\n, note that the term\n\n(cid:18) c(p,4)N 10 \u02dc\u22060\n\u22062\n\u00afL\n\n(cid:19)\n\nfrom R \u00afL \u2265\n\n(cid:18)\n\ns1 \u2264 exp\n\n\u2212\n\n(cid:19)\n\n\u03b7\u00b5R \u00afL\n4\n\n(cid:18)\n\nS0 \u2264 exp\n\n\u2212\n\n(cid:19)\n\n\u03b7\u00b5R \u00afL\n4\n\n10 \u02dc\u22060 \u2264\n\n\u22062\n\u00afL\nc(p,4)N\n\n.\n\nUsing M \u00afL =\n\nM \u00afL =\n\nc(p,4)\n\u22062\n\u00afL\n\nlog\n\nc(p,4)\nlog\n\u22062\n\u00afL\n(cid:16) 8DN R \u00afL\n\u03b4 \u00afL\n\n(cid:17)\n\n(cid:16) 8DN R \u00afL\n\u03b4 \u00afL\n\n(cid:17)\n\n\u2265 log\n\nin the above, we have s1 \u2264\n(cid:16) 8DN R \u00afL\n\u03b4 \u00afL\n\n, we have\n\n(cid:17)\n\nlog\n\n(cid:19)\n\n(cid:18) 8DN R \u00afL\n\u03b4 \u00afL\nM \u00afLN\n\n. Furthermore, since\n\n(cid:114)\n\n(cid:16) 8DN R \u00afL\nlog\n\u03b4 \u00afL\n(cid:112)M \u00afLN\n\n(cid:17)\n\n\u2264\n\n(cid:114)\n\n(cid:16) 8DN R \u00afL\nlog\n\u03b4 \u00afL\n(cid:112)M \u00afLN\n\n(cid:17)\n\n\u2264\n\n(cid:114)\n\n(cid:16) 8DN R \u00afL\nlog\n\u03b4 \u00afL\n(cid:112)M \u00afL|M|\n\n(cid:17)\n\n.\n\ns1 \u2264\n\n27\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nTogether with the term s2 we obtain the following with probability at least 1 \u2212 \u03b4 \u00afL/4 for all agents\ni \u2208 [N ]:\n\nC\u03c3(i)( \u02c6K( \u00afL)\n\ni\n\n) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i)) \u2264\n\n2 max{1, c(p,9)D}\n\n(cid:113)\n\nM \u00afL|M\u03c3(i)|\n\n(cid:115)\n\nlog\n\n(cid:18) 8DN R \u00afL\n\u03b4\n\n(cid:19)\n.\n\n(35)\n\nThe above holds on the event E \u00afL conditioned on the event EThm1. We have P(E \u00afL|EThm1) \u2265\n\n(cid:16)\n\n1 \u2212 (cid:80) \u00afL\n\nj=1\n\n\u03b4j\n4\n\n(cid:17)\n\n(cid:16)\n\n=\n\n1 \u2212 (cid:80) \u00afL\n\nj=1\n\n\u03b4\n8j2\n\n(cid:17)\n\n\u2265 1 \u2212 \u03b4/4. Therefore,\n\nP(EE \u00afL \u2229 EThm1) = P(EE \u00afL|EThm1)P(EThm1) \u2265 (1 \u2212 \u03b4/4)(1 \u2212 \u03b4/2) \u2265 1 \u2212 (\u03b4/4 + \u03b4/2) \u2265 1 \u2212 \u03b4.\n\nNote that the guarantee in (35) provides a rate \u02dcO\n\n. It remains to show that\nM \u00afL = \u02dc\u2126(T ). Since \u2206l = \u22060/4l and Rl \u2265 1 for all l \u2208 {1, 2, . . . , \u00afL}, consider the following as\nMl \u2264 T\n\nM \u00afL|M\u03c3(i)|\n\n1/\n\n(cid:16)\n\n(cid:113)\n\n(cid:17)\n\nMl \u2264 T =\u21d2 4l \u2264\n\n(cid:1)\n\nT \u22062\n0\nc(p,10) log (cid:0) 8DN\n\u03b4\nT \u22062\n0\nc(p,10) log (cid:0) 8DN\n\n(cid:32)\n\n\u03b4\n\n(cid:33)\n\n(cid:1)\n\n=\u21d2 l \u2264 log\n\n= log(c(p,11)T ),\n\n(36)\n\nwhere we defined c(p,11) :=\nfollows:\n\n(cid:18)\n\n\u22062\n0\nc(p,10) log( 8DN\n\u03b4 )\n\n(cid:19)\n\n. Now, we use the upper bound on l to bound Rl as\n\nRl =\n\n4\n\u03b7\u00b5\n(cid:32)\n\n(a)\n\u2264 l\n\n(cid:32)\n\n(cid:32)\n\nlog\n\n(cid:32)\n\n4\n\u03b7\u00b5\n\nlog\n\n(cid:33)\n\nc(p,10)N \u02dc\u22062\n0\n\u22062\n0\nc(p,10)N \u02dc\u22062\n0\n\u22062\n0\n\n(cid:32)\n\n(cid:33)\n\n+ l log(4)\n\n(cid:33)\n\n(cid:33)(cid:33)\n\n+ log(4)\n\n(b)\n\u2264 c(p,12) log(c(p,11)T ),\n\n28\n\n\fHARNESSING DATA FROM CLUSTERED LQR SYSTEMS\n\nwhere (a) follows as l \u2265 1, and (b) follows from (36) with c(p,12) := 4\n\u03b7\u00b5\nTherefore, the overall sample complexity has the following bound:\n\n(cid:18)\n\nlog\n\n(cid:18) c(p,10)N \u02dc\u22062\n\u22062\n0\n\n0\n\n(cid:19)\n\n+ log(4)\n\n(cid:19)\n.\n\n\u00afL\n(cid:88)\n\nT =\n\n(2MlRl + Ml)\n\nl=1\n\u00afL\n(cid:88)\n\n(3MlRl)\n\n(a)\n\u2264\n\n(b)\n\u2264\n\nl=1\n\u00afL\n(cid:88)\n\nl=1\n\n3c(p,12) log(c(p,11)T )\n\n= 4c(p,12) log(c(p,11)T )\n\n(cid:33)\n\n(cid:1)\n\n4l\n\n(cid:32)\n\n\u03b4\n\nc(p,10) log (cid:0) 8DN T\n\u22062\n0\nc(p,10) log (cid:0) 8DN T\n\u22062\n0\n\n(cid:33)\n\n(cid:1)\n\n\u03b4\n\n(cid:32)\n\n\u00afL\n4\n\n=\u21d2\n\nT\n\n4c(p,12) log(c(p,11)T )\n\n(cid:18) c(p,10) log( 8DN T\n\u22062\n0\n\n\u03b4\n\n)\n\n(cid:19) \u2264 4\n\n\u00afL.\n\n\uf8fc\n\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\n\n(37)\n\nIn the above, (a) follows as Rl \u2265 1 and (b) as we used Rl \u2264 T in Ml. Using the lower bound\n\non 4 \u00afL as obtained above in M \u00afL, we obtain M \u00afL \u2265\n(35), we have the following with probability 1 \u2212 \u03b4:\n\nT log\n\n(cid:16) 8DN R \u00afL\n\u03b4\n\n(cid:17)\n\n4c(p,12) log(c(p,11)T ) log( 8DN T\n\n\u03b4\n\n. Using this bound in\n\n)\n\nC\u03c3(i)( \u02c6K( \u00afL)\n\ni\n\n) \u2212 C\u03c3(i)(K\u2217\n\n\u03c3(i)) \u2264\n\n=\n\n4D max{1, c(p,9)}\n\n(cid:113)\n\nc(p,12) log(c(p,11)T ) log (cid:0) 8DN T\n(cid:113)\n\n\u03b4\n\nT |M\u03c3(i)|\n\nDc(p,13)\n(cid:113)\n\n(cid:113)\n\nlog (cid:0) 8DN T\n\n\u03b4\n\n(cid:1)\n\n,\n\nT |M\u03c3(i)|\n\n(cid:1)\n\n(38)\n\nwhere we defined c(p,13) := 4 max{1, c(p,9)}\n\n(cid:113)\n\nc(p,12) log(c(p,11)T ).\n\nFinally, it remains to show that \u00afL > L when T \u2265 \u02dcO(1/\u22062). To ensure, \u00afL > L, consider the\nnumber of rollouts required up to (L + 1)th epoch. From (37) with the summation from 1 to L + 1\nwe have:\n\nL+1\n(cid:88)\n\n(2MlRl + Ml) \u2264 4c(p,12) log(c(p,11)T )\n\nl=1\n\n(cid:32)\n\nc(p,10) log (cid:0) 8DN T\n\u22062\n0\n\n\u03b4\n\n(cid:33)\n\n(cid:1)\n\n4L+1.\n\nIn the above, setting T \u2265 RHS to ensure \u00afL > L, we have\n\nT \u2265 4c(p,12) log(c(p,11)T )\n\n(cid:32)\n\n(a)\n\u2265 4c(p,12) log(c(p,11)T )\n\n\u03b4\n\nc(p,10) log (cid:0) 8DN T\n\u22062\n0\nc(p,10) log (cid:0) 8DN T\n\u22062\n0\n\n\u03b4\n\n(cid:32)\n\n(cid:1)\n\n(cid:33)\n\n4L+1\n\n(cid:1)\n\n(cid:33) (cid:18) \u22060\n\u2206\n\n(cid:19)2\n\n.\n\n29\n\n\fKANAKERI BAJAJ VERMA GUPTA MITRA\n\nIn the above, since L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2}, (a) follows from the fact that \u2206L+1 =\n\u22060/(2L+1) \u2264 \u2206/2. Hence, when T \u2265 \u02dcO(1/\u22062), we have \u00afL > L. This completes the proof of\nTheorem 2.\n\nAppendix D. Proof of Corollary 3\n\nIn this section, we analyze the total communication complexity of the PCPO algorithm. In every\nepoch, each agent communicates with the server once in every iteration of the collaborative policy\noptimization subroutine, and once to send the local policy to update the neighborhood sets. Hence,\nthe overall communication complexity is (cid:80) \u00afL\nl=1(Rl + 1) \u2264 (R \u00afL + 1) \u00afL\n\nsince R \u00afL = c(p,2) log\n\n\u2265 c(p,2) log\n\n(cid:19)\n\n(cid:18) 2 \u00afLc(p,3)N\n\u22062\n0\n\nl=1(Rl + 1). Note that (cid:80) \u00afL\n(cid:17)\n(cid:16) 2lc(p,3)N\n\u22062\n0\n\n= Rl.\n\nFrom (37), \u00afL is logarithmic in T . Furthermore, R \u00afL = c(p,2) log\n\n(cid:18) 2 \u00afLc(p,3)N\n\u22062\n0\n\n(cid:19)\n\nis logarithmic in\n\nthe number of agents N and T . Finally, since T \u2265 \u02dcO(1/\u22062), the overall communication complexity\nis logarithmic in T , N and 1/\u2206.\n\n30\n\n\f"
}