{
  "id": "2511.17475",
  "content": "Under consideration for publication in J. Fluid Mech.\n\n1\n\nBanner appropriate to article type will appear here in typeset article\n\n5\n2\n0\n2\n\nv\no\nN\n1\n2\n\n]\nn\ny\nd\n-\nu\nl\nf\n.\ns\nc\ni\ns\ny\nh\np\n[\n\n1\nv\n5\n7\n4\n7\n1\n.\n1\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nAddressing A Posteriori Performance Degradation\nin Neural Network Subgrid Stress Models\n\nAndy Wu1\u2020 and Sanjiva K. Lele1, 2\n1Department of Aeronautics and Astronautics, Stanford University, Stanford, California, USA\n2Department of Mechanical Engineering, Stanford University, Stanford, California, USA\n\n(Received xx; revised xx; accepted xx)\n\nNeural network subgrid stress models often have a priori performance that is far better than\nthe a posteriori performance, leading to neural network models that look very promising a\npriori completely failing in a posteriori Large Eddy Simulations (LES). This performance\ngap can be decreased by combining two different methods, training data augmentation and\nreducing input complexity to the neural network. Augmenting the training data with two\ndifferent filters before training the neural networks has no performance degradation a priori\nas compared to a neural network trained with one filter. A posteriori, neural networks trained\nwith two different filters are far more robust across two different LES codes with different\nnumerical schemes. In addition, by ablating away the higher order terms input into the neural\nnetwork, the a priori versus a posteriori performance changes become less apparent. When\ncombined, neural networks that use both training data augmentation and a less complex set\nof inputs have a posteriori performance far more reflective of their a priori evaluation.\n\nKey words: Authors should not enter keywords on the manuscript\n\n1. Introduction\nLarge Eddy Simulations (LES) provides an economical paradigm for high fidelity flow\nprediction by resolving the larger, energy containing scales and modelling the spatial scales\nof turbulence that are smaller than the grid scale through a subgrid scale (SGS) closure (Sagaut\n2006). Spatially filtering the Navier-Stokes equations results in an unclosed term called the\nsubgrid stress tensor, \ud835\udf0f\ud835\udc56 \ud835\udc57, which represents the effect of the subgrid spatial scales of turbulence\non the resolved flow (Pope 2000).\n\nRecently, with the availability of Direct Numerical Simulation (DNS) data online and\nadvances in deep learning theory, neural networks have been explored as subgrid stress\nmodels (Sarghini et al. 2003; Beck et al. 2019; Park & Choi 2021; Cho et al. 2024; Xie et al.\n2020a,b; Stoffer et al. 2021; Kang et al. 2023; Maejimam & Kawai 2024; Cheng et al. 2022;\nWu & Lele 2025b). A persistent challenge is the discrepancy in neural network a priori and\na posteriori performance (Beck et al. 2019; Beck & Kurz 2021; Stoffer et al. 2021; Park &\n\n\u2020 Email address for correspondence: awu1018@stanford.edu\n\n \n \n \n \n \n \n\f2\n\nChoi 2021), which can be interpreted as a distribution shift between the training data and the\nLES simulation (Hu et al. 2025). Due to this distribution shift, ad-hoc methods are sometimes\nused, such as adding an additional Smagorinsky term to help the neural network dissipate\nenergy so that simulation remains stable (Beck et al. 2019). Two mechanisms may contribute\nto the distribution shift. First, the input fields provided to the neural network during training,\nwhich are generated assuming an explicit filter (oftentimes a box filter), can differ from the\nLES resolved fields, since the LES filter is implicit and is a product of both the grid and the\nnumerical method (Sagaut 2006). Second, the output SGS stress statistics also differ across\nfiltering operations. For example, the omission or use of two-thirds dealiasing in spectral\nmethods modifies the effective filter transfer function, producing different SGS statistics.\nThis motivates the hypothesis that neural networks trained on one filter may struggle to\ngeneralize to LES solvers a posteriori. To address these two mechanisms of distribution\nshift, this work introduces a multi-filter data augmentation strategy that exposes the neural\nnetwork to various plausible filtered inputs and SGS stress distributions.\n\nReinforcement learning approaches (Kim et al. 2022; Bae & Koumoutsakos 2022; Kurz\net al. 2023) aim to bypass these issues by learning the SGS model (or wall model) directly\nin a LES solver. However, models learned in this manner are limited in their transferability\nto other solvers, as they learn the combined effect of solver-specific numerical effects and\nturbulence physics. In contrast, the present supervised learning approach aims to not learn\nsolver peculiarities, and incorporates known filtering operations into the training data. The\nfilters used do not mimic the specific transfer function of any specific LES solver and instead\nare inspired by common LES numerical behavior.\n\nFurthermore, another potential source of distribution shift can come from the use of\nincreasingly complex neural network inputs, such as higher order powers of the velocity\ngradient tensor. For example, the second invariant \ud835\udc44 = 0.5(\ud835\udc432 \u2212 trace(\ud835\udc49 2\n\ud835\udc56 \ud835\udc57))) where \ud835\udc43 =\ntrace(\ud835\udc49\ud835\udc56 \ud835\udc57) and \ud835\udc49\ud835\udc56 \ud835\udc57 is the velocity gradient tensor, and the third invariant \ud835\udc45 = \ud835\udc51\ud835\udc52\ud835\udc61 (\ud835\udc49\ud835\udc56 \ud835\udc57)\nare more sensitive to aliasing and differences in numerical methods (where \ud835\udc51\ud835\udc52\ud835\udc61 denotes\nthe determinant). Therefore, a systematic ablation of neural network input complexity is\nperformed to analyze the effect of inputs to neural network performance a posteriori.\n\nThe contributions of this work are twofold. First, multi-filter data augmentation is shown\nto significantly improve the robustness of learned SGS models across small architecture\nperturbations and across LES solvers with different numerical methods. Second, an ablation\nof input feature complexity reveals that more complex neural network inputs suffer more\nfrom a posteriori performance degradation. Together, these results enable the training of\nneural networks that are far more robust a posteriori, and are more indicative of their a priori\nperformance.\n\n2. Numerical Details\n\n2.1. LES\nLES simulations solve the filtered Navier-Stokes equations. The filtering operation is defined\nbelow:\n\n\u222b\n\n\u00af\ud835\udf12(x, \ud835\udc61) =\n\n\ud835\udc3a (r) \ud835\udf12(x \u2212 r, \ud835\udc61)\ud835\udc51\ud835\udc5f,\n\n\u222b\n\n\ud835\udc3a (r)\ud835\udc51\ud835\udc5f = 1\n\n(2.1)\n\nwhere \ud835\udf12 is the flow variable to be filtered, \ud835\udc3a (r) is the filtering kernel, and \u00af\ud835\udf12 is the filtered\nflow variable. In all sections, the overbar operator is used to denote filtered quantities. For\nrobust subgrid stress modeling, the filtering operation must not be oscillatory in physical\nspace, as this would create additional small scales purely through the act of filtering (Sagaut\n\n\f2006). This means that the spectral cutoff filter, which is in the shape of a sinc function in\nphysical space (while being sharp in spectral space) is not appropriate as a filter for subgrid\nstress modeling. With the definition of the filter above, the LES governing equations are\nwritten below: (in Einstein\u2019s summation notation)\n\n3\n\n\ud835\udf15 \u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc61\n\n+ \u00af\ud835\udc62 \ud835\udc57\n\n= 0\n\n\ud835\udf15 \u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc65\ud835\udc56\n1\n\ud835\udf15 \u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc65 \ud835\udc57\n\ud835\udf0c\n\ud835\udf0f\ud835\udc56 \ud835\udc57 = \ud835\udc62\ud835\udc56\ud835\udc62 \ud835\udc57 \u2212 \u00af\ud835\udc62\ud835\udc56 \u00af\ud835\udc62 \ud835\udc57\n\n\ud835\udf15 \u00af\ud835\udc5d\n\ud835\udf15\ud835\udc65\ud835\udc56\n\n= \u2212\n\n+ \ud835\udf08\n\n\ud835\udf152 \u00af\ud835\udc62\ud835\udc56\n\ud835\udf15\ud835\udc65 \ud835\udc57 \ud835\udf15\ud835\udc65 \ud835\udc57\n\n\u2212\n\n\ud835\udf15\ud835\udf0f\ud835\udc56 \ud835\udc57\n\ud835\udf15\ud835\udc65 \ud835\udc57\n\n(2.2)\n\nwhere \ud835\udc5d is the filtered pressure, \ud835\udf0c is the density, \ud835\udc62 is the filtered velocity, and \ud835\udf08 is the\nkinematic viscosity. As seen, an unclosed term, \ud835\udf0f\ud835\udc56 \ud835\udc57, is obtained by filtering the momentum\nequation. In LES, only the filtered values of the dependent variables are available, thus, \ud835\udf0f\ud835\udc56 \ud835\udc57\ncan\u2019t be computed and has to be modelled. In the present work, the neural network will learn\nto approximate \ud835\udf0f\ud835\udc56 \ud835\udc57 directly.\n\n2.2. A Posteriori Flow Solvers\nSince this work involves drawing conclusions about a posteriori performance, two in-house\nLES flow solvers, PadeOps and PadeLibs (with different numerical methods) are used (Ghate\n& Lele 2017; Song et al. 2024). When running forced homogenous isotropic turbulence\n(HIT), PadeOps is a Fourier-Spectral code with RK4-5 time stepping (Bogacki & Shampine\n1996). Therefore, two-thirds dealiasing is applied due to the non-linear component in the\nconvective acceleration term. Meanwhile, PadeLibs is a 6th order compact code with RK3\ntime stepping (with no 2/3 dealiasing). These two codes have vastly different numerical\nmethods, so the a posteriori conclusions obtained hold for different numerical schemes.\n\n3. Filtering and Neural Network Details\nNeural network SGS models depend on explicitly filtered DNS data, making the choice\nof filters critical. Mismatches between the explicit filters chosen and the implicit filters\nencountered in a posteriori LES can produce both input-side and output-side distribution\nshifts. To reduce such mismatches, the present work adopts multi-filter training, selecting\nexplicit filters to diversify the training distribution so that the filters span representative LES\nfiltering behaviors without approximating a specific LES solver\u2019s transfer function. The first\nexplicit filter chosen is the box filter, as it is a common filter used in previous studies. The\nbox filter is defined as follows:\n\n\u00af\ud835\udf12(x, \ud835\udc61) =\n\n\u222b\n\n\ud835\udc49\n\n\ud835\udf12(x \u2212 r, \ud835\udc61)\n|\ud835\udc49 |\n\n\ud835\udc51r\n\n(3.1)\n\nwhere \ud835\udc49 is the volume (or region) of the box filter, |\ud835\udc49 | is its measure. The box filter is not\n\noscillatory in physical space, but is oscillatory in spectral space.\n\n3.0.1. DSCF: A Custom Localized Low-Pass Filter\nTo approximate a \u201ccleaner\u201d grid-cutoff at the Nyquist wavenumber, a discrete spectral cutoff\nfilter (DSCF) is introduced. The DSCF approximates the spectral cutoff filter and avoids the\noscillatory kernel of an ideal spectral cutoff filter in physical space. To construct this localized\nlow-pass filter kernel, a discrete rectangular transfer function on a uniform frequency grid\nwas constructed, and then the inverse discrete Fourier transform was taken. The discrete\n\n\f4\n\nFigure 1: Characterizations of the DSCF in both physical and spectral space. \ud835\udf14\ud835\udc50 = 2\ud835\udf0b \ud835\udc53\ud835\udc50\ncorresponds to the filter cutoff point, here designated at a 16\u0394\ud835\udc37 \ud835\udc41 \ud835\udc46 filter width, while\n\ud835\udf14\ud835\udc50 = 2\ud835\udf0b \ud835\udc53\ud835\udc5b, \ud835\udc53\ud835\udc5b \u2208 [\u22120.5, 0.5] denotes the normalized angular frequency. Two different\nDSCF versions are shown, one with a 17 and the other with a 33 point support.\n\nfrequency response (from the rectangular transfer function) was specified as\n\n\ud835\udc3a \ud835\udc58 =\n\n(cid:40)1,\n\n|\ud835\udc62\ud835\udc58 | \u2a7d \ud835\udc53\ud835\udc50,\n\n0,\n\notherwise,\n\n(3.2)\n\nwhere \ud835\udc62\ud835\udc58 \u2208 [\u22120.5, 0.5) denotes the normalized discrete frequency and \ud835\udc53\ud835\udc50 is the prescribed\ncutoff. This would be the grid scale cutoff of LES, for example, if one wanted to filter the\nDNS data at a \u0394\ud835\udc3f\ud835\udc38\ud835\udc46 = 16\u0394\ud835\udc37 \ud835\udc41 \ud835\udc46 filter width, then \ud835\udc53\ud835\udc50 = 1\n16 = 0.03125. The corresponding\nspatial filter kernel is obtained using the inverse discrete Fourier transform, followed by a\nnormalization to ensure (cid:205)\ud835\udc5b \ud835\udc54\ud835\udc5b = 1.\n\nThe filtering operation applied to a discrete field \ud835\udf12\ud835\udc5b is then given by the discrete convolution\n\n\ud835\udc41 /2\n\u2211\ufe01\n\n\u00af\ud835\udf12\ud835\udc5b =\n\n\ud835\udc5a=\u2212 \ud835\udc41 /2\n\n\ud835\udc54\ud835\udc5a \ud835\udf12\ud835\udc5b\u2212\ud835\udc5a.\n\n(3.3)\n\nAlthough the construction in (3.2) is formally based on an ideal spectral cutoff filter, the\nuse of a discrete frequency grid and discrete inverse transform produce a spatial kernel that\nis compactly supported and strictly positive in physical space. Both the physical space and\nspectral space characterizations of the DSCF are shown in figure 1. As seen in figure 1, the\nfrequency response is not perfectly rectangular, instead exhibiting a smoother roll-off with\nmild high wavenumber oscillations when compared to a box filter. Meanwhile, in physical\nspace, the filter is strictly positive, non-oscillatory, and has finite support. A drawback to\nDSCF is that achieving good frequency behavior requires roughly twice as many points in the\nphysical space kernel. For example, if filtering at a 16\u0394\ud835\udc37 \ud835\udc41 \ud835\udc46 filter width, a box filter uses 17\npoints, while the DSCF requires the use of 33 points. Using only 17 points causes the DSCF\nto \u201croll off\u201d more slowly, removing less high frequency content. Meanwhile, the 33 point\nDSCF matches the initial decay of the box filter but significantly suppresses the oscillations\n(peak oscillation magnitude of 0.028 vs 0.22), and attenuates frequencies beyond the grid\ncutoff more than the Gaussian filter. As such, if trying to filter at a LES grid cutoff frequency\nof \ud835\udc65\u0394\ud835\udc37 \ud835\udc41 \ud835\udc46, the corresponding DSCF support is always set to 2\ud835\udc65 + 1 (to maintain symmetry\nof the kernel). Training datasets are formed using (i) only the box filter and (ii) both the box\nfilter and the DSCF. Neural networks trained with these datasets are evaluated in PadeLibs,\nwhose implicit filtering properties differ from both the box filter and the DSCF.\n\n3.0.2. Two-Thirds Dealiasing as an Additional Filter\nPadeOps uses a two-thirds dealiasing procedure, a filtering operation that modifies the\neffective transfer function. To study how multi-filter training interacts with this procedure,\na filter is derived that is consistent with two-thirds dealiasing. The total subgrid stress can\n\n\f5\n\nFigure 2: Overall graph neural network architecture. Note that \u201cres. conn.\u201d denotes\nresidual connections.\n\nbe calculated by applying the two-thirds dealiasing operation on top of the already filtered\nNavier-Stokes equations, defined as the tilde operation:\n\n\ud835\udf0f \ud835\udc53\n\ud835\udc56 \ud835\udc57 = (cid:103)\ud835\udc62\ud835\udc56\ud835\udc62 \ud835\udc57 \u2212 \u02dc\u00af\ud835\udc62\ud835\udc56 \u02dc\u00af\ud835\udc62 \ud835\udc57\nAs seen in equation 3.4, the subgrid stress tensor, after accounting for this two-thirds de-\naliasing operation, is denoted as \ud835\udf0f \ud835\udc53\n\ud835\udc56 \ud835\udc57. The two-thirds de-aliasing operation is computed as\nan additional spectral cutoff filter on top of the box filtered data. This filtering operation\nwill be called box and two-thirds (BTF), and is used alongside the box filter to form the\naugmented dataset for PadeOps. Note that the BTF is not intended to exactly replicate the\ntransfer function of PadeOps, and it is unlikely that the transfer function of a fourier-spectral\ncode with 2/3 dealiasing is identical to BTF. Rather, BTF provides the neural network with\nexposure to the type of behavior that two-thirds dealiasing qualitatively introduces, to reduce\nthe training data distribution shift compared to a posteriori PadeOps.\n\n(3.4)\n\n3.1. Neural Network details\nThe neural network used is a hybrid tensor basis neural network (TBNN) from Ling et al.\n(2016) and graph neural network (GNN). In a classic TBNN, an Artificial Neural Network\n(ANN) maps inputs to scalar coefficients of the tensor basis expansion, Here, the ANN front-\nend architecture is substituted with a GNN, which allows for learned input stencils instead of\nprescribed input stencils (Abekawa et al. 2023). To recap, the tensor basis expansion for LES,\nwhere the neural network predicts 8 scalar coefficients \ud835\udc501 \u2212 \ud835\udc508, can be written as (Stallcup\net al. 2022; Wu & Lele 2025b):\n\ud835\udf0f\ud835\udc56 \ud835\udc57 = \ud835\udc501I+\ud835\udc502 \u00afS+\ud835\udc503 \u00afS2+\ud835\udc504 \u00af\ud835\udec02+\ud835\udc505( \u00afS \u00af\ud835\udec0 \u2212 \u00af\ud835\udec0 \u00afS)+\ud835\udc506 \u00af\ud835\udec0 \u00afS \u00afR+\ud835\udc507( \u00afS2 \u00af\ud835\udec0 \u2212 \u00af\ud835\udec0 \u00afS2)+\ud835\udc508( \u00af\ud835\udec0 \u00afS \u00af\ud835\udec02 \u2212 \u00af\ud835\udec02 \u00afS \u00af\ud835\udec0)\n(3.5)\nwhere \u00afS and \u00af\ud835\udec0 denote the filtered strain rate or rotation rate tensor. The invariants, which\nare costly to compute, can be substituted with the velocity gradient tensor with no change in\nneural network accuracy (Wu & Lele 2025a). This is taken one step further, where one can\nuse the invariants of the velocity gradient tensor (\ud835\udc43, \ud835\udc44, \ud835\udc45), as well as the magnitudes of the\nstrain rate and rotation rate tensor as inputs instead (|\ud835\udc46|, |\u03a9|). The neural network architecture\nis shown in figure 2, which has many similarities to Wu & Lele (2025a). As seen, there are\ntwo different inputs to the neural network. The first group of inputs (such as \ud835\udc43, \ud835\udc44, \ud835\udc45) pass\nthrough layers with learnable weights and nonlinear activation functions. In contrast, the\ntensor basis inputs are passed directly to the model without undergoing any nonlinearities\nor learnable transformations that alter their overall structure. Instead, the tensor basis serve\nas an inductive bias for the neural network so that any output of the neural network obeys\nthe structure of \ud835\udf0f\ud835\udc56 \ud835\udc57. This distinction is important since the neural network is more sensitive\nto inputs that go through nonlinear transformations (Novak et al. 2018). Simplifying these\n\n\f6\n\nTable 1: Neural Network Configurations\n\nNumber of Filters\n\nInputs\n\nNeural Network Structure Flow Solver Integrated Normalization\n\nNeural Network Name\n\nOne filter (box)\n\ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9|\nTwo filter (box and BTF)\n\ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9|\nTwo filter (box and DSCF) \ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9|\nOne filter (box)\n\ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9|\nTwo filter (box and BTF)\n\ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9|\nTwo filter (box and DSCF) \ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9|\nOne filter (box)\nTwo filter (box and BTF)\nTwo filter (box and DSCF)\nOne filter (box)\nTwo filter (box and BTF)\nTwo filter (box and DSCF)\nOne filter (box)\nTwo filter (box and BTF)\nTwo filter (box and DSCF)\n\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\ud835\udc43, |\ud835\udc46|, |\u03a9|\n\nOriginal\nOriginal\nOriginal\nAdditional LN\nAdditional LN\nAdditional LN\nOriginal\nOriginal\nOriginal\nAdditional LN\nAdditional LN\nAdditional LN\nOriginal\nAdditional LN\nAdditional LN\n\nPadeOps, Padelibs\nPadeOps\nPadelibs\nPadeOps, Padelibs\nPadeOps\nPadelibs\nPadeOps, Padelibs\nPadeOps\nPadelibs\nPadeOps, Padelibs\nPadeOps\nPadelibs\nPadeOps, Padelibs\nPadeOps\nPadelibs\n\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nGlobal\nLocal\nLocal\nLocal\n\nNN-Box-Complex-Original-G\nNN-BoxBTF-Complex-Original-G\nNN-BoxDSCF-Complex-Original-G\nNN-Box-Complex-ALN-G\nNN-BoxBTF-Complex-ALN-G\nNN-BoxDSCF-Complex-ALN-G\nNN-Box-Simple-Original-G\nNN-BoxBTF-Simple-Original-G\nNN-BoxDSCF-Simple-Original-G\nNN-Box-Simple-ALN-G\nNN-BoxBTF-Simple-ALN-G\nNN-BoxDSCF-Simple-ALN-G\nNN-Box-Simple-Original-L\nNN-BoxBTF-Simple-ALN-L\nNN-BoxDSCF-Simple-ALN-L\n\ninputs (less higher powers of the velocity gradient tensor) would reduce their susceptibility\nto distribution shifts, leading to better a posteriori performance. To investigate this, neural\nnetworks are trained with two different sets of inputs, \ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9| (\u201ccomplex inputs\u201d) and\n\ud835\udc43, |\ud835\udc46|, |\u03a9| (\u201csimple inputs\u201d). The simpler set is hypothesized to perform better a posteriori.\nThe neural network model inputs and outputs should be dimensionless. A global normal-\n\nization (max-min normalization) is used for the input fields:\n\n\ud835\udc65\ud835\udc5b\ud835\udc5c\ud835\udc5f \ud835\udc5a =\n\n\ud835\udc65 \u2212 \ud835\udc65\ud835\udc5a\ud835\udc56\ud835\udc5b\n\ud835\udc65\ud835\udc5a\ud835\udc4e\ud835\udc65 \u2212 \ud835\udc65\ud835\udc5a\ud835\udc56\ud835\udc5b\n\n\u2217 2 \u2212 1\n\n(3.6)\n\nwhile a local normalization is adopted for the tensor basis and the subgrid stress tensor:\n\n\ud835\udc4a \ud835\udc5b\ud835\udc5c\ud835\udc5f \ud835\udc5a\n\ud835\udc56 \ud835\udc57\n\n=\n\n\ud835\udc4a\ud835\udc56 \ud835\udc57\n|\ud835\udc49\ud835\udc56 \ud835\udc57 | \ud835\udc65\n\n, \ud835\udf0f\u2217\n\n\ud835\udc56 \ud835\udc57 =\n\n\ud835\udf0f\ud835\udc56 \ud835\udc57\n\u03942\n\ud835\udc3f\ud835\udc38\ud835\udc46 |\ud835\udc49\ud835\udc56 \ud835\udc57 |2\n\n(3.7)\n\nwhere \ud835\udc4a\ud835\udc56 \ud835\udc57 is the tensor to be normalized, \ud835\udc49\ud835\udc56 \ud835\udc57 denotes the velocity gradient tensor, |\ud835\udc49\ud835\udc56 \ud835\udc57 | its\nmagnitude, \ud835\udc65 the exponent to ensure dimensionless-ness, \u0394\ud835\udc3f\ud835\udc38\ud835\udc46 corresponds to the LES grid\nspacing, and repeated indices do not imply summation.\n\nTo ensure that the results are consistent across slight perturbations to the neural network\narchitecture, a variant architecture is introduced with additional layer normalization opera-\ntions between select GNN layers and the first 1x1 convolution layer. Input normalization is\nalso varied, using local normalization. Training follows (Wu & Lele 2025b) with a composite\nloss function that minimizes the root mean squared error between the predicted and actual\nsubgrid stress tensor (denoted as non-bolded RMSE), and the RMSE between the predicted\nand actual energy dissipation, \ud835\udf16 = \u2212\ud835\udf0f\ud835\udc56 \ud835\udc57\n\n\u00af\ud835\udc46\ud835\udc56 \ud835\udc57 (denoted as non-bolded DRMSE):\n\n\ud835\udc3f = RMSE(\ud835\udf0f\ud835\udc56 \ud835\udc57, \ud835\udc5d\ud835\udc5f \ud835\udc52\ud835\udc51 \u2212 \ud835\udf0f\ud835\udc56 \ud835\udc57)/RMS(\ud835\udf0f\ud835\udc56 \ud835\udc57) + RMSE(\ud835\udf16 \ud835\udc5d\ud835\udc5f \ud835\udc52\ud835\udc51 \u2212 \ud835\udf16)/RMS(\ud835\udf16)\nwhere RMSE is the root-mean squared error operation and RMS is the root-mean square\n\n(3.8)\n\noperation.\n\n3.2. All Neural Network Configurations\nA summary of all the neural networks trained are seen in table 1. Neural networks are\ntrained with different training data (one filter versus augmented training data), different\ninputs (complex versus simple), and slightly different architectures (more or less layer\nnormalization). All neural networks are trained on forced HIT at a filter width of 16\u0394\ud835\udc37 \ud835\udc41 \ud835\udc46.\n\n\f7\n\nTable 2: A priori testing, each value is given as mean (standard deviation).\n\nNeural Network\n\nRMSE\n\nDRMSE\n\nCorrelation\n\nNN-Box-Complex-Original-G\n0.751 (0.017)\n0.772 (0.017)\nNN-BoxBTF-Complex-Original-G\nNN-BoxDSCF-Complex-Original-G 0.755 (0.017)\n0.750 (0.017)\nNN-Box-Complex-ALN-G\n0.753 (0.017)\nNN-BoxBTF-Complex-ALN-G\n0.758 (0.017)\nNN-BoxDSCF-Complex-ALN-G\n0.751 (0.017)\nNN-Box-Simple-Original-G\n0.758 (0.017)\nNN-BoxBTF-Simple-Original-G\n0.757 (0.017)\nNN-BoxDSCF-Simple-Original-G\n\n0.664 (0.026)\n0.693 (0.028)\n0.675 (0.028)\n0.661 (0.026)\n0.664 (0.026)\n0.675 (0.028)\n0.662 (0.024)\n0.668 (0.028)\n0.672 (0.027)\n\n0.660 (0.020)\n0.636 (0.020)\n0.660 (0.020)\n0.661 (0.020)\n0.659 (0.021)\n0.660 (0.021)\n0.660 (0.020)\n0.657 (0.021)\n0.661 (0.020)\n\n4. A Priori Analysis\n\n4.1. One Filter versus Two Filter Results\nAs seen in table 2, where the neural networks are evaluated on the test set box filtered data,\nthere is no a priori performance degradation when training on two filters versus training on\none filter since all RMSE, DRMSE, and correlation coefficient values are within 5 percent\nof each other. For example, neural networks trained with a max-min normalization and\nusing the set of inputs \ud835\udc43, \ud835\udc44, \ud835\udc45, |\ud835\udc46|, |\u03a9| have all have RMSE values in the 0.75-0.78 range,\nDRMSE values in the 0.66-0.70 range, and correlation coefficients in the 0.63-0.66 range.\nAlso, using a complex or simple set of inputs has negligible difference on the neural network\nperformance, suggesting that just using |\ud835\udc46|, |\u03a9| is sufficient (\ud835\udc43 = 0 for incompressible flow,\nand is manually masked to be zero to prevent the neural network from overfitting to noise. It\nis kept so that the model can generalize to compressibility in future work).\n\n5. A Posteriori Analysis\nThe neural networks are evaluated on two different flow solvers running forced HIT\non a 64 \u00d7 64 \u00d7 64 grid (corresponding to 16\u0394\ud835\udc37 \ud835\udc41 \ud835\udc46) and a Taylor Reynolds number of\n820. A quantitative metric is also given, sum spectral error (\ud835\udc46\ud835\udc46\ud835\udc38 = (cid:205)\ud835\udc58 |(\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc38\ud835\udc37 \ud835\udc41 \ud835\udc46\ud835\udc58 ) \u2212\n\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc38\ud835\udc3f\ud835\udc38\ud835\udc46\ud835\udc58 ))|, \ud835\udc58 \ud835\udc43\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc3f\ud835\udc56\ud835\udc4f\ud835\udc60 \u2208 [0, 35], \ud835\udc58 \ud835\udc43\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc42 \ud835\udc5d\ud835\udc60 \u2208 [0, 24]), where \ud835\udc38 (\ud835\udc58) denotes the energy\nspectra, with differences in \u201ck\u201d since PadeOps has 2/3 dealiasing. While SSE is a good\nfirst-order metric, it must be interpreted in a physical context. SSE is computed using the\nlogarithm of the energy spectra values, rendering it artificially sensitive to deviations in\nthe higher-wavenumber, dissipative range. This can manifest in SSE oftentimes assigning a\nlarger SSE value to dissipative spectra even if it is physically desirable, while not putting\nenough emphasis on high-wavenumber energy buildup. Therefore, visual inspection of the\nspectral decay or pile-up is still essential.\n\n5.1. PadeOps\nThe neural networks are integrated into PadeOps, and after running to statistical stationarity,\nthe spectra are shown below in figures 3a-3c. One can see that training with two filters\nsignificantly increases the robustness of the neural network a posteriori, as the spectra for\nneural networks trained with only one filter vary significantly in figures 3a-3b for small\nperturbations in the neural network architecture. Note that as seen in the a priori analysis\n\n\f8\n\n(a)\n\n(b)\n\n(c)\n\nFigure 3: A posteriori PadeOps HIT Spectra, SSE is given in the legend (lower the better)\n\n(a)\n\n(b)\n\n(c)\n\nFigure 4: A posteriori Padelibs HIT Spectra, SSE is given in the legend (lower the better)\n\nsection, all neural networks have similar a priori performance, but the a posteriori SSE for\nneural networks trained with one filter can vary by a factor of 4. When considering fig. 3c,\nusing a simpler set of inputs also significantly helps reduce the a posteriori variability across\nvarious neural network architectures and normalizations, which is also seen in the SSE. Note\nthat in situations where the one filter approach performs well, the two filter approach also\nperforms well, while if the one filter approach performs poorly, oftentimes the two filter\napproach performs better.\n\n5.2. PadeLibs\nThe neural network a posteriori results for PadeLibs, after running to statistical stationarity,\nare shown in figures 4a-4c to investigate if the results hold for a different flow solver with\ndifferent numerics. From figure 4a and figure 4b, one can see the same trend: training\nwith two filters significantly increases the robustness of the neural network a posteriori, as\ntraining with one filter has very large variance for slight perturbations of neural network\narchitectures, and has large SSE values even though a priori performance of the neural\nnetworks are similar. The same trend holds for figure 4c. Using the simpler set of inputs leads\nto more consistent a posteriori performance across different neural network normalizations\nand architectures. Models trained with one filter that perform well also show good results\nwhen trained on two filters, with a posteriori SSE values more similar to one another,\nconsistent with a priori results. This suggests that using the simpler input set results in\nless distribution shift. The robustness seen likely stems from the simpler input set having\nless numerical error amplification. Numerical error and aliasing often manifest as high-\nwavenumber perturbations, which are often amplified when taking higher powers of the\nvelocity gradient tensor. Thus, the \u201ccomplex\u201d input set suffers from a larger distribution\nshift. Even when the normalization is changed or the neural network architecture changes\nslightly, the simpler input set retains a reasonable spectra shape, with consistent results\nbetween one filter versus two filter cases. In general, when models trained with one filter\n\n\f9\n\nperform poorly, their two filter counterparts perform better, while if models trained on one\nfilter perform well, their two filter counterparts also perform well.\n\n5.3. Combined A Posteriori Analysis\nOverall, combining data augmentation and simpler inputs yield robust a posteriori perfor-\nmance across solvers with different numerical methods. Training with two filters allows\nthe neural network to see more diverse input and output distributions, leading to reduced\ndistribution shift a posteriori. This allows the neural network, even though it is trained on\ngeneric filters not exactly mimicking the LES implicit transfer function, to perform more\naccurately to unseen implicit filters as it is not overfitting to the kernel artifacts associated with\none specific filter (e.g. Gibbs Oscillations). Meanwhile, the \u201csimple\u201d training set outperforms\nthe more complex training set because it is able to reduce the input distribution shift, as higher\norder powers of the velocity gradient tensor suffer more from numerical artifacts such as\naliasing, especially at the grid cutoff resolution. By relying on the lower-order terms, the\n\u201csimple\u201d inputs allow the neural network to be less exposed to and not overfit to high-\nwavenumber numerical artifacts, increasing the a posteriori robustness of the neural network\nmodel.\n\n6. Conclusion\nTwo methods to increase the a posteriori robustness of neural networks are proposed, data\naugmentation and decreasing the complexity of neural network inputs. Data augmentation\ninvolves training with multiple filters in the training data, and two filters have been proposed.\nBTF accounts for two-thirds dealiasing as an additional filter, whereas DSCF is non-\noscillatory in physical space while approximating a sharp spectral cutoff filter in spectral\nspace. A priori, neural networks suffer no performance degradation when trained on one filter\nas compared to two filters, suggesting that neural network models are able to distinguish\nbetween various filters. A posteriori, neural network models trained with two filters are\nsignificantly more robust than neural networks trained with one filter, and this trend is seen\nacross two different LES flow solvers. Furthermore, neural network models with less complex\ninputs perform better, significantly reducing the distribution shift a posteriori. Training with\ntwo different filters does not reduce the performance of a neural network subgrid stress model\neither a priori or a posteriori. These trends hold across small neural network architecture\nperturbations and input normalizations. Training with data filtered with two different filters\nand also using less complex inputs to neural networks significantly increases the robustness\nof neural networks a posteriori.\nAcknowledgements. This work used CPU and GPU resources via Bridges2 at the Pittsburgh Supercomputing\nCenter through allocation PHY230025 from the Advanced Cyberinfrastructure Coordination Ecosystem:\nServices & Support (ACCESS) program, which is supported by National Science Foundation grants\n#2138259, #2138286, #2138307, #2137603, and #2138296. DNS data is provided by the Johns Hopkins\nTurbulence Database.\n\nFunding. Andy Wu is partially supported by NASA Cooperative Agreement number 80NSSC22M0108 and\nNorthrop Grumman, as well as the NDSEG Fellowship.\n\nDeclaration of interests. Declaration of Interests. The authors report no conflict of interest.\n\nAbekawa, A, Minamoto, Y, Osawa, K, Shimamoto, H & Tanahashi, M 2023 Exploration of robust\n\nmachine learning strategy for subgrid scale stress modeling. Physics of Fluids 35 (1).\n\nREFERENCES\n\n\f10\n\nBae, HJ & Koumoutsakos, P 2022 Scientific multi-agent reinforcement learning for wall-models of\n\nturbulent flows. Nature Communications 13 (1), 1443.\n\nBeck, A, Flad, D & Munz, C-D 2019 Deep neural networks for data-driven LES closure models. Journal\n\nof Computational Physics 398 (108910).\n\nBeck, A & Kurz, M 2021 A Perspective on Machine Learning Methods in Turbulence Modelling. Surveys\n\nfor Applied Mathematics and Mechanics 44 (1).\n\nBogacki, P & Shampine, LF 1996 An efficient Runge-Kutta (4,5) pair. Computers & Mathematics with\n\nApplications 32 (6), 15\u201328.\n\nCheng, Y, Giometto, MG, Kauffmann, P, Lin, L, Cao, C, Zupnick, C, Li, H, Li, Q, Huang, Y,\nAbernathey, R & others 2022 Deep learning for subgrid-scale turbulence modeling in large-\neddy simulations of the convective atmospheric boundary layer. Journal of Advances in Modeling\nEarth Systems 14 (5), e2021MS002847.\n\nCho, C, Park, J & Choi, H 2024 A recursive neural-network-based subgrid-scale model for large eddy\n\nsimulation: application to homogeneous isotropic turbulence. Journal of Fluid Mechanics 1000.\n\nGhate, AS & Lele, SK 2017 Subfilter-scale Enrichment of Planetary Boundary Layer Large Eddy\n\nSimulation Using Discrete Fourier\u2013Gabor modes. Journal of Fluid Mechanics 819.\n\nHu, Z, Subramaniam, A, Kuang, Z, Lin, J, Yu, S, Hannah, WM, Brenowitz, ND, Romero, J\n& Pritchard, MS 2025 Stable machine-learning parameterization of subgrid processes in a\ncomprehensive atmospheric model learned from embedded convection-permitting simulations,\narXiv: 2407.00124.\n\nKang, M, Jeon, Y & You, D 2023 Neural-network-based mixed subgrid-scale model for turbulent flow.\n\nJournal of Fluid Mechanics 962.\n\nKim, J, Kim, H, Kim, J & Lee, C 2022 Deep reinforcement learning for large-eddy simulation modeling in\n\nwall-bounded turbulence. Physics of Fluids 34 (10).\n\nKurz, M, Offenh \u00a8auser, P & Beck, A 2023 Deep reinforcement learning for turbulence modeling in large\n\neddy simulations. International journal of heat and fluid flow 99, 109094.\n\nLing, J, Kurzawski, A & Templeton, J 2016 Reynolds averaged turbulence modelling using deep neural\n\nnetworks with embedded invariance. Journal of Fluid Mechanics 807, 155\u2013166.\n\nMaejimam, S & Kawai, S 2024 Coarse-grid large-eddy simulation by unsupervised-learning-based sub-grid\n\nscale modeling. In AIAA SciTech 2024 Forum, pp. AIAA 2024\u20131361.\n\nNovak, R, Bahri, Y, Abolafia, DA, Pennington, J & Sohl-Dickstein, J 2018 Sensitivity and\n\ngeneralization in neural networks: an empirical study, arXiv: 1802.08760.\n\nPark, J & Choi, H 2021 Toward neural-network-based large eddy simulation: application to turbulent\n\nchannel flow. Journal of Fluid Mechanics 914.\nPope, SB 2000 Turbulent Flows, 1st edn. Cambridge University.\nSagaut, P 2006 Large Eddy Simulation for Incompressible Flows: An Introduction, 3rd edn. Springer\n\nScience & Business Media.\n\nSarghini, F, De Felice, G & Santini, S 2003 Neural networks based subgrid scale modeling in large eddy\n\nsimulations. Computers & Fluids 32 (1), 97\u2013108.\n\nSong, H, Ghate, AS, Matsuno, KV, West, JR, Subramaniam, A & Lele, SK 2024 A robust compact\nfinite difference framework for simulations of compressible turbulent flows. Journal of Computational\nPhysics 519, 113419.\n\nStallcup, EW, Kshitij, A & Dahm, WJ 2022 Adaptive Scale-Similar Closure for Large Eddy Simulations.\n\nPart 1: Subgrid Stress Closure. In AIAA SciTech, pp. AIAA 2022\u20130595. San Diego, CA: AIAA.\n\nStoffer, R, Leeuwen, CM, Podareanu, D, Codreanu, V, Veerman, MA, Janssens, M, Hartogensis,\nOK & Heerwaarden, CC 2021 Development of a large-eddy simulation subgrid model based on\nartificial neural networks: a case study of turbulent channel flow. Geoscientific Model Development .\nWu, A & Lele, SK 2025a Subgrid stress modelling with multi-dimensional state space sequence models,\n\narXiv: 2511.10910.\n\nWu, A & Lele, SK 2025b Two neural network unet architecture for subfilter stress modeling. Physical\n\nReview Fluids 10 (1), 014601.\n\nXie, C, Wang, J & Weinan, E 2020a Modeling subgrid-scale forces by spatial artificial neural networks in\n\nlarge eddy simulation of turbulence. Physical Review of Fluids 5 (5).\n\nXie, C, Yuan, Z & Wang, J 2020b Artificial neural network-based nonlinear algebraic models for large\n\neddy simulation of turbulence. Physical of Fluids 32 (11).\n\n\f"
}