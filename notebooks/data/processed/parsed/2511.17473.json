{
  "id": "2511.17473",
  "content": "MASKED-AND-REORDERED SELF-SUPERVISION FOR\nREINFORCEMENT LEARNING FROM VERIFIABLE REWARDS\n\n5\n2\n0\n2\n\nv\no\nN\n1\n2\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n3\n7\n4\n7\n1\n.\n1\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nZhen Wang\u2217\nDP Technology\n\nZhifeng Gao\nDP Technology\n\nGuolin Ke\nDP Technology\n\nNovember 24, 2025\n\nABSTRACT\n\nTest-time scaling has been shown to substantially improve large language models\u2019 (LLMs) mathemat-\nical reasoning. However, for a large portion of mathematical corpora, especially theorem proving,\nRLVR\u2019s scalability is limited: intermediate reasoning is crucial, while final answers are difficult to\ndirectly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization\nrather than inducing longer chains of thought. Inspired by BERT\u2019s self-supervised tasks, we propose\nMR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards\nvia \u201cmasked-then-fill\u201d and \u201cstep reordering\u201d to extract learnable signals from intermediate reasoning.\nOur training pipeline comprises two stages: we first perform self-supervised training on sampled\nmathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical cal-\nculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B\nand DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500.\nUnder a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the\noriginal RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate\nthat incorporating process-aware self-supervised signals can effectively enhance RLVR\u2019s scalability\nand performance in only outcome-verifiable settings.\n\n1\n\nIntroduction\n\nLarge language models (LLMs) have recently made rapid progress on mathematical and scientific reasoning tasks,\ndriven by techniques such as chain-of-thought prompting, diverse sampling, and test-time scaling. Reinforcement\nlearning (RL) has further improved performance on reasoning-intensive tasks including mathematical problem solving,\ncode generation, and program synthesis [Yang et al., 2025, Guo et al., 2025]. A central question in these settings\nis how to design reward signals that guide models toward reliable and generalizable reasoning strategies. Verifiable\nrewards obtained by programmatically checking final answers or executing unit tests offer a practical solution: whether\na model output satisfies predefined symbolic or numeric constraints can often be determined automatically, providing\nscalable and low-cost supervision for RL [Guo et al., 2024, Yang et al., 2024]. Reinforcement Learning from Verifiable\nRewards (RLVR) [Shao et al., 2024] instantiates this idea by directly optimizing policies to pass symbolic or numerical\nchecks at the level of final answers, and has shown strong performance on code generation and mathematical reasoning\ntasks [Shao et al., 2024, Guo et al., 2025].\n\nHowever, terminally verifiable rewards primarily constrain the final answer. To further improve complex multi-step\nreasoning, it is crucial yet challenging to construct equally informative training signals for the intermediate reasoning\nprocess. One line of work explicitly leverages intermediate steps: process supervision and Process Reward Model\n(PRM) frameworks [Lightman et al., 2023, Guan et al., 2025] provide step-level supervision by scoring or classifying\nintermediate steps, improving stability and interpretability in mathematical reasoning. These approaches, however,\ntypically require large-scale, high-quality human annotations, and in complex theorem-proving scenarios it is inherently\ndifficult to decide whether a local step is reasonable. This leads to high annotation costs and limits scalability to diverse\nlarge-scale corpora. In addition, token-level supervised fine-tuning (SFT) on chain-of-thought data often degenerates\n\n\u2217Corresponding author. wangz@dp.tech\n\n \n \n \n \n \n \n\fA PREPRINT - NOVEMBER 24, 2025\n\ninto imitating specific solution templates rather than learning transferable reasoning strategies [Lightman et al., 2023].\nHuman-centric supervision thus struggles to simultaneously achieve low annotation cost, scalability, and transferability\nof reasoning ability.\n\nRLVR removes the need for step-level labels but provides only weak constraints on the intermediate reasoning trajectory.\nAs a result, it is susceptible to process hallucinations: the model may generate plausible-looking yet incorrect or\nredundant reasoning steps, and such errors are often difficult to detect and correct using only terminal verification. In\nmany mathematical and theorem-proving datasets, correctness at the step level is important in its own right, yet hard\nto verify via a unified, low-cost programmatic procedure. Complementary to process supervision, recent work has\nexplored self-supervised signals or existing trajectories as denser rewards for reasoning, for example by using model\nconfidence in reference answers as intrinsic rewards or by designing next-segment reasoning objectives from expert\ntraces and pretraining corpora and converting them into RL training signals [Yu et al., 2025, Li et al., 2025, Deng et al.,\n2025]. These approaches mitigate reward sparsity without additional human annotations or domain-specific verifiers,\nsuggesting that combining self-supervision with RL is a promising direction for strengthening multi-step reasoning.\n\nWe ask whether it is possible to extract process-level signals directly from existing mathematical reasoning trajectories\nand convert them into verifiable rewards that are compatible with RLVR, without relying on additional human process\nannotations or explicit expert action sequences. Our starting point is the observation that self-supervised destruction\u2013\nreconstruction objectives, such as those used in BERT[Devlin et al., 2019, Raffel et al., 2023], help models capture\nsemantic and structural dependencies within context by masking and reconstructing missing spans. This property\nnaturally aligns with modeling the constraints and dependencies between steps in multi-step reasoning.\n\nIn this work, we propose MR-RLVR (Masked-and-Reordered RLVR), which augments RLVR with dense, structured\nprocess-level self-supervision derived from mathematical reasoning trajectories. Under a setting where only terminal\nrewards are externally verifiable, we construct internal process rewards that can be computed automatically and\nintegrated into RL training. Concretely, we design two process-aware tasks on proof-style and computation-style\ntrajectories: (1) Masked-Then-Fill, which masks key formulas, reasoning steps, or theorem invocations and requires the\nmodel to reconstruct the missing content given surrounding context; and (2) Step Reordering, which shuffles reasoning\nsteps and asks the model to recover a coherent logical order. For both tasks, we define process-level rewards based on\nthe match between generated spans and reference reasoning at the levels of mathematical entities and text, and on the\nagreement between predicted and reference step orders. These rewards can be computed automatically from existing\ntrajectories and used directly as RL signals, without any additional human annotation.\n\nMR-RLVR adopts a simple two-stage training framework. In Stage I, we use only the process-level rewards described\nabove to perform RLVR updates on mathematical corpora that jointly cover proof-style and computation-style reasoning,\nencouraging the policy to produce reasoning processes with more coherent local logic and clearer step dependencies.\nIn Stage II, starting from the Stage I checkpoint, we fine-tune the model on computational math problems with\nprogrammatically verifiable final answers, using only terminally verifiable rewards. The model first learns better\nreasoning structure under dense process-level signals, and then adapts to verifiable tasks under sparse but precise\nterminal supervision. This combination aims to improve the stability and scalability of RLVR on complex mathematical\nreasoning tasks without increasing human annotation cost, and to reduce process hallucinations.\n\nOur contributions are summarized as follows:\n\n1. Process-level self-supervision as verifiable rewards. We propose a framework that designs Mask-Then-Fill\nand Step Reordering tasks on mathematical reasoning trajectories and converts their outcomes into process-\nlevel rewards based on mathematical-entity matching and ordering consistency. This allows RLVR to receive\nfine-grained process supervision without any human process annotations.\n\n2. Two-stage MR-RLVR training under terminal-verifiable supervision. We introduce a two-stage training\nprocedure that first performs RLVR pretraining with process-level rewards on diverse proof and computational\nreasoning corpora, and then applies outcome-level RLVR fine-tuning on computational problems with verifiable\nfinal answers, alleviating exploration difficulties under sparse rewards.\n\n3. Empirical gains and data efficiency on mathematical reasoning benchmarks. On Qwen2.5-3B and\nDeepSeek-R1-Distill-Qwen-1.5B, MR-RLVR consistently outperforms a GRPO baseline on AIME24,\nAIME25, AMC23, and MATH500, achieving an average relative improvement of about 9.86% in Pass@1,\n5.27% in Pass@5 and 4.00% in Pass@8 under a fixed sampling budget. In low-data regimes, MR-RLVR also\nexhibits better sample efficiency than standard RLVR.\n\n2\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\n2 Related work\n\nPRM and RLVR for math reasoning. From the perspective of training signals, large-model training for mathematical\nreasoning typically follows two lines: process supervision and outcome-verifiable rewards. The former is exemplified\nby Process Reward Models (PRMs) combined with MCTS-style search, which score intermediate steps and use step-\nlevel value estimates to guide tree expansion and pruning, thereby achieving stable, interpretable, and search-capable\nreinforcement reasoning on math and code tasks [Lightman et al., 2023, Wang et al., 2024, Zhang et al., 2024, Guan et al.,\n2025]. However, such methods require expensive step-level annotations (or large-model scoring) as well as additional\nsearch infrastructure. In contrast, the RLVR line avoids explicit step labels and relies solely on programmable outcome\nverifiers that score final answers or executable code [Shao et al., 2024]. On automatically gradable benchmarks such\nas AIME, AMC, and MATH, systems including DeepSeek-Math, and Qwen2.5-Math have demonstrated substantial\ngains in mathematical and code reasoning under this paradigm [Guo et al., 2024, Shao et al., 2024, Guo et al., 2025,\nYang et al., 2024, 2025]. Nevertheless, standard RLVR imposes almost no constraints on the intermediate reasoning\nprocess and is thus prone to process hallucinations and redundant steps. In comparison, MR-RLVR preserves the RLVR\nassumption of relying only on outcome-verifiable rewards, without introducing external PRMs or human step labels;\ninstead, it automatically constructs self-supervised tasks such as masked reconstruction and step permutation on existing\nmathematical reasoning trajectories, converts their completion quality into process-level rewards usable by RL, and\ncombines them with a second-stage RLVR training based on outcome rewards.\n\nSelf-supervised process signals for reasoning tasks. A complementary line of work explores self-supervised process\nsignals for reasoning tasks. ClozeMath adapts text-infilling and PrefixLM objectives to the mathematical setting\nby masking intermediate equations during supervised fine-tuning and requiring the model to recover them, thereby\nstrengthening the modeling of key mathematical entities and local structure; however, its self-supervised signal is\nonly used as an SFT loss and is not explicitly converted into RL rewards [Pham et al., 2025]. RLPR, RLPT, and\nSRL instead embed self-supervised signals into reinforcement learning or preference-optimization frameworks: RLPR\nuses the model\u2019s (relative) generation probability of a reference answer as an intrinsic reward, extending RLVR to\ngeneral domains without human scoring [Yu et al., 2025]; RLPT defines a next-chunk prediction objective on large-\nscale unlabeled text and employs an auxiliary model to score semantic consistency between the prediction and the\nground-truth continuation as a reward [Li et al., 2025]; SRL decomposes expert solutions into sequences of actions\nand uses action-level similarity as rewards to guide stepwise imitation of expert trajectories [Deng et al., 2025]. These\nworks demonstrate that probabilities, semantic consistency, and action similarity can all serve as effective intrinsic\nrewards, but their reward designs are mostly centered on the consistency or similarity of whole answers or relatively\nlong segments, with limited specialization for key entities, local logical dependencies, and step ordering in mathematical\nreasoning. MR-RLVR instead directly designs fine-grained, structure-aware self-supervised tasks (masked refilling and\nstep permutation) on mathematical reasoning trajectories and converts task outcomes into automatically computable\nprocess-level rewards that integrate seamlessly into an RLVR pipeline.\n\n3 Preliminaries\n\nA large language model (LLM) can be regarded as a conditional probabilistic model \u03c0\u03b8(y, z | x), where x \u2208 X\ndenotes the input problem or context, z = (z1, . . . , zT ) represents the reasoning trajectory, and y \u2208 Y denotes the final\noutput. When a ground-truth answer is available, it is denoted by y\u22c6. The learning signal is provided by a verifiable\nreward function r(x, z, y) \u2208 [0, 1], which quantifies the degree to which a model-generated solution satisfies the task\nspecification. This reward is computed automatically through a programmatic evaluation that checks the logical or\nfactual consistency of the model output. Depending on the task, r may be instantiated through numerical tolerance\nscoring, symbolic or textual equivalence, structured output validation, or code-level unit and integration tests.\n\nRLVR objective. Reinforcement Learning from Verifiable Rewards (RLVR) maximizes the expected verifiable reward\nwhile penalizing divergence from a reference policy through a Kullback\u2013Leibler (KL) regularization term. Formally,\nthe optimization objective is\n\nmax\n\u03b8\n\nEx\u223cD, (z,y)\u223c\u03c0\u03b8(z,y|x)[r(x, z, y)] \u2212 \u03b2 Ex\u223cD[KL(\u03c0\u03b8(z, y | x) \u2225 \u03c0ref(z, y | x))] ,\n\n(1)\n\nwhere \u03c0ref is a fixed or exponentially moving-averaged (EMA) reference policy, and \u03b2 > 0 controls the trade-off\nbetween maximizing the verifiable reward and maintaining proximity to the reference distribution. The first term\nencourages the model to generate reasoning paths and answers that satisfy programmatically verifiable conditions,\nwhereas the KL penalty mitigates excessive policy drift and stabilizes training under the restricted data regime typical\nof verifiable tasks. This formulation parallels the general form of Reward-Regularized Policy Optimization (RRPO)\nand serves as the foundation for the GRPO update rule introduced below.\n\n3\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nGRPO formulation. Generalized Reinforcement Policy Optimization (GRPO)[Shao et al., 2024]optimizes the policy\nby maximizing a clipped surrogate objective over groups of sampled outputs, providing stable updates within the RLVR\nframework. For each input q, a set of responses {oi}G\ni=1 is drawn from the old policy \u03c0\u03b8old, and the policy parameters\nare optimized under the objective\n\nJGRPO(\u03b8) = E\n\n\uf8ee\n\n\uf8f0\n\n1\nG\n\nG\n(cid:88)\n\ni=1\n\n1\n|oi|\n\n|oi|\n(cid:88)\n\nt=1\n\n(cid:16)\n\nmin\n\n\u03c1\u03b8,i,t \u02c6Ai,t, clip(\u03c1\u03b8,i,t, 1 \u2212 \u03f5, 1 + \u03f5) \u02c6Ai,t\n\n(cid:17)\n\n\uf8f9\n\n\uf8fb \u2212 \u03b2 DKL(\u03c0\u03b8 \u2225 \u03c0ref ) ,\n\n(2)\n\nwhere \u03c1\u03b8,i,t = \u03c0\u03b8(oi,t|q,oi,<t)\n\u03c0\u03b8old (oi,t|q,oi,<t) is the per-token likelihood ratio and \u02c6Ai,t is the corresponding advantage estimate.\nOutcome-level supervision assigns each sequence oi a normalized scalar reward \u02c6Ai,t = (ri \u2212 mean(r))/ std(r), shared\nacross all tokens in that sequence. The KL regularization term constrains deviation from reference model, and is\nestimated using the per-token unbiased form\n\nDKL(\u03c0\u03b8 \u2225 \u03c0ref ) =\n\n\u03c0ref (oi,t | q, oi,<t)\n\u03c0\u03b8(oi,t | q, oi,<t)\n\n\u2212 log\n\n\u03c0ref (oi,t | q, oi,<t)\n\u03c0\u03b8(oi,t | q, oi,<t)\n\n\u2212 1,\n\n(3)\n\nfollowing the unbiased estimator proposed by Schulman et al. [2015]. This formulation retains the stability properties\nof PPO while optimizing directly toward verifiable reward signals.\n\n4 Methodology\n\nMR-RLVR enables models to exploit intermediate reasoning even when only final answers are verifiable. Training pro-\nceeds in two stages. In Stage I, we derive process-level rewards from reasoning traces via process-aware self-supervised\ntasks (Masked-Then-Fill and Step Reordering) and run an initial RLVR phase using only these process-level rewards.\nIn Stage II, we fine-tune with RLVR supervised exclusively by programmatically verifiable final-answer rewards. This\ntwo-stage design first shapes the policy distribution with process-level signals and then trains under sparse outcome\nsupervision, yielding more informative gradients and more stable exploration. Figure 1 illustrates the framework: Stage\nI uses only process-level rewards; Stage II uses only final-answer rewards.\n\n4.1 Process-aware self-supervised data curation\n\nIn the first stage, we construct training data for process-aware self-supervised tasks, designed to guide the model\nin learning local logical consistency and step dependencies within reasoning trajectories. Specifically, we sample\nmathematical proof and computational reasoning problems to form the process-level self-supervised training corpus,\nwhile only the computational reasoning subset is used in Stage II fine-tuning.\n\nFor each problem x and its corresponding reasoning trajectory and answer (z, y), we not only filter semantically\nwell-structured and symbolically valid reasoning texts, but also restructure overly verbose or redundant reasoning chains\nto obtain concise and logically coherent reasoning processes. Two structural transformations are then applied:\n\n(1)Masked-Then-Fill task. Key formulas, inference steps, or theorem invocations in the reasoning text are masked\nto create a list of masked positions M = {mk}, along with the corresponding ground-truth completions. This task\nrequires the model to reconstruct missing reasoning content given contextual information.\n\n(2)Step Reordering task. The reasoning trajectory is decomposed into ordered steps {sj}, which are randomly\npermuted to form a perturbed sequence. The model is required to recover the correct order based on logical coherence.\n\nAfter these operations, we denote the processed reasoning sequence as \u02c6z. The model input consists of the problem\nstatement x and the modified reasoning \u02c6z, while the supervision signal involves either token restoration or step order\nprediction.\n\n4.2 Process-level reward design for self-supervision\n\nDuring the self-supervised phase, the model constructs a process-level reward signal based on either the Masked-Then-\nFill or Step Reordering task (only one of them is used per training run).\n\n(1) Masked-Then-Fill reward. For masked samples, h masked locations are randomly selected for evaluation. The\nreward is defined as the mean semantic match score between model completions and ground truths, measured by\n\n4\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nFigure 1: Overview of the MR-RLVR two-stage training framework. Stage I uses process-level rewards from self-\nsupervised tasks; Stage II uses final-outcome rewards.\n\nMathRuler for mathematical entity alignment and supplemented by textual similarity as fallback:\n\nrmask(x, \u02c6z, y) =\n\n1\nh\n\nh\n(cid:88)\n\nk=1\n\nMatchentity( \u02c6mk, m\u22c6\n\nk) .\n\n(4)\n\n(2) Step Reordering reward. For the step reordering task, we measure how well the model recovers the correct\nposition of each reasoning step in the sequence. Let otrue denote the reference order over n reasoning steps, and opred\nthe permutation predicted by the model. We denote by postrue(k) the index of step k in the reference order otrue, and by\npospred(k) its index in the predicted order opred. We define a normalized position-based distance\n\ndpos(opred, otrue) =\n\n1\nn\n\nn\n(cid:88)\n\nk=1\n\nI(cid:2)pospred(k) \u0338= postrue(k)(cid:3),\n\nwhere I[\u00b7] is the indicator function. This distance dpos \u2208 [0, 1] measures the fraction of steps that are placed at incorrect\npositions.\n\nThe corresponding step-order reward is then defined as\n\nrorder(x, \u02dcz, y) = 1 \u2212 dpos(opred, otrue),\nso that perfectly ordered sequences receive reward 1, while sequences in which all steps are misplaced receive reward 0.\nThis reward naturally lies in [0, 1] and can be directly combined with the masked-then-fill reward within the MR-RLVR\nframework.\n\nAccordingly, the process-level reward is defined as\n\nrproc(x, \u02dcz, y) = Imask \u00b7 rmask(x, \u02dcz, y) + Iorder \u00b7 rorder(x, \u02dcz, y),\n(5)\nwhere Imask, Iorder \u2208 {0, 1} are indicator functions for the Masked-Then-Fill and Step Reordering tasks with Imask +\nIorder = 1.\n\n5\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\n4.3 Stage II: fine-tuning with outcome-only rewards\n\nIn the second stage, we initialize from the checkpoint obtained through process-level reinforcement learning and\nfine-tune on programmatically verifiable problem instances. The model now generates complete reasoning trajectories\nfrom the problem statement x and the model generates both reasoning z and answer y, receiving supervision from the\nfinal-outcome reward rfinal(y\u22c6, y).\nOnly computational reasoning tasks are included here, as they feature open-ended reasoning trajectories but deterministic\nfinal answers. During fine-tuning, the model receives a binary verifiable reward:\nrfinal(y\u22c6, y) = I(cid:2)Verify(y\u22c6, y) = True(cid:3),\nwhere y\u22c6 is the ground-truth answer and Verify(y\u22c6, y) denotes symbolic and numerical verification comparing the\ngenerated answer y against y\u22c6.\n\n(6)\n\nThis signal is used to perform RLVR optimization under the GRPO objective. The two-stage scheme first shapes the\nreasoning distribution using dense process-level supervision, then refines it with sparse but precise outcome rewards,\nyielding stable optimization and verifiably correct multi-step reasoning behavior.\n\n5 Experiment\n\n5.1 Experimental Setup\n\nTraining Data Curation To accommodate diverse reasoning styles, we construct our training corpus from two\ndata sources: DeepTheorem[Zhang et al., 2025] primarily contains theorem-proving problems, while DeepMath\n[He et al., 2025] focuses on computational reasoning tasks. We sample 10k problems equally from both datasets.\nSince the reasoning traces in DeepMath are generated by DeepSeek models and contain multiple internal verification\nsteps that may lead to information leakage within trajectories, we refine all DeepMath samples using GPT-o3-mini\nto ensure clean, step-by-step reasoning without redundant self-verification. As identifying key theorems, formulas,\nand reasoning steps for masking or reordering is non-trivial, we employ DeepSeek-R1-0528 to process the refined\nreasoning trajectories and generate task-specific annotations: masked positions for the Masked-Then-Fill task and step\nboundaries for the Step Reordering task (prompt templates in Appendix A). To avoid trivial masking tasks, we retain\nonly samples with at least 7 masked positions. From the filtered pool, we select 10k samples for each of the two tasks\n(Masked-Then-Fill and Step Reordering), randomly sampling 20k instances for Stage I training and 6k for validation.\nFor Stage II fine-tuning, we select 5k computational reasoning samples from the Stage I training data as the training set,\nwith 1.5k held out for validation, ensuring that Stage II focuses on verifiable computational problems with deterministic\nanswers.\n\nModel Configuration We conduct experiments on Qwen2.5-3B-Base[Qwen et al., 2025] and DeepSeek-R1-Distill-\nQwen-1.5B[Guo et al., 2025]. We employ the GRPO objective for reinforcement learning. All experiments are\nconducted within the verl framework [Sheng et al., 2025] on a single node with 8 NVIDIA A100 (80 GB) or A800 (80\nGB) GPUs. To optimize GPU memory usage, several parameters differ slightly between Qwen-3B and DeepSeek-R1-\nDistill-Qwen-1.5. Detailed experimental hyperparameters can be found in the Appendix B.\n\nEvaluation Setup We evaluate reasoning performance on four challenging mathematical benchmarks: AIME 2024,\nAIME 2025, AMC 2023 Li et al., 2024, and MATH500 Hendrycks et al., 2021. We report the unbiased estimator of\nPass@k Chen et al. [2021], defined as\n\n(cid:34)\n\nPass@k = Ex\u223cD\n\n1 \u2212\n\n(cid:35)\n\n(cid:1)\n\n,\n\n(cid:0)n\u2212c\nk\n(cid:1)\n(cid:0)n\nk\n\n(7)\n\nwhere n is the number of generated solutions per problem, c is the number of correct solutions, and k \u2208 {1, 5, 8} denotes\nthe number of attempts allowed. We set n = 64 for all evaluations. During inference, we use nucleus sampling with\ntemperature 0.6, top-p 0.95, and a maximum generation length of 4096 tokens. Answers are verified programmatically\nthrough symbolic computation using MathRuler hiyouga [2025] and text matching.\n\n5.2 Main Results\n\nTable 1 presents the main experimental results comparing our MR-RLVR method against the GRPO baseline across\nfour mathematical reasoning benchmarks. Overall, MR-RLVR demonstrates consistent performance improvements over\n\n6\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nTable 1: Performance comparison across mathematical reasoning benchmarks.\nQwen2.5-3B\n\nBenchmark\n\nPass@1 (%)\n\nPass@5 (%)\n\nPass@8 (%)\n\nBase +GRPO +MR-RLVR\n\nBase +GRPO +MR-RLVR\n\nBase +GRPO +MR-RLVR\n\nAIME24\nAIME25\nAMC23\nMATH500\n\n1.93\n0.73\n14.06\n27.75\n\n5.63\n2.03\n36.13\n63.30\n\n6.30 (\u219112.04%)\n7.48\n2.76 (\u219135.98%)\n3.58\n40.82 (\u219112.98%) 41.70\n65.87 (\u21914.06%) 62.97\n\n14.29\n8.53\n60.39\n79.83\n\n13.20 (\u21937.61%) 10.23\n10.44 (\u219122.44%)\n5.13\n64.48 (\u21916.82%) 50.89\n80.94 (\u21911.39%) 70.97\n\n17.29\n12.10\n66.29\n83.29\n\n15.43 (\u219310.74%)\n14.05 (\u219116.11%)\n69.80 (\u21915.29%)\n83.85 (\u21910.67%)\n\nDeepSeek-R1-Distill-Qwen-1.5B\n\nBenchmark\n\nPass@1 (%)\n\nPass@5 (%)\n\nPass@8 (%)\n\nBase +GRPO +MR-RLVR\n\nBase +GRPO +MR-RLVR\n\nBase +GRPO +MR-RLVR\n\nAIME24\nAIME25\nAMC23\nMATH500\n\n9.17\n10.62\n36.84\n60.80\n\n18.70\n15.94\n62.30\n78.05\n\n19.43 (\u21913.90%) 21.93\n17.24 (\u21918.17%) 23.10\n63.01 (\u21911.14%) 66.57\n78.51 (\u21910.59%) 85.04\n\n36.40\n27.40\n84.23\n90.08\n\n36.96 (\u21911.54%) 26.00\n31.72 (\u219115.77%) 26.43\n85.62 (\u21911.65%) 72.98\n90.25 (\u21910.19%) 88.38\n\n41.98\n29.50\n89.30\n91.85\n\n42.08 (\u21910.24%)\n35.43 (\u219120.12%)\n89.48 (\u21910.20%)\n91.97 (\u21910.13%)\n\nWe report Pass@k (k \u2208 {1, 5, 8}) with n = 64 samples per problem. Arrows indicate relative improvement (%) over the GRPO\nbaseline. Bold indicates the best performance within each model family.\n\nGRPO, particularly in challenging scenarios where baseline performance is relatively low. On the Qwen2.5-3B model,\nwe observe substantial gains on AIME 2025, with Pass@1, Pass@5, and Pass@8 achieving relative improvements of\n35.98%, 22.44%, and 16.11%, respectively. Similar trends are evident in AIME24 Pass@1 (+12.04%) and across all\nmetrics on AMC23 (+5.29% to +12.98%).\n\nInterestingly, the performance gains exhibit interesting patterns across different model architectures and task difficulties.\nIn terms of average improvement magnitude, MR-RLVR yields significantly larger gains on Qwen2.5-3B (average\n8.29%) compared to DeepSeek-R1-Distill-Qwen-1.5B (average 4.47%). This discrepancy may be attributed to the\nfact that the former has only undergone basic pretraining without complex post-training procedures, thus offering\ngreater room for optimization through verification enhancement. Another notable observation is that both models show\nrelatively limited improvements on MATH500 (0.13%-4.06%). This is not only because the baseline performance is\nalready high (Pass@1>78%), but more importantly, the problems in MATH500 are relatively simple and the models\nhave largely mastered their solution patterns, resulting in minimal marginal gains from multi-round verification. In\ncontrast, our method demonstrates much stronger value on competition-level problems such as AIME and AMC. We\nalso observe minor performance degradation on AIME24 Pass@5/Pass@8 for Qwen2.5-3B, which we attribute to the\ninherent variance in the sampling process with n=64 samples.\n\nThese results validate the effectiveness of our MR-RLVR framework in improving mathematical reasoning capabilities.\nThe consistent gains on challenging benchmarks (AIME24, AIME25, AMC23) demonstrate that MR-RLVR success-\nfully enhances the model\u2019s ability to generate and verify correct solutions, particularly in scenarios where baseline\nperformance leaves substantial room for improvement. Meanwhile, the more modest improvements on relatively simple\ntasks like MATH500 indicate that our method provides maximum value when applied to high-difficulty problems at the\nboundary of model competence, which aligns well with the design principle of MR-RLVR, to tackle complex reasoning\ntasks not yet fully mastered by the model through iterative refinement.\n\n5.3 More Analysis about MR-RLVR\n\n5.3.1 Data Efficiency Analysis\n\nTo investigate the data efficiency of MR-RLVR, we conduct experiments with different training data scales on DeepSeek-\nR1-Distill-Qwen-1.5B. Table 2 compares MR-RLVR against the GRPO baseline using 1k and 3k training samples.\n\nResults show that MR-RLVR consistently outperforms GRPO across different data regimes. With 1k samples, MR-\nRLVR demonstrates significant improvements over GRPO, especially on Pass@5 and Pass@8 metrics, while Pass@1\nshows minimal gains. On AIME24, MR-RLVR achieves 26.90% and 31.84% for Pass@5/Pass@8 compared to GRPO\u2019s\n24.62% and 28.69%, representing relative gains of 9.26% and 10.99%, whereas Pass@1 remains unchanged at 11.09%.\nThis pattern suggests that with limited training data, the MR-RLVR framework primarily improves the model\u2019s ability\nto generate diverse high-quality candidates rather than directly enhancing single-sample accuracy. When scaled to\n3k samples, MR-RLVR maintains its advantage with a 10.08% relative improvement on AIME24 Pass@8 (40.97%\n\n7\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nTable 2: Performance comparison of DeepSeek-R1-Distill-Qwen-1.5B under different training data scales.\n\nMethod\n\nAIME24\n\nAIME25\n\nAMC23\n\nMATH500\n\nP@1 P@5 P@8 P@1 P@5 P@8 P@1 P@5 P@8 P@1 P@5 P@8\n\nBase Model\n\n9.17 21.93 26.00 10.62 23.10 26.43 36.84 66.57 72.98 60.80 85.04 88.38\n\nTraining with 1k samples\n\n11.09 24.62 28.69 11.98 24.56 28.02 41.99 70.81 77.11 65.24 86.28 89.01\n+GRPO\n+MR-RLVR 11.09 26.90 31.84 11.93 25.45 28.91 42.73 72.82 79.25 65.67 86.49 89.24\n\nTraining with 3k samples\n\n16.67 32.41 37.22 15.31 28.67 31.97 58.01 81.86 86.91 76.40 89.77 91.79\n+GRPO\n+MR-RLVR 16.56 35.13 40.97 15.57 29.37 32.35 59.02 82.72 86.31 76.24 89.45 91.47\n\nTable 3: All experiments use DeepSeek-R1-Distill-Qwen-1.5B as the base model. Bold indicates better performance\nbetween GRPO and MR-RLVR at the same data scale.\n\nvs. 37.22%). This consistent advantage suggests that process-level self-supervision in MR-RLVR provides more\nsample-efficient learning signals than standard GRPO, enabling better generalization with limited training data. On\nthe simpler MATH500 benchmark, the gap narrows, confirming that MR-RLVR\u2019s benefits are most pronounced on\nchallenging problems.\n\n5.4 MR-RLVR Tasks for Data Augmentation\n\nGiven the sample efficiency gains demonstrated by MR-RLVR, we further explore self-supervised pretraining tasks for\nexpanding training signals without additional human annotations. We design two tasks, step reordering and masked-\nthen-fill, that leverage existing mathematical reasoning corpora to automatically generate diverse reasoning trajectories.\nTable 6 and Table 7 presents two representative cases together with model outputs.\n\nValue of the step reordering task. As shown in Table 6, in the case involving the Lebesgue differentiation theorem,\nthe model needs to restore 6 shuffled proof steps to their correct logical order. During this process, the model performs\ndetailed logical analysis of each step and identifies inter-step dependencies. For instance, the model recognizes that Step\n2 (defining F (x)) is the starting point of the proof, Step 4 (providing the dominating function) is a necessary condition\nfor applying the Dominated Convergence Theorem, and Step 1 is the key theorem application step. This analytical\nprocess constitutes a structured reconstruction of the original proof: the model not only produces the correct ordering\nbut also generates an explanation of why this ordering is valid. Compared to the original shuffled steps, the model\nautomatically generates logical interpretations of each step and explicit annotations of inter-step dependencies during\nthe reordering process. These generated reasoning trajectories effectively complement the original concise reasoning\nprocess. While original proofs typically only provide key steps, the model\u2019s analysis reveals how to identify the overall\nproof structure and how to determine logical dependencies between steps, thereby making implicit reasoning structures\nexplicit. This automatically generated structured interpretation provides richer training signals for models to learn\ncomplete proof construction capabilities.\n\nValue of the masked-then-fill task. Table 7 presents a masked-then-fill case involving bitwise operations. The task\nrequires the model to complete three masked key formulas. The model successfully derives the first two: simplifying\nA \u2295 0xFFFFFFFF to \u223c A, and determining A = 0x81010100 through bit analysis. However, at the third mask position\n(verification step), the model provides 0x7EFEFEFF\u22950x81010100 = 0xFFFFFFFF, while the original solution requires\ncomputing the addition 0x7EFEFEFF + 0x81010100. Although this equation is mathematically correct, since the\nXOR operation does yield 0xFFFFFFFF, it uses the wrong operator. More subtly, because these two numbers have\nno overlapping bits (no bit position is 1 in both numbers), addition and XOR happen to produce identical results in\nthis case. This coincidentally correct situation reveals speculative behavior: the model may directly apply the same\npattern after seeing XOR operators multiple times in the preceding text, or infer from context that the result should\nbe 0xFFFFFFFF and then reverse-engineer a seemingly reasonable formula, rather than strictly following the required\nreasoning procedure. Such errors are more difficult to detect than obvious computational mistakes.\n\nOverall, the two tasks augment the training corpus in two complementary ways. First, correct trajectories generated\nduring step reordering and masked-then-fill provide detailed and structured reasoning traces that can be directly reused\nas additional training data. Second, speculative errors surfaced by the **masked-then-fill task** can be turned into\nerror-correction objectives, where models are trained to identify and fix logical flaws, thereby improving their self-\n\n8\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nchecking capability. This pretraining stage therefore supplies more informative and sample-efficient learning signals\nthan relying solely on supervised solutions.\n\n6 Conclusion\n\nThis paper presents MR-RLVR, a framework that enriches reinforcement learning from verifiable rewards with\nprocess-level self-supervision. Instead of relying solely on outcome-level rewards derived from final-answer checking,\nMR-RLVR constructs two types of process-level tasks, namely masked-then-fill and step reordering, on mathematical\nreasoning traces, thereby providing dense training signals for intermediate reasoning steps. These tasks encourage the\nmodel not only to produce correct final answers, but also to acquire reusable patterns and structures of reasoning, rather\nthan merely memorizing superficial solution templates.\n\nWe implement and evaluate MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and conduct systematic\nexperiments on a diverse set of mathematical benchmarks, including AIME24, AIME25, AMC23, and MATH500.\nUnder a fixed sampling and decoding budget, MR-RLVR consistently outperforms standard RLVR. This indicates that\nprocess-level self-supervision becomes especially beneficial when problems require long-horizon, multi-step reasoning.\nOur data efficiency analysis further shows that, compared to relying solely on outcome-level rewards, MR-RLVR\nprovides more informative learning signals in low-data regimes.\n\nFor future work, we first note that the current implementation adopts fixed masking positions for masked-then-fill\ntask and a fixed shuffling scheme for step reordering. An interesting direction is to explore dynamically sampling\nmasking locations and reordering strategies during training, allowing data augmentation and process-level tasks to\nadapt to the model\u2019s current state and further improve sample efficiency. Second, we plan to extend MR-RLVR to\nbroader structured reasoning domains such as program synthesis and formal theorem proving, as well as to multimodal\nreasoning tasks involving images, diagrams, and geometric figures, where rich structure and verifiable signals naturally\narise. In addition to masking and reordering, we aim to design more diverse process-level tasks, such as error correction\ntasks that explicitly require the model to identify and revise incorrect steps in a reasoning chain. Finally, MR-RLVR is\nhighly complementary to explicit process reward models and test-time scaling techniques; integrating these components\nmore tightly may further enhance the reliability and scalability of reasoning-focused language models. We hope that\nMR-RLVR offers a useful starting point for more principled integration of self-supervision and verifiable rewards in the\ntraining of reasoning-oriented large language models.\n\nReferences\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas\nTezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira\nMurati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\nZaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.\n\nYihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister,\nand Chen-Yu Lee. Supervised reinforcement learning: From expert trajectories to step-wise reasoning, 2025. URL\nhttps://arxiv.org/abs/2510.25992.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n\ntransformers for language understanding, 2019. URL https://arxiv.org/abs/1810.04805.\n\nXinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small\nllms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.\n04519.\n\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li,\nFuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming\n\u2013 the rise of code intelligence, 2024. URL https://arxiv.org/abs/2401.14196.\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,\nXiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\n\n9\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nZhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang,\nWenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: A\nlarge-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning, 2025. URL\nhttps://arxiv.org/abs/2504.11456.\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n\nhiyouga. Mathruler. https://github.com/hiyouga/MathRuler, 2025.\n\nJia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu,\nAlbert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition\nmath problems and solutions. Hugging Face repository, 13(9):9, 2024.\n\nSiheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng,\nChenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan\nZeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu\nTao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, and Di Wang.\nReinforcement learning on pre-training data, 2025. URL https://arxiv.org/abs/2509.19249.\n\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,\nIlya Sutskever, and Karl Cobbe. Let\u2019s verify step by step, 2023. URL https://arxiv.org/abs/2305.20050.\n\nQuang Hieu Pham, Thuy Duong Nguyen, Tung Pham, Anh Tuan Luu, and Dat Quoc Nguyen. Clozemath: Improving\nmathematical reasoning in language models by learning to fill equations, 2025. URL https://arxiv.org/abs/\n2506.03763.\n\nQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu,\nRui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su,\nYichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025.\nURL https://arxiv.org/abs/2412.15115.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023. URL\nhttps://arxiv.org/abs/1910.10683.\n\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous\n\ncontrol using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv\npreprint arXiv:2402.03300, 2024.\n\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and\nChuan Wu. Hybridflow: A flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference\non Computer Systems, EuroSys \u201925, page 1279\u20131297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL\nhttp://dx.doi.org/10.1145/3689031.3696075.\n\nPeiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd:\nVerify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.\n08935.\n\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren\nZhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang.\nQwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https:\n//arxiv.org/abs/2409.12122.\n\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,\nChenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong\nTang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai\nDang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang,\nQin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren,\nXinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong\nLiu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL\nhttps://arxiv.org/abs/2505.09388.\n\n10\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nTianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu,\nMaosong Sun, and Tat-Seng Chua. Rlpr: Extrapolating rlvr to general domains without verifiers, 2025. URL\nhttps://arxiv.org/abs/2506.18254.\n\nDan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via\n\nprocess reward guided tree search, 2024. URL https://arxiv.org/abs/2406.03816.\n\nZiyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng\nZhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deeptheorem: Advancing llm reasoning for theorem\nproving through natural language and reinforcement learning, 2025. URL https://arxiv.org/abs/2505.23754.\n\n11\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nAppendices\n\nA Prompts\n\nA.1 Prompts for Data Curation\n\nPrompt for masked-then-fill data curation\n\nYou are a helpful assistant.\nTask: Extract the most key formulas or theorem names from the following original answer text and save them in\nJSON format.\nOutput Format: Return the key formulas or theorem names in a JSON object with the following structure:\n\n{\n\n\"theorems\": [\n\n\"Theorem or formula name 1\",\n\"Theorem or formula name 2\",\n\"Theorem or formula name 3\",\n// Continue until all key formulas or theorem names are included\n\n]\n\n}\nRequirements:\n\n\u2022 Extract only the content from the original text without adding new formulas or theorems.\n\u2022 Use standard LaTeX format for all mathematical symbols and expressions.\n\u2022 Sort the extracted theorems by importance, placing the most important ones first and the less important\n\nones later.\n\n\u2022 The output must comply with JSON format and be ready for use.\n\nPrompt for step reordering data curation\n\nYou are a helpful assistant.\nTask: Split the following answer into independent logical steps while maintaining the original meaning of the\ncontent.\nOutput Format: Return the steps in a JSON object with the following structure:\n\n{\n\n\"steps\": [\n\n\"Step 1 description...\",\n\"Step 2 description...\",\n\"Step 3 description...\",\n// Continue until all steps are included\n\n]\n\n}\nRequirements:\n\n\u2022 All steps must be generated from the original answer text without creating new steps or content.\n\u2022 Each step should maintain an independent logical meaning, allowing it to stand alone.\n\u2022 The steps should connect logically in a way that reconstructs the original answer when combined\n\ntogether.\n\n\u2022 Ensure clarity and conciseness in each step to facilitate understanding.\n\u2022 Use standard LaTeX format for all mathematical symbols and expressions.\n\nA.2 Prompts for MR-RLVR\n\nblank\n\n12\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nPrompt for Masked-Then-Fill task\n\nSystem:\nA conversation between the User and the Assistant.\nThe User supplies a mathematical statement together with a partial solution in which some formulas or theorems\nare masked with <formula_masked> tags.\nThe Assistant\u2019s task is to complete the missing portions of the solution by replacing the <formula_masked>\ntags with the appropriate mathematical formulas or theorems.\nPlease adhere to the following structured approach:\n\n1. Begin by performing a comprehensive logical analysis to determine the precise formula\nrequired for each <formula_masked> tag. The objective is to ensure the logical coherence\nand completeness of the entire solution.\n2. Enclose your **detailed logical analysis**, explaining the derivation of each missing\nformula, within <think> tags, formatted as follows:\n\n<think>\n[Your detailed reasoning process, explaining how each missing formula was de-\nrived.]\n</think>\n\n3. Finally, upon completion of the analysis and derivation of all missing formulas, provide\n**only** the derived formulas, enclosed within \\boxed{} notation:\n\\boxed{formula_1; formula_2; ...; formula_n}\n\nThe formulas within the \\boxed{} answer must appear in the same order as their correspond-\ning <formula_masked> tags in the original solution. All mathematical formulas should be\npresented using proper LaTeX notation.\n\nUser:\nThe user\u2019s statement:\nThe partial solution is:\n\nPrompt for Step reordering task\n\nSystem:\nA conversation between the User and the Assistant.\nThe User supplies a mathematical statement and a solution whose steps are out of order (each step is already\nnumbered with \u2019Step i\u2019).\nThe Assistant\u2019s task is to determine the correct logical sequence of these steps.\nPlease adhere to the following structured approach:\n\n1. Begin by performing a comprehensive logical analysis of the mathematical statement and all given\nsteps to establish their correct sequential order. The objective is to reconstruct a logically sound and\ncomplete solution.\n\n2. Enclose your **detailed logical analysis**, explaining how you determined the correct sequence,\n\nwithin <think> tags, formatted as follows:\n\n<think>\n[Your detailed reasoning process, explaining how the logical sequence of steps was deter-\nmined.]\n</think>\n\n3. Finally, provide **only** the correct sequence of step numbers, enclosed within \\boxed{} notation:\n\n\\boxed{n1, n2, n3, ..., nk}\n\nThe step numbers within the \\boxed{} answer must represent the final, logically ordered sequence of\nthe steps.\n\nUser:\nThe user\u2019s statement:\nThe shuffled solution:\n\n13\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nPrompt for Outcome-only Task\n\nSystem:\nA conversation between User and Assistant.\nThe User provides a question, and the Assistant outputs the answer.\nThe Assistant\u2019s task is to solve the question and provide the final answer.\nPlease adhere to the following structured approach:\n\n1. Provide a concise solution analysis to determine how to compute the answer and enclose a detailed,\n\nstep-by-step derivation within <think> tags. Use the following format:\n\n<think>\n[Your detailed reasoning process analysis, explained through a step-by-step derivation.]\n</think>\n\n2. Finally, provide only the final result written in standard LaTeX and enclosed within \\boxed{ }.\n\nUser:\nhe user\u2019s question:\n\nB Implementation Details\n\nParameter\nLearning Rate (lr)\nRollout Number\nRollout Temperature\nPrompt Length Token\nResponse Length Token\nTraining Batch Size\nPPO Mini Batch Size\nKL Loss Coefficient\nTraining Epochs\n\nqwen-3b STAGE I\n1 \u00d7 10\u22126\n16\n1.0\n2048\n4096\n512\n64\n0.001\n3\n\nqwen-3b STAGE II\n1 \u00d7 10\u22126\n16\n1.0\n1024\n4096\n512\n64\n0.001\n3\n\nTable 4: The training hyperparameters of MR-RLVR for qwen-3b\n\nParameter\nLearning Rate\nRollout Number\nTemperature\nPrompt Length Token\nResponse Length Token\nTraining Batch Size\nPPO Mini Batch Size\nKL Loss Coefficient\nTraining Epochs\n\nDeepseek-R1-distill-Q wen-1.5b STAGE I Deepseek-R1-distill-Q wen-1.5b STAGE II\n\n1 \u00d7 10\u22126\n8\n1.0\n2048\n8192\n512\n64\n0.001\n3\n\n1 \u00d7 10\u22126\n16\n1.0\n1024\n4096\n512\n64\n0.001\n3\n\nTable 5: The training hyperparameters of MR-RLVR for deepseek-dis-qwen-1.5b\n\nC Test Cases\n\nblank\n\n14\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nCase of Reordering Steps Task\n\nProblem: Leibniz Integral Rule under Lebesgue Conditions\nLet f : Rn \u2192 R satisfy the conditions of the Lebesgue differentiation theorem, and let S \u2282 Rn be measurable\nwith 0 < |S| < \u221e.\nProve: For all x such that \u2202f\n\n\u2202x exists a.e. in S,\n\nd\ndx\n\n(cid:90)\n\nS\n\nf (x, y) dy =\n\n(cid:90)\n\nS\n\n\u2202f\n\u2202x\n\n(x, y) dy.\n\nInput:\nStep 0: State that for almost every y \u2208 S, the pointwise limit\n\nlim\nh\u21920\n\nf (x + h, y) \u2212 f (x, y)\nh\n\n=\n\n\u2202f\n\u2202x\n\n(x, y)\n\nexists by hypothesis.\nStep 1: Apply the Dominated Convergence Theorem to interchange limit and integral, yielding\n\n(cid:90)\n\nS\n\nlim\nh\u21920\n\nf (x + h, y) \u2212 f (x, y)\nh\n\ndy =\n\n(cid:90)\n\nS\n\n\u2202f\n\u2202x\n\n(x, y) dy.\n\nStep 2: Define F (x) = (cid:82)\n\nStep 3: Conclude that\n\nS f (x, y) dy and state the goal to show\ndF\ndx\n\n\u2202f\n\u2202x\n\n=\n\n(cid:90)\n\n(x, y) dy.\n\nS\n\ndF\ndx\n\n(cid:90)\n\n=\n\nS\n\n\u2202f\n\u2202x\n\n(x, y) dy.\n\nStep 4: Note that\n\n(cid:12)\n(cid:12)\n(cid:12)\n\n(cid:12)\n\u2202f\n(cid:12)\n(cid:12) \u2264 g(y) and\n\u2202x (x, y)\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\nf (x + h, y) \u2212 f (x, y)\nh\n\n(cid:12)\n(cid:12)\n(cid:12)\n(cid:12)\n\n\u2264 g(y)\n\nfor sufficiently small h, where g is Lebesgue integrable over S.\nStep 5: Express the difference quotient for F (x) as\n\nF (x + h) \u2212 F (x)\nh\n\n(cid:90)\n\n=\n\nS\n\nf (x + h, y) \u2212 f (x, y)\nh\n\ndy\n\nfor h \u0338= 0.\n\nOutput:\nStep 0: This is a crucial step in establishing the differentiability of f in the x-direction.\nStep 1: This is a key step in proving the differentiability of F (x).\nStep 2: This is the starting point of the proof.\nStep 3: This is the final result we are aiming to prove.\nStep 4: This is a necessary step to apply the Dominated Convergence Theorem.\nStep 5: This is a crucial step in establishing the differentiability of F (x).\nBased on this analysis, the correct logical sequence is:\n\n2 \u2192 5 \u2192 0 \u2192 4 \u2192 1 \u2192 3\n\nTable 6: Detailed Examination of Reordering Steps in the Proof of the Leibniz Integral Rule under Lebesgue Conditions\n\n15\n\n\fA PREPRINT - NOVEMBER 24, 2025\n\nCase of Masked-Then-Fill Task\n\nProblem: Find the hexadecimal value of A satisfying\n\n(0x7EFEFEFF + A) \u2295 (A \u2295 0xFFFFFFFF) = 0x81010100\n\nwhere + denotes bitwise addition and \u2295 denotes XOR.\nInput:\nStep 1. Simplification\nNote that A \u2295 0xFFFFFFFF =\u223c A for any 32-bit value. Rewrite the equation as:\n\nStep 2. Binary Analysis\nWrite 0x7EFEFEFF in binary:\n\n<formula_masked>\n\n0x7EFEFEFF = 01111110 11111110 11111110 11111111\n\nObserve: zeros at bit positions 31, 24, 16, and 8.\nStep 3. Deriving A\nTo avoid carries during addition, A should have 1\u2019s only where 0x7EFEFEFF has 0\u2019s:\n\nStep 4. Verification\nCompute 0x7EFEFEFF + A. Since there is no overlap (no bit is 1 in both numbers), the addition result is:\n\n<formula_masked>\n\nThen verify:\n\nOutput:\n\n<formula_masked>\n\n0xFFFFFFFF \u2295 0x7EFEFEFF = 0x81010100\n\n<formula_masked> \u2192 (0x7EFEFEFF + A)\u2295 \u223c A = 0x81010100 \u2713 Correct\n\n<formula_masked> \u2192 A = 0x81010100 \u2713 Correct\n\n<formula_masked> \u2192 0x7EFEFEFF \u2295 0x81010100 = 0xFFFFFFFF \u00d7 Wrong operator! Should be \u2019+\u2019\n\nExpected: 0x7EFEFEFF + 0x81010100 = 0xFFFFFFFF\n\nTable 7: Detailed Examination of Mask-Then-Fill for Finding the Hexadecimal Value of A Based on Bitwise Operations\n\n16\n\n\f"
}