5
2
0
2

v
o
N
1
2

]

G
L
.
s
c
[

1
v
9
8
4
7
1
.
1
1
5
2
:
v
i
X
r
a

1–30

Harnessing Data from Clustered LQR Systems: Personalized and
Collaborative Policy Optimization

Vinay Kanakeri
Department of Electrical and Computer Engineering, North Carolina State University

VKANAKE@NCSU.EDU

Shivam Bajaj
The Elmore Family School of Electrical and Computer Engineering, Purdue University

BAJAJ41@PURDUE.EDU

Ashwin Verma
The Elmore Family School of Electrical and Computer Engineering, Purdue University

VERMA240@PURDUE.EDU

Vijay Gupta
The Elmore Family School of Electrical and Computer Engineering, Purdue University

GUPTA869@PURDUE.EDU

Aritra Mitra
Department of Electrical and Computer Engineering, North Carolina State University

AMITRA2@NCSU.EDU

Abstract
It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it
has been proposed that the learning algorithm utilize data from ‘approximately similar’ processes.
However, since the process models are unknown, identifying which other processes are similar poses
a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic
Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding
to a copy of a linear process to be controlled. The agents’ local processes can be partitioned into
clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination
and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous
clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable
notion of cluster separation that captures differences in closed-loop performance across systems,
we prove that our approach guarantees correct clustering with high probability. Furthermore, we
show that the sub-optimality gap of the policy learned for each cluster scales inversely with the
size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based
control. Our work is the first to reveal how clustering can be used in data-driven control to learn
personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality
due to inclusion of data from dissimilar processes. From a distributed implementation perspective,
our method is attractive as it incurs only a mild logarithmic communication overhead.
Keywords: Policy gradients for LQR; Collaborative Learning; Transfer/Multi-Task Learning.

1. Introduction

The last decade or so has seen a surge of interest in model-free data-driven control (Hu et al., 2023),
where control laws (policies) are learned directly from data, bypassing the need to estimate the
system model as an intermediate step. Although such a framework is promising, it relies on the
availability of adequate data to learn high-precision policies. Unfortunately, however, data from
physical processes (such as real-world robotic environments) could be scarce and/or difficult to
collect. Drawing inspiration from popular paradigms such as federated and meta-learning, some
recent papers (Zhang et al., 2023; Wang et al., 2023a; Toso et al., 2024) have attempted to mitigate
this challenge by exploring the idea of combining information generated by multiple environments,
where each environment represents a dynamical system with an associated cost performance metric

© V. Kanakeri, S. Bajaj, A. Verma, V. Gupta & A. Mitra.

 
 
 
 
 
 
KANAKERI BAJAJ VERMA GUPTA MITRA

that captures a task or a goal. The unifying theme in such papers is to learn a single common policy
that performs well across all environments by minimizing an average-cost performance metric. When
environments differ considerably in their tasks, such a single common policy might incur highly
sub-optimal performance on any given environment. More fundamentally, when environments differ
in dynamics, even the existence of a common stabilizing policy is unclear and difficult to verify in the
absence of models. Departing from the approach of learning a single common policy, in this paper,
we ask: (When) is it possible to learn personalized policies in a sample-efficient way by leveraging
data generated by potentially non-identical dynamical processes?

To formalize our study, we consider a scenario involving multiple agents that can be partitioned
into distinct clusters. We assume that all agents within a given cluster interact with the same physical
environment modeled as a linear time-variant (LTI) system with unknown dynamics; furthermore,
all agents within a cluster share the same quadratic cost function. However, the dynamics and cost
functions across clusters can be arbitrarily different. Thus, our setting captures both similarities
and differences in dynamics and tasks. As is common in collaborative and federated learning, we
allow agents to exchange information via a central aggregator. Concretely, our problem of interest
is to learn a personalized policy for each cluster that enjoys the benefits of collaboration, i.e., we
wish to show that such a policy can be learned faster (relative to a single-agent setting) by using the
collective samples available within the cluster. However, this is challenging, as we explain below.

Challenges. To make our setting realistic, we assume that the cluster structure is unknown a
priori. Since the system models associated with the clusters are also unknown, it becomes difficult
to decide how information should be exchanged between agents. In particular, care needs to be
taken to avoid misclustering, since transfer of information across clusters with arbitrarily different
LTI systems can lead to the learning of destabilizing policies; thus, in our setting, more data can
potentially hurt if not used judiciously. Additional subtleties arise as the agents in our setting access
only noisy zeroth-order information for both clustering and learning policies; we discuss them in
Sections 2 and 3. In light of these challenges, the main contributions of this paper are as follows.
• Problem Formulation. While clustering has been explored in federated learning (FL) for
static supervised learning tasks (Ghosh et al., 2020), our work provides the first principled study of
clustering in the context of model-free data-driven control, and shows how such a formalism can
enable learning personalized policies in a sample-efficient manner. As part of our formulation, we
identify a dissimilarity metric ∆ (see (3)) that captures differences in optimal costs between clusters.
Our results reveal that a larger value of ∆ leads to a faster separation of clusters.

• Novel Algorithm. The primary contribution of this paper is the development of a novel
model-free Personalized and Collaborative Policy Optimization (PO) algorithm (Algorithm 1) called
PCPO that combines ideas from sequential elimination in multi-armed bandits (Even-Dar et al.,
2006) and policy gradient algorithms in reinforcement learning (RL) (Agarwal et al., 2021). The lack
of prior knowledge of the cluster-separation gap ∆ motivates the need for a sequential elimination
strategy to identify the clusters. Moreover, since it is non-trivial to decide when to stop clustering and
start collaborating, we propose an epoch-based approach that involves clustering and collaboration
in every epoch by requiring agents to maintain two separate sequences of policies: local policies
used purely for sequential clustering and global policies for collaboration. Another key feature of
our algorithm is that it only incurs a mild communication cost that scales logarithmically with the
number of agents and samples, making it particularly appealing for distributed implementation.

• Collaborative Gains. In Theorem 1, we prove that with high probability, our approach leads to
correct clustering, despite the absence of prior knowledge of models, cluster structure, and separation

2

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

gap ∆. This result also reveals that more heterogeneity can actually aid the learning process in that
a larger ∆ incurs fewer noisy function evaluations for correct clustering. Building on Theorem 1,
our main result in Theorem 2 proves that by using PCPO, each agent can learn a near-optimal
personalized policy for its own system with a sub-optimality gap that scales inversely with the
number of agents in its cluster. In other words, PCPO prevents negative transfer of information across
clusters, while ensuring sample-complexity reductions via collaboration within each cluster. To our
knowledge, this is the first result to show how data from heterogeneous dynamical systems admitting
a cluster structure can be harnessed to expedite the learning of personalized policies.

Since this is a preliminary investigation of clustering in data-driven control, we restrict our
attention to the canonical Linear Quadratic Regulator (LQR) formalism. That said, we anticipate that
our general algorithmic template can be used in other supervised or RL problems.

Related Work. To put our contributions into perspective, we discuss relevant literature below.
• Policy Gradient for LQR. We build on the rich set of results on policy gradient methods for
the LQR problem (Fazel et al., 2018; Malik et al., 2020; Gravell et al., 2020; Zhang et al., 2021;
Mohammadi et al., 2019, 2020; Hu et al., 2023; Moghaddam et al., 2025). Generalizing these results
from the single system setting to our clustered multi-system formulation introduces various nuances
and challenges (outlined in Sections 2 and 3) that we address in this paper.

• Personalized Federated Learning. We draw inspiration from the work on clustering in
FL (Ghosh et al., 2020, 2022; Sattler et al., 2020) that aims to learn personalized models for groups of
agents that are similar in terms of their data distributions. Despite cosmetic similarities, the specifics
of our setting differ significantly in that we focus on control of dynamical systems where stability
plays a crucial role; no such stability concerns arise in the FL papers above on supervised learning.
• Collaborative System Identification. Our paper is related to a growing body of work that seeks
to leverage data from multiple dynamical systems to achieve statistical gains in estimation accuracy.
In this context, several papers (Wang et al., 2023b; Toso et al., 2023; Chen et al., 2023; Modi et al.,
2024; Rui and Dahleh, 2025; Tupe et al., 2025; Xin et al., 2025) have explored collaborative system
identification by combining trajectory data from multiple systems that share structural similarities.
In particular, Toso et al. (2023) and Rui and Dahleh (2025) assume a cluster structure like us. While
the above papers focus on using collective data for an open-loop estimation problem, namely system-
identification, our work focuses instead on data-efficient closed-loop control by directly learning
policies. As such, our notion of heterogeneity captures differences in closed-loop performance across
clusters as opposed to similarity metrics imposed on open-loop system matrices in the papers above.
• Meta, Multi-Task, and Transfer Learning in Control. Under the umbrella framework of
meta and transfer learning, various recent papers (Wang et al., 2023a; Toso et al., 2024; Aravind et al.,
2024; Stamouli et al., 2025) have used PO methods to study how information from multiple LTI
systems can be aggregated to learn policies that adapt across similar systems. Our formulation, which
seeks to find a personalized policy for every system, departs fundamentally from this line of work
which instead aims to learn a common policy for all systems. In this regard, we note that the closely
related papers of Wang et al. (2023a) and Toso et al. (2024) need to assume that all the systems are
sufficiently similar to admit a common stabilizing set. Even under this restrictive assumption, the
results in these papers indicate that the sub-optimality gap exhibits an additive heterogeneity-induced
bias term that might negate the speedups from collaboration. In contrast, our work does not require
a common stabilizing policy to exist for systems across clusters. Furthermore, our personalization
approach completely eliminates heterogeneity-induced biases. We also note that our approach incurs
a logarithmic (in agents and samples) communication cost as opposed to the linear cost in Wang

3

KANAKERI BAJAJ VERMA GUPTA MITRA

et al. (2023a). Finally, complementary to our clustering-based approach, ideas from representation
learning (Zhang et al., 2023; Guo et al., 2023; Lee et al., 2025) and domain randomization (Fujinami
et al., 2025) have also been recently used to improve data-efficiency in dynamic control tasks.

2. Problem Formulation

We consider a setting with N agents partitioned into H disjoint clusters {Mj}j∈[H]. With each
cluster j ∈ [H], we associate a tuple Sj = (Aj, Bj, Qj, Rj), comprising a system matrix Aj ∈ Rn×n,
a control input matrix Bj ∈ Rn×m, and two positive definite matrices Qj ∈ Rn×n, Rj ∈ Rm×m that
define the LQR cost function for cluster j. Each agent in cluster j interacts with the same instance
of the LQR problem specified by Sj, and aims to find a linear policy of the form ut = −Kxt that
minimizes the following infinite-horizon discounted cost:

Cj(K) = E

(cid:34) ∞
(cid:88)

t=0

γt (cid:16)

t Qjxt + u⊤
x⊤

t Rjut

(cid:35)

(cid:17)

subject to xt+1 = Ajxt + Bjut + zt,

(1)

where x0 = 0 and xt, ut, and zt are the state, control input (action), and exogenous process noise,
respectively, at time t, γ ∈ (0, 1) is a discount factor, and K is a control gain matrix. We make the
standard assumption that the pair (Aj, Bj) is controllable for every j ∈ [H]. Following Malik et al.
(2020), we assume that zt is sampled independently from a distribution D, such that:

E[zt] = 0, E[ztz⊤

t ] = I, and ∥zt∥2

2 ≤ B, ∀t,

(2)

where B > 0 is some positive constant. For the LQR problem described in (1), it is well known (Bert-
sekas, 2015) that the optimal control law is a linear feedback policy of the form ut = −K∗
j xt, where
K∗
j is the optimal control gain matrix for cluster j. When Sj is known, each agent in Mj can obtain
K∗
j by solving the discrete-time algebraic Riccati equation (DARE) (Anderson and Moore, 2007).
However, our interest is in the learning scenario where the system matrices {(Aj, Bj)}j∈[H] are
unknown to the agents. Even in this setting, it is known that policy optimization (PO) algorithms that
treat the control gain as the optimization variable converge to the optimal policy (Fazel et al., 2018;
Malik et al., 2020). The implementation of such algorithms relies on noisy trajectory rollouts to
compute estimates of policy gradients.1 Specifically, given T independent rollouts from the tuple Sj,
each agent within Mj can generate a gain ˆK such that with high probability, Cj( ˆK) − Cj(K∗
j ) ≤
˜O(1/
T ) (Malik et al., 2020). Our goal is to investigate whether this sample-complexity bound can
be improved by leveraging the cluster structure in our problem.

√

To achieve potential gains in sample-complexity via collaboration, we allow the agents to
communicate via a central server, and make the following assumption that is common in the literature
on collaborative/federated learning (Koneˇcn`y et al., 2016; McMahan et al., 2017).

Assumption 1 The noise processes across agents are statistically independent, i.e., for all i1, i2 ∈
[N ] such that i1 ̸= i2, the noise stochastic processes {z(i1)
} are mutually independent.
Here, with a slight overload of notation, we use {z(i)
t } to denote the noise process for agent i ∈ [N ].

} and {z(i2)

t

t

Although the above assumption suggests that exchange of information between agents can accelerate
the learning of an optimal policy, collaboration is complicated by the heterogeneity among clusters,
due to the difference in system dynamics (Aj, Bj) and in task objectives (Qj, Rj). To capture such

1. The notion of a rollout will be made precise later in this section.

4

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

heterogeneity across clusters, we take inspiration from the notion of “cluster separation gaps" in
supervised learning (Ghosh et al., 2022; Su et al., 2022; Sattler et al., 2020), and introduce

∆ :=

min
j1,j2∈[H]:j1̸=j2

|Cj1(K∗

j1) − Cj2(K∗

j2)|.

(3)

We assume a non-zero separation across clusters, i.e., ∆ > 0. While one can certainly formu-
late alternative notions of heterogeneity, we will show (in Theorem 2) that our metric of cluster-
dissimilarity, as captured by ∆ in (3), can be suitably exploited to separate clusters and learn
personalized policies for each cluster. In particular, our results reveal that heterogeneity can be
helpful: a larger value of ∆ leads to faster cluster separation using fewer rollouts. With the above
ideas in place, we are now ready to formally state our problem of interest. For each agent i ∈ [N ],
let us use σ(i) to represent the index of the cluster to which it belongs.

Problem 1 (Clustered LQR Problem) Let δ ∈ (0, 1) be a given failure probability. Suppose
every agent i ∈ [N ] has access to T independent rollouts from its corresponding system Sσ(i).
Develop an algorithm that returns { ˆKi}i∈[N ] such that with probability at least 1 − δ, the
following is true ∀i ∈ [N ]:

Cσ(i)( ˆKi) − Cσ(i)(K∗

σ(i)) ≤ ˜O





1

(cid:113)

|Mσ(i)|T



 .

In simple words, our goal is to come up with an algorithm that generates a personalized control
policy for every agent that benefits from the collective information available within that agent’s
cluster. This is quite non-trivial due to the following technical challenges.

• In our setting, the system dynamics and the cluster identities are both unknown a priori. Thus,

our problem requires learning the cluster identities and optimal policies simultaneously.

• The clustering process is complicated by two main issues. First, the information used for
clustering is based on noisy function evaluations that are insufficient for estimating the system models,
ruling out system-identification-based approaches in Toso et al. (2023) and Rui and Dahleh (2025).
Thus, we need to develop a model-free clustering algorithm. Second, the minimum separation gap ∆
in (3) is assumed to be unknown, ruling out the possibility of simple one-shot clustering approaches.
• Unlike supervised learning problems in Ghosh et al. (2020) and Su et al. (2022) where
misclustering only introduces a bias due to heterogeneity, the price of misclustering can be more
severe in our control setting. In particular, since the system tuples across clusters are allowed to be
arbitrarily different, transfer of information across clusters can lead to destabilizing policies.

In the next section, we will develop the PCPO algorithm that addresses the above challenges
and solves Problem 1, while incurring only a logarithmic (with respect to the number of agents and
rollouts) communication cost. In preparation for the next section, we now define the notion of a
rollout and a zeroth-order gradient estimator. Given a policy K, a rollout for an agent i ∈ Mj yields
a noisy sample of the infinite-horizon trajectory cost, defined as:

Cj(K; Z (i)) =

γt (cid:16)

∞
(cid:88)

t=0

t Qjxt + u⊤
x⊤

t Rjut

(cid:17)

, where xt+1 = Ajxt + Bjut + zt, ut = −Kxt, (4)

5

KANAKERI BAJAJ VERMA GUPTA MITRA

x0 = 0 and Z (i) = {z(i)
t }. We will interpret each rollout as a sample. Using such noisy function
evaluations for a policy K run by an agent i ∈ Mj, we define the M -minibatched zeroth-order
gradient estimator with a smoothing radius r, as follows (Fazel et al., 2018; Malik et al., 2020):

gi(K) :=

1
M

M
(cid:88)

k=1

Cj(K + rUk; Z (i)
k )

(cid:19)

(cid:18) D
r

Uk,

(5)

where D = mn, Uk is drawn independently from a uniform distribution over matrices with unit
Frobenius norm, and Z (i)
k are independent copies of Z (i) for all k ∈ [M ]. In the sequel, for an
agent i ∈ Mj, we use the shorthand ZOi(K, M, r) to refer to the M -minibatched zeroth-order
gradient estimator at policy K with smoothing radius r, as defined in (5). We use the notation c(p,_)
to denote problem-parameter-dependent constants and provide their expressions in Appendix A of
Kanakeri et al. (2025). We make the standard assumption (Fazel et al., 2018; Malik et al., 2020)
that each agent i has access to an initial controller K(0)
that lies within its respective stabilizing set:
{K ∈ Rm×n : ρ(Aσ(i) − Bσ(i)K) < 1}, where ρ(X) is the spectral radius of a matrix X ∈ Rn×n.

i

3. Description of the Algorithm

In this section, we present our proposed algorithm, Personalized and Collaborative Policy Opti-
mization (PCPO) (Algorithm 1), which effectively addresses Problem 1 by carefully accounting for
its inherent challenges. Since the cluster separation gap ∆ is unknown, we propose a sequential
elimination strategy to identify the correct clusters. While the idea of sequential elimination has
been explored in the context of multi-armed bandits (Even-Dar et al., 2006), we show that a similar
approach can be used to effectively cluster LTI systems that satisfy the heterogeneity metric defined
in (3). The algorithm proceeds in epochs (indexed by l) of increasing duration, where in each epoch,
each agent i updates two sequences: a local sequence, {X (l)
i }l≥0.
The local sequence is updated using the zeroth-order policy optimization algorithm in Fazel et al.
(2018); Malik et al. (2020), and is used exclusively for clustering the agents. The global sequence is
updated by aggregating the gradient estimates from all the agents within an appropriately defined
neighborhood set. After each epoch, the neighborhood sets are updated based on the concentration
of the estimated cost around the optimal cost. As the number of rollouts increases across epochs, this
concentration becomes tighter, hence pruning out misclustered agents over successive epochs. The
various components of the algorithm are succinctly captured in Figure 1. In Section 4, we show that
the neighborhood sets eventually converge to the correct clusters with high probability, after which
the global sequence enjoys the collaborative gains without any heterogeneity bias. In what follows,
we elaborate on the rationale behind the design of the various components of PCPO.

i }l≥0, and a global sequence, { ˆK(l)

Building intuition. Correctly identifying the agents’ clusters is crucial to reap any potential ben-
efits from collaboration, as collaborating with agents from a different cluster can lead to destabilizing
policies. In this regard, the major difficulty arises from the fact that the cluster separation gap ∆
in (3) is unknown a priori. To appreciate the associated challenges, as a thought experiment, let us
consider a simpler case where ∆ is known. Under this scenario, each agent can locally run policy
optimization to obtain a policy in a sufficiently close neighborhood of the optimal policy. Then, each
agent can evaluate the cost at this policy and ensure that it is concentrated around the optimal cost.
If the neighborhood radius, which depends on ∆, is carefully chosen, it is easy to see that such a
one-shot approach leads to correct clustering. However, when ∆ is unknown, one-shot clustering
may no longer work, motivating the sequential clustering idea in our proposed PCPO Algorithm.

6

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

Figure 1: Illustration of the epoch-based structure of PCPO, where each epoch involves three key

steps: local policy optimization (PO), cost estimation, and global PO.

i

Sequential elimination. In each epoch l of PCPO, for every agent i ∈ [N ], the server maintains
a neighborhood set N (l)
as an estimate of the true cluster Mσ(i). All such neighborhood sets are
initialized from the set of all agents and sequentially pruned over epochs. For pruning, we start with
an initial estimate of ∆, denoted by ∆0, and update it by halving its value at the beginning of each
epoch l to obtain ∆l (Line 3 of Algo. 1). Our goal is to ensure that for all agents, the estimated cost
in epoch l is in the ∆l/4 neighborhood of its optimal cost. We achieve this in a two-step process,
where we first perform local policy optimization to obtain a policy that is ∆l/8-suboptimal (Line 4).
The localPO subroutine performs Ml-minibatched policy optimization for Rl iterations starting
with a controller X (l−1)
. In every iteration t ∈ [Rl], X(i,t), the t-th sub-iterate of localPO, is
updated as X(i,t+1) ← X(i,t) − ηgi(X(i,t)), where gi(X(i,t)) = ZOi(X(i,t), Ml, r(loc)
). Then, we
estimate the cost at the policy X (l)
i obtained from localPO with an error tolerance of ∆l/8 using
Ml rollouts (Line 5). Having achieved the desired cost-estimation accuracy of ∆l/4 for all agents,
we prune the neighborhood sets according to (7) in Line 13. Eventually, as ∆l ≤ ∆, which happens
in O(log(∆0/∆)) epochs, correct clustering takes place, as elaborated in the next section.

i

l

Local and Global Sequences. To motivate the need for maintaining two sequences in PCPO, let
us again consider the case where ∆ is known, where it would suffice for the agents to only maintain a
single sequence to run local PO until clustering, as discussed earlier in the one-shot clustering scheme.
After the clusters are identified, the same sequence can be used for collaboration. In our setting,
however, although the neighborhood sets eventually converge to the correct clusters in logarithmic
number of epochs with respect to 1/∆, the number of such epochs cannot be determined a priori with-
out knowledge of ∆, making it difficult to decide when to initiate collaboration. Furthermore, with a
single sequence, collaborating with misclustered agents can lead to an undesirable scenario where the
sequence used to cluster is itself contaminated due to misclustering. PCPO carefully navigates this
difficulty by maintaining two sequences of policies at each agent. The local sequence, {X (l)
i }l≥0, is
used purely for clustering, and the global sequence, { ˆK(l)
i }l≥0, is updated by aggregating gradients
from agents within the neighborhood set N (l−1)

from the previous epoch l − 1; see (6) in Line 9.

i

Logarithmic communication. Since both local and global PO are performed for Rl iterations
with Ml rollouts per iteration, the overall sample complexity per epoch is Tl = 2RlMl + Ml, where
the additional Ml rollouts are due to the cost estimation step. Each iteration of global PO proceeds as
follows. First, for every agent, the server combines the minibatched zeroth-order gradient estimates
as per (6). Then, the agents update the iterates using the averaged gradient with an appropriately
chosen but fixed step size η (Line 10). Since the above essentially incurs O(Rl) communication steps

7

KANAKERI BAJAJ VERMA GUPTA MITRA

Algorithm 1 Personalized and Collaborative Policy Optimization (PCPO)
1: Initialization: ∆0; ∀i ∈ [N ], ˆK(0)
2: For l = 1, 2, . . . ,

i ← K(0)

i ← K(0)

, N (0)

, X (0)

i ← [N ].

i

i

At Each Agent i: ∆l ← ∆l−1
2

2l2 , η ← c(p,1), Rl ← c(p,2) log

(cid:17)

(cid:16) c(p,3)N
∆2
l

Ml ←

c(p,4)
∆2
l

log

(cid:16) 8DN Rl
δl

(cid:17)

, ˜rl ←

(cid:114)

log

(cid:16) 8DN Rl
δl

(cid:17)(cid:19)1/2

, r(loc)

l ← min{c(p,6), ˜rl}.

, δl ← δ
(cid:18) c(p,5)√

Ml

Local Policy Optimization: X (l)
Cost estimation: ˆCσ(i)(X (l)
Initialize Y (0)

i ← ˆK(l−1)
For k = 0, 1, . . . , Rl − 1

i ← localPO(X (l−1)
j=1 Cσ(i)(X (l)

i ) ← 1
Ml
and set r(global)
(i,l) ← min

(cid:80)Ml

(cid:26)

i

i

i

, Ml, Rl, r(loc)
, Z (i)
j ).

l

).

(cid:27)

c(p,6),

˜rl
|N (l−1)
i

|1/4

At Each Agent i: Transmit gi(Y (k)
) to the Server.
At Server: Compute and transmit the averaged gradient estimate as follows:

) ← ZOi(Y (k)

, Ml, r(global)

(i,l)

i

i

▷ For collaborative PO

Gi ←

1
|N (l−1)
i

|

(cid:88)

gj(Y (k)
j

).

j∈N (l−1)
i

(6)

At Each Agent i: Y (k+1)

i

← Y (k)

i − ηGi.

▷ Global policy update via collaboration

End For
. Transmit X (l)
At Each Agent i: Update ˆK(l)
i
At Server: Update the neighborhood set as follows:

i ← Y (Rl)

i

, ˆCσ(i)(X (l)

i ) to the server.

▷ Sequential elimination

N (l)

i ← {j ∈ N (l−1)

i

(cid:12)
ˆCσ(j)(X (l)
(cid:12)
(cid:12)

(cid:12)
j ) − ˆCσ(i)(X (l)
(cid:12)
(cid:12) ≤ ∆l/2}
i )

:

(7)

If N (l)
i

̸= N (l−1)
i

for some i ∈ [N ]:
For all agents i ∈ [N ], update and transmit ˆK(l)
i

▷ Reinitialization to ensure stability

as follows:

ˆK(l)

i ← argmin
:j∈N (l)
{X (l)
i }
j

{ ˆCσ(j)(X (l)

j ) : j ∈ N (l)

i }.

(8)

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16: End For

per epoch, the total communication complexity of PCPO is O(Rl × Number of Epochs). Based on
our choice of parameters, both objects in the above product are logarithmic in the number of agents
N , the gap 1/∆, and the number of total rollouts per agent, namely T .

Note on reinitialization. Since the server averages gradients in every epoch based on the
neighborhood set, agents inevitably collaborate across clusters until correct clustering is achieved.
This can lead to destabilizing policies in the global sequence. To mitigate this, at the end of each
epoch, we reinitialize the global policy sequences for all agents whenever any neighborhood set is
updated, as specified in (8). In the next section, we establish in Theorem 1 that the neighborhood
sets eventually converge to the correct clusters and cease to update. Consequently, reinitialization

8

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

ensures that the global sequences of agents within the same cluster evolve identically and achieve
collaborative gains once the correct clusters are identified.

4. Main Results

Our main results concern the two key components of the PCPO algorithm: (i) identifying the
correct clusters via sequential elimination, and (ii) performing collaborative policy optimization with
logarithmic communication. The following theorem captures the clustering component.

Theorem 1 (Clustering with sequential elimination) Define L = min{l ∈ 1, 2, . . . : ∆l ≤ ∆/2}.
Given a failure probability δ ∈ (0, 1), with probability at least 1 − δ/2, the following statements
concerning the neighborhood sets from the PCPO algorithm hold for every agent i ∈ [N ]:

1. In every epoch l, we have Mσ(i) ⊆ N (l)
i

.

2. For any epoch l such that l ≥ L, we have Mσ(i) = N (l)
i

.

Discussion. The key technical contribution of Theorem 1 lies in showing that the sequential
elimination strategy successfully lets the neighborhood sets converge to the correct clusters with high
probability. In particular, we show that the true clusters are included in the neighborhood sets for all
agents in each epoch, and there exists an epoch L after which the neighborhood sets have converged
to the true clusters and remain fixed for all subsequent epochs (l ≥ L). Later in this section, we
provide a proof sketch and discuss how these claims follow from the local policy optimization and
the cost estimation step, relying on our notion of the cluster separation gap as defined in (3). The
following theorem captures the key result concerning the collaborative optimization part in PCPO.

Theorem 2 (Collaborative Policy Optimization) Let the failure probability be δ ∈ (0, 1). Define
L = min{l ∈ 1, 2, . . . : ∆l ≤ ∆/2} and let ¯L denote the last epoch. If the number of rollouts per
agent satisfies T ≥ ˜O(1/∆2), and Assumption 1 holds, then ¯L > L and ˆK( ¯L)
satisfies the following
with probability at least 1 − δ for every agent i ∈ [N ]:

i

Cσ(i)( ˆK( ¯L)

i

) − Cσ(i)(K∗

σ(i)) ≤ O





(cid:113)

c(p,7)
(cid:113)

log (cid:0) 8DN T

δ



(cid:1)

 .

(9)

T |Mσ(i)|

√

Discussion. It was shown in Malik et al. (2020) that zeroth-order policy optimization provides a
˜O(1/
T ) suboptimal policy using T rollouts for a single system LQR problem. In contrast, Wang
et al. (2023a) showed collaborative gains for a federated LQR setting while incurring additive bias
terms that depend on the heterogeneity gap. Moreover, the results in Wang et al. (2023a) apply only
to systems with bounded heterogeneity. Theorem 2 bridges this gap by showing that collaborative
|Mσ(i)|-factor speedup for each agent i in (9)) can be achieved without
gains (as evidenced by the
any additive bias through careful cluster identification and collaboration exclusively within clusters.
Furthermore, these gains hold for systems that can be arbitrarily different, as long as their optimal
costs are separated according to (3).

(cid:113)

Corollary 3 (Logarithmic communication complexity) The PCPO algorithm guarantees a loga-
rithmic communication complexity with respect to the total number of rollouts T , the number of
agents N , and the inverse of the separation gap 1/∆.

9

KANAKERI BAJAJ VERMA GUPTA MITRA

In the following, we provide proof sketches for both Theorem 1 and Theorem 2, while deferring
the detailed proofs to Kanakeri et al. (2025). The statements made in the proof sketches are proba-
bilistic in nature. However, to keep the exposition simpler, we omit specifying the success/failure
probability of the statements and refer the readers to Kanakeri et al. (2025) for such details.

Proof sketch for Theorem 1. We start by showing that the estimated cost for each agent is in the

∆l/4 neighborhood of its optimal cost, i.e., in each epoch l, for each agent i, we prove that

(10)

| ˆCσ(i)(X (l)

σ(i))| ≤ ∆l/4.
σ(i)) = Cσ(j)(K∗

i ) − Cσ(i)(K∗
Note that for any agent j ∈ Mσ(i), since Cσ(i)(K∗
σ(j)), in light of (10), it is
apparent that such an agent will pass the requirement in (7), and hence, never be eliminated from the
neighborhood sets of agent i. This explains the first claim of Theorem 1. As for the second claim, note
that for any j /∈ Mσ(i), in light of our dissimilarity metric ∆ in (3), |Cσ(i)(K∗
σ(j))| ≥
∆. Now for an epoch l such that ∆l ≤ ∆/2, the above inequality can be combined with that in (10)
to see that | ˆCσ(i)(X (l)
j )| ≥ 3∆/4, violating the requirement for inclusion in (7). It
remains to establish (10), which follows from two guarantees: (i) | ˆCσ(i)(X (l)
i )| ≤
∆l/8, and (ii) |Cσ(i)(X (l)
σ(i))| ≤ ∆l/8. The second guarantee follows from an analysis
of the local PO sub-routine in Line 4 of Algo. 1, drawing on Malik et al. (2020); the first follows
from analyzing the cost estimation step in Line 5 based on a simple Hoeffding bound.

i ) − ˆCσ(j)(X (l)

i ) − Cσ(i)(X (l)

σ(i)) − Cσ(j)(K∗

i ) − Cσ(i)(K∗

Proof sketch for Theorem 2. We prove Theorem 2 by conditioning on the event where the claims
made in Theorem 1 hold. As agents collaborate exclusively within their respective clusters and the
reinitialization step synchronizes their global sequences after correct clustering, the gradient estimate
obtained by averaging (see (6)) estimates of agents within a cluster enjoys a variance reduction effect
under Assumption 1. Using this, and the fact that the LQR cost satisfies a ϕ-smoothness and µ-PL
condition locally (Fazel et al., 2018; Malik et al., 2020), we establish that in each iteration k of the
final epoch ¯L, the following recursion holds with probability 1 − δ′:




Sk+1 ≤

(cid:16)

1 −

(cid:17)

ηµ
4

Sk + 3η


(p,8)D2
c2



(r(global)
)2|Mσ(i)|M ¯L

(i, ¯L)

(cid:124)

(cid:123)(cid:122)
s1

log

(cid:19)

(cid:18) 2D
δ′

(cid:125)

+ ϕ2(r(global)

(cid:124)

(i, ¯L)
(cid:123)(cid:122)
s2



)2



(cid:125)


,

(11)

i

) − Cσ(i)(K∗

where Sk := Cσ(i)(Y (k)
σ(i)). The term s1 is due to the concentration of the minibatched
gradient estimate around the gradient of a smoothed cost defined in Appendix A of Kanakeri et al.
(2025); this is the term that benefits from collaboration. The term s2 captures the bias that arises
when estimating gradients from noisy function evaluations. To ensure that this bias term does not
negate the collaborative speedup in s1, we choose the smoothing radius r(global)
to minimize the
sum s1 + s2. Unrolling the recursion for Rl iterations (with the choice of Rl in PCPO) provides
the per epoch convergence with rate ˜O
. Finally, using the fact that Ml increases
|Mσ(i)|M ¯L
exponentially with epochs, we establish that M ¯L = ˜Ω(T ).

(i, ¯L)

(cid:113)

1/

(cid:17)

(cid:16)

5. Conclusion

We developed a novel clustering-based approach for learning personalized control policies using data
from heterogeneous dynamical processes. As future work, we will explore (i) alternative measures of
dissimilarity across systems, (ii) more general dynamical processes, and (iii) online settings.

10

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

References

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1–76, 2021.

Brian DO Anderson and John B Moore. Optimal control: linear quadratic methods. Courier

Corporation, 2007.

Ashwin Aravind, Mohammad Taha Toghani, and César A Uribe. A moreau envelope approach for
LQR meta-policy estimation. In 2024 IEEE 63rd Conference on Decision and Control (CDC),
pages 415–420. IEEE, 2024.

Dimitri P Bertsekas. Dynamic programming and optimal control 4th edition, volume ii. Athena

Scientific, 2015.

Yiting Chen, Ana M Ospina, Fabio Pasqualetti, and Emiliano Dall’Anese. Multi-task system
identification of similar linear time-invariant dynamical systems. In 2023 62nd IEEE Conference
on Decision and Control (CDC), pages 7342–7349. IEEE, 2023.

Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and
stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of
machine learning research, 7(6), 2006.

Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient
methods for the linear quadratic regulator. In Int. Conf. on Machine Learning, pages 1467–1476.
PMLR, 2018.

Tesshu Fujinami, Bruce D Lee, Nikolai Matni, and George J Pappas. Domain randomization is

sample efficient for linear quadratic control. arXiv preprint arXiv:2502.12310, 2025.

Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for
clustered federated learning. Advances in neural information processing systems, 33:19586–19597,
2020.

Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for
clustered federated learning. IEEE Transactions on Information Theory, 68(12):8076–8091, 2022.

Benjamin Gravell, Peyman Mohajerin Esfahani, and Tyler Summers. Learning optimal controllers
for linear systems with multiplicative noise via policy gradient. IEEE Transactions on Automatic
Control, 66(11):5283–5298, 2020.

Taosha Guo, Abed AlRahman Al Makdah, Vishaal Krishnan, and Fabio Pasqualetti. Imitation and

transfer learning for lqg control. IEEE Control Systems Letters, 7:2149–2154, 2023.

Bin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer Ba¸sar. Toward a
theoretical foundation of policy optimization for learning control policies. Annual Review of
Control, Robotics, and Autonomous Systems, 6(1):123–158, 2023.

11

KANAKERI BAJAJ VERMA GUPTA MITRA

Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note
arXiv preprint

on concentration inequalities for random vectors with subgaussian norm.
arXiv:1902.03736, 2019.

Vinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, and Aritra Mitra. Harnessing data from
clustered LQR systems: Personalized and collaborative policy optimization. arXiv preprint, 2025.

Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.

Bruce D Lee, Leonardo F Toso, Thomas T Zhang, James Anderson, and Nikolai Matni. Regret
analysis of multi-task representation learning for linear-quadratic adaptive control. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 39, pages 18062–18070, 2025.

Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter L Bartlett, and Martin J
Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic
systems. Journal of Machine Learning Research, 21(21):1–51, 2020.

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pages 1273–1282. PMLR, 2017.

Aditya Modi, Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Joint

learning of linear time-invariant dynamical systems. Automatica, 164:111635, 2024.

Amirreza Neshaei Moghaddam, Alex Olshevsky, and Bahman Gharesifard. Sample complexity
of the linear quadratic regulator: A reinforcement learning lens. Journal of Machine Learning
Research, 26(151):1–50, 2025.

Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanovi´c. Global
exponential convergence of gradient methods over the nonconvex landscape of the linear quadratic
regulator. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 7474–7479.
IEEE, 2019.

Hesameddin Mohammadi, Mahdi Soltanolkotabi, and Mihailo R Jovanovi´c. On the linear conver-
gence of random search for discrete-time LQR. IEEE Control Systems Letters, 5(3):989–994,
2020.

Maryann Rui and Munther A Dahleh. Learning clusters of partially observed linear dynamical

systems. In 2025 American Control Conference (ACC), pages 3545–3550. IEEE, 2025.

Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. Clustered federated learning: Model-
agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural
networks and learning systems, 32(8):3710–3722, 2020.

Charis Stamouli, Leonardo F Toso, Anastasios Tsiamis, George J Pappas, and James Anderson.

Policy gradient bounds in multitask LQR. arXiv preprint arXiv:2509.19266, 2025.

12

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

Lili Su, Jiaming Xu, and Pengkun Yang. Global convergence of federated learning for mixed

regression. Advances in Neural Information Processing Systems, 35:29889–29902, 2022.

Leonardo F Toso, Han Wang, and James Anderson. Learning personalized models with clustered
system identification. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages
7162–7169. IEEE, 2023.

Leonardo Felipe Toso, Donglin Zhan, James Anderson, and Han Wang. Meta-learning linear
In 6th Annual

quadratic regulators: a policy gradient maml approach for model-free LQR.
Learning for Dynamics & Control Conference, pages 902–915. PMLR, 2024.

Omkar Tupe, Max Hartman, Lav R Varshney, and Saurav Prakash. Federated nonlinear system

identification. arXiv preprint arXiv:2508.15025, 2025.

Han Wang, Leonardo F Toso, Aritra Mitra, and James Anderson. Model-free learning with het-
erogeneous dynamical systems: A federated LQR approach. arXiv preprint arXiv:2308.11743,
2023a.

Han Wang, Leonardo Felipe Toso, and James Anderson. Fedsysid: A federated approach to
sample-efficient system identification. In Learning for Dynamics and Control Conference, pages
1308–1320. PMLR, 2023b.

Lei Xin, Lintao Ye, George Chiu, and Shreyas Sundaram. Learning dynamical systems by leveraging

data from similar systems. IEEE Transactions on Automatic Control, 2025.

Kaiqing Zhang, Bin Hu, and Tamer Basar. Policy optimization for h2 linear control with h∞
robustness guarantee: Implicit regularization and global convergence. SIAM Journal on Control
and Optimization, 59(6):4081–4109, 2021.

Thomas T Zhang, Katie Kang, Bruce D Lee, Claire Tomlin, Sergey Levine, Stephen Tu, and Nikolai
Matni. Multi-task imitation learning for linear dynamical systems. In Learning for Dynamics and
Control Conference, pages 586–599. PMLR, 2023.

13

KANAKERI BAJAJ VERMA GUPTA MITRA

Appendix A. Properties of the LQR problem

Notation. For matrices A ∈ Rm×n and B ∈ Rm×n, we use ∥A∥ to denote the Frobenius norm of
A which is defined as ∥A∥ = (cid:112)trace(A⊤A). We use ⟨A, B⟩ to denote the Frobenius inner-product
defined as ⟨A, B⟩ = trace(A⊤B).

In this section, we discuss some of the properties of the LQR cost that were established in Fazel
et al. (2018); Malik et al. (2020). In particular, the LQR cost in (1) is locally Lipschitz, locally
smooth, and enjoys a gradient-domination property over the set of stabilizing controllers. These key
properties aid in the convergence analysis of the model-free policy gradient algorithm, and we use
them in the proofs of Theorem 1 in Appendix B and Theorem 2 in Appendix C. However, to show
the convergence, it is crucial to ensure that the policy gradient iterates always lie within a restricted
subset of the stabilizing set with high probability. In Malik et al. (2020), such a restricted set is
chosen based on the initial suboptimality gap. In our setting, given access to a set of initial stabilizing
controllers for all agents, {K(0)
i }i∈[N ], and our initial guess for the cluster separation gap, ∆0, we
define ˜∆0 := max{maxi∈[N ](Cσ(i)(K(0)
σ(i))), ∆0}, and the restricted sets as follows
for all j ∈ [H]:

) − Cσ(i)(K∗

i

(12)

j ) ≤ 10 ˜∆0}.

j := {K ∈ Rm×n : Cj(K) − Cj(K∗
G0
Properties of the LQR cost. For each system j ∈ [H], on the restricted domain G0

j , Malik et al.
(2020) showed that the local properties hold uniformly, i.e., ∃ϕj > 0, λj > 0, ρj > 0, such that the
LQR cost in (1) is (λj, ρj)-locally Lipschitz and (ϕj, ρj)-locally smooth for all policies K ∈ G0
j .
Furthermore, it is known that the LQR cost satisfies the PL (gradient-domination) condition for all
policies in the stabilizing set (see Lemma 3 of Malik et al. (2020)). Denoting the parameter for the
PL condition for system j by µj > 0, we define µ := min{µ1, µ2, . . . , µH }. Similarly, defining
ϕ := max{ϕ1, ϕ2, . . . , ϕH }, λ := max{λ1, λ2, . . . , λH }, and ρ := min{ρ1, ρ2, . . . , ρH }, we can
ensure that for every system j ∈ [H], the cost in (1) is (λ, ρ)-locally Lipschitz and (ϕ, ρ)-locally
smooth for all policies in their respective restricted sets G0
j , and µ-PL in their respective stabilizing
sets. The following lemmas from Malik et al. (2020) capture these properties.

Lemma 4 (LQR cost is locally Lipschitz). For any system j ∈ [H], given a pair of policies
(K, K′) ∈ (G0

j × G0

j ), if ∥K − K′∥ ≤ ρ, we have
(cid:12)Cj(K) − Cj(K′)(cid:12)
(cid:12)

(cid:12) ≤ λ∥K − K′∥.

Lemma 5 (LQR cost has locally Lipschitz gradients.) For any system j ∈ [H], given a pair of
policies (K, K′) ∈ (G0

j × G0

j ), if ∥K − K′∥ ≤ ρ, we have
(cid:13)∇Cj(K) − ∇Cj(K′)(cid:13)
(cid:13)

(cid:13) ≤ ϕ∥K − K′∥.

Lemma 6 (LQR cost satisfies PL.) For any system j ∈ [H], given a stable policy K, we have

∥∇Cj(K)∥2 ≥ µ(Cj(K) − Cj(K∗

j )).

Smoothed cost and the properties of the gradient estimate. The smoothed cost with a radius
r for a system j ∈ [H] is defined as Cj,r(K) := E[Cj(K + rv)], where v is uniformly distributed
over all matrices in Rm×n with the Frobenius norm of at most 1. It is shown in Fazel et al. (2018);
Malik et al. (2020) that the zeroth-order gradient estimate gi(·) as defined in (5) for an agent i is an

14

(13)

(14)

(15)

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

unbiased estimator of the gradient of the smoothed cost. In particular, for all systems j ∈ [H] and all
agents i ∈ Mj, we have the following properties for all K ∈ G0

j and r ∈ (0, ρ):

E[gi(K)] = ∇Cσ(i),r(K)

∥∇Cj,r(K) − ∇Cj(K)∥ ≤ ϕr.

(16)

(17)

j , ∀r ∈ (0, ρ) and all U ∈ Rm×n with ∥U ∥ = 1. In other words, there exists G(j)

Furthermore, it is known that the noisy rollout cost Cj(K + rU ; Z (i)) is uniformly bounded
∞ ≥ 0 such
∞ .) Let
∞ }. Hence, for any agent i ∈ [N ], we have the following

∞ (see Lemma 11 of Malik et al. (2020) for the expression of G(j)

∀K ∈ G0
that Cj(K + rU ; Z (i)) ≤ G(j)
us define G∞ := max{G(1)
∀K ∈ G0

j , ∀r ∈ (0, ρ) and all U ∈ Rm×n with ∥U ∥ = 1:

∞ , . . . , G(H)

∞ , G(2)

Cσ(i)(K + rU ; Z (i)) ≤ G∞.

(18)

We then have the following concentration result that will prove useful in establishing “variance-

reduction" effects.

Lemma 7 (Concentration of the zeroth-order gradient estimates). For any system j ∈ [H], given
a policy K ∈ G0
j , a smoothing radius r ∈ (0, ρ) and a failure probability δ′ ∈ (0, 1), the following
holds for the M -minibatched zeroth-order gradient estimate of an agent i ∈ Mj with probability at
least 1 − δ′:

∥gi(K) − ∇Cj,r(K)∥ ≤

(cid:16)

D + ϕ ρ2
G∞ + λ ρ
√
M

D

r

(cid:17)

(cid:115)

D

log

(cid:18) 2D
δ′

(cid:19)
.

(19)

(cid:13)
(cid:13)
k=1(g(i,k)(K) − ∇Cj,r(K))
Proof We have ∥gi(K) − ∇Cj,r(K)∥ =
(cid:13), where we denoted
the k-th component of the minibatch as g(i,k)(K). Recall from (5) that this k-th component takes the
form

(cid:80)M

(cid:13)
(cid:13)
(cid:13)

1
M

g(i,k)(K) = Cj(K + rUk; Z (i)
k )

(cid:19)

(cid:18) D
r

Uk.

Hence, we have ∥g(i,k)(K)∥ ≤ D
We then have

r G∞ due to (18) as ∥Uk∥ = 1. Let us define c(p,8) :=

(cid:16)

G∞ + λ ρ

D + ϕ ρ2

D

(cid:17)

.

∥g(i,k)(K) − ∇Cj,r(K)∥ = ∥g(i,k)(K) − ∇Cj,r(K) + ∇Cj(K) − ∇Cj(K)∥

(a)
≤ ∥g(i,k)(K)∥ + ∥∇Cj,r(K) − ∇Cj(K)∥ + ∥∇Cj(K)∥
(b)
≤

D
r
D
r

D
r
D
r

=

(c)
≤

=

G∞ + ϕr + λ
(cid:18)

G∞ +

(cid:18)

G∞ +

r2
D
ρ2
D

ϕ +

ϕ +

(cid:19)

λ

r
D

(cid:19)

λ

ρ
D

c(p,8).

15

KANAKERI BAJAJ VERMA GUPTA MITRA

Table 1: Relevant notation and definitions

Notation
Ml
Rl
η
rl
∆l
N (l)
i
X (l)
i
ˆK(l)
i

Definition
Minibatch size used to estimate zeroth-order gradients in the l-th epoch.
Number of steps/iterations of the local and global policy optimization in the l-th epoch.
Step size for both local and global policy optimization.
Smoothing radius used in the zeroth-order gradient estimates in the l-th epoch.
Estimate of ∆ used to cluster the agents in the lth epoch.
Neighborhood set corresponding to the i-th agent in the l-th epoch.
Local policy for the i-th agent in the l-th epoch.
Global policy for the i-th agent in the l-th epoch.

In the above, (a) follows from the triangle inequality, and (b) follows from the uniform-boundedness
of the noisy rollout together with (5), the bounded bias property as shown in (17), and the local-
Lipschitz property in Lemma 4. Finally, (c) follows from using r < ρ. Hence, g(i,k)(K) − ∇Cj,r(K)
has a bounded norm and therefore belongs to a class of norm sub-Gaussian random matrices (Jin
et al., 2019). Furthermore, it has zero mean due to (16). Therefore, the concentration result follows
from a direct application of Corollary 7 from Jin et al. (2019) which provides a Hoeffding-type
inequality for norm sub-Gaussian random matrices.

Corollary 8 (Concentration of the collaborative zeroth-order gradient estimates.) Suppose As-
sumption 1 holds. For any system j ∈ [H], given a policy K ∈ G0
j , a smoothing radius r ∈ (0, ρ),
define the collaborative zeroth-order gradient estimate as Gj(K) = 1
gi(K), where
gi(K) is the M -minibatched gradient estimate from agent i ∈ Mj. Let δ′ ∈ (0, 1). The following
holds with probability at least 1 − δ′:

i∈Mj

|Mj |

(cid:80)

(cid:16)

∥Gj(K) − ∇Cj,r(K)∥ ≤

G∞ + λ ρ

D + ϕ ρ2
r(cid:112)|Mj|M

D

(cid:17)

(cid:115)

D

log

(cid:18) 2D
δ′

(cid:19)
.

(20)

Proof Under Assumption 1, we note that the noise processes for all agents in Mj are independent.
The proof then follows from Lemma 7 as the collaborative zeroth-order gradient estimate can be
interpreted as a gradient estimate for system j with a |Mj|-fold increased minibatch size.

Note on the problem dependent constants. In Malik et al. (2020), the values of the constants
λj, ϕj, ρj, G(j)
∞ are first derived locally in terms of the local cost Cj(K), and then, the global
parameters are obtained by noting that the local cost is uniformly bounded over the restricted domain
as shown in Lemma 9 of Malik et al. (2020). Since we have a different definition of the restricted
domain, the values of our parameters vary from the ones provided in Malik et al. (2020). That said,
the global parameters in our setting can be derived exactly in the same way as in Malik et al. (2020)
by bounding the local cost as Cj(K) ≤ 10 ˜∆0 + Cj(K∗

j ).

For convenience, we compile all the relevant notation in Table 1.
In the main text, we used the notation c(p,_) to denote the problem-parameter-dependent constants

which are defined in the following.

16

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

(cid:18)

c(p,8) =

G∞ + λ

ρ
D

+ ϕ

(cid:19)

ρ2
D

c(p,9) =

12c(p,8)
µ

(cid:18)

max

(cid:26)

(cid:112)ϕ,

(cid:27)(cid:19)2

1
ρ

(p,9)D2, c2
(cid:33)

(p,8)D2∆2

0, 36G2
∞

c(p,10) = max

∆2

0, 256c2

(cid:110)

c(p,11) =

c(p,12) =

(cid:32)

(cid:1)

∆2
0
c(p,10) log (cid:0) 8DN
(cid:32)
(cid:32)
4
ηµ

log

δ
c(p,10)N ˜∆2
0
∆2
0
(cid:113)

c(p,13) = 4 max{1, c(p,9)}






8
µ

,

1
4ϕ

,

c(p,1) = min

c(p,2) =

4
ηµ

(cid:33)

(cid:33)

+ log(4)

c(p,12) log(c(p,11)T )



ρ

λ + 2 max

(cid:110)√

ϕ, 1
ρ

(cid:111)



(cid:111)

(21)

c(p,3) = ˜∆0 max{16, 10c(p,10)}
c(p,4) = c(p,10)

c(p,5) =

c(p,8)D
ϕ

c(p,6) = ρ
c(p,7) = Dc(p,13)

Based on the above definitions of the problem-parameter-dependent constants, we provide the

values used for the hyperparameters in the l-th epoch of the PCPO algorithm in Table 2.

17

KANAKERI BAJAJ VERMA GUPTA MITRA

Table 2: Hyperparameters with their values in the lth epoch

Hyperparameters

Values

∆l

δl

η

Rl

Ml

˜rl

r(loc)
l

r(global)
l

∆0
2l

δ
2l2

c(p,1)

c(p,2) log

c(p,4)
∆2
l
(cid:114)

log

log

(cid:18) c(p,5)√

Ml

(cid:17)

(cid:17)

(cid:16) c(p,3)N
∆2
l
(cid:16) 8DN Rl
δl
(cid:16) 8DN Rl
δl

(cid:17)(cid:19)1/2

min{c(p,6), ˜rl}
(cid:26)

(cid:27)

min

c(p,6),

˜rl
|N (l−1)
i

|1/4

Appendix B. Proof of Theorem 1

In this section, we provide the proof of Theorem 1 which concerns the clustering aspect of the PCPO
algorithm. In particular, we show that, with high probability, the true clusters are included in the
neighborhood sets for all agents in each epoch, and if epoch l ≥ L = min{l ∈ 1, 2, . . . : ∆l ≤ ∆/2},
the neighborhood sets are identical to the clusters. More specifically, we show that for all agents
i ∈ [N ], with probability at least 1 − δ/2, Mσ(i) ⊆ N (l)
in every epoch l, and if l ≥ L, then
i
Mσ(i) = N (l)
i

.

To prove both claims, it suffices to show that with high probability, the estimated cost at a locally
optimized policy is concentrated in the ∆l/4-neighborhood of the optimal cost in every epoch l for
all agents i ∈ [N ]. To see this, consider a “good” event that occurs with probability 1 − δ/2 where
the following holds for all agents i ∈ [N ] in every epoch l (we will prove that such an event exists
later in this section):

| ˆCσ(i)(X (l)

i ) − Cσ(i)(K∗

σ(i))| ≤ ∆l/4.

(22)

On this “good” event, in what follows, we show that the first claim of Theorem 1 holds. Accordingly,
fix an agent i and consider an agent j ∈ Mσ(i). We now show by induction that j belongs to N (l)
i
in every epoch l. For the base case of induction, note that since we initialize the neighborhood sets
. Next, for an epoch l − 1 ≥ 1, let us assume that j ∈ N (l−1)
with all agents, j ∈ N (0)
. Since
σ(i)) = Cσ(j)(K∗
Cσ(i)(K∗
σ(j)) as a consequence of j ∈ Mσ(i), in the l-th epoch under the “good”

i

i

18

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

event where (22) holds, we have

| ˆCσ(i)(X (l)

i ) − ˆCσ(j)(X (l)

j )| ≤ | ˆCσ(i)(X (l)

i ) − Cσ(i)(K∗

σ(i))| + | ˆCσ(j)(X (l)

j ) − Cσ(j)(K∗

σ(j))|

≤ ∆l/4 + ∆l/4 = ∆l/2,

based on the neighborhood set update rule in (7). Hence, by induction,

implying that j ∈ N (l)
j ∈ N (l)

i

i

in every epoch, therefore establishing the first claim of Theorem 1.

Next, we show that on the “good” event where (22) holds for all agents in every epoch, the
second claim of Theorem 1 is also true. We prove this claim via contradiction. To proceed, suppose
that there exist an epoch l ≥ L, an agent i, and an agent j /∈ Mσ(i) such that j ∈ N (l)
. Then, we
i
have the following in light of the heterogeneity metric defined in (3):

∆ ≤

≤

≤

(cid:12)
(cid:12)Cσ(i)(K∗
(cid:12)
(cid:12)
(cid:12)Cσ(i)(K∗
(cid:12)
(cid:12)
(cid:12)Cσ(i)(K∗
(cid:12)

(cid:12)
(cid:12)
σ(j))
(cid:12)
i ) + ˆCσ(j)(X (l)
(cid:12)
ˆCσ(j)(X (l)
(cid:12)
(cid:12)

σ(i)) − Cσ(j)(K∗
σ(i)) − ˆCσ(i)(X (l)
(cid:12)
σ(i)) − ˆCσ(i)(X (l)
(cid:12)
(cid:12) +
i )
(cid:12)
ˆCσ(i)(X (l)
(cid:12)
(cid:12)

(cid:12)
i ) − ˆCσ(j)(X (l)
(cid:12)
j )
(cid:12)
(cid:12)
i ) − ˆCσ(j)(X (l)
(cid:12)
(cid:12) ,
j )

ˆCσ(i)(X (l)

(a)
≤ ∆l/4 + ∆l/4 +

(b)
≤ ∆/4 +

(cid:12)
(cid:12)
(cid:12)

j ) − Cσ(j)(K∗

j ) − Cσ(j)(K∗

σ(j)) + ˆCσ(i)(X (l)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) +

σ(j))

ˆCσ(i)(X (l)

i ) − ˆCσ(j)(X (l)
j )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
i ) − ˆCσ(j)(X (l)
(cid:12)
j )
(cid:12)

(cid:12)
(cid:12)
(cid:12)

ˆCσ(i)(X (l)

where (a) holds due to (22), and (b) follows as ∆l ≤ ∆/2 since l ≥ L. The above set of inequalities
(cid:12)
i ) − ˆCσ(j)(X (l)
(cid:12)
(cid:12) ≥ (3/4)∆ ≥ (3/2)∆l, contradicting our assumption that
j )
imply that
(cid:12)
(cid:12)
(cid:12) ≤ ∆l/2. Therefore, N (l)
i ) − ˆCσ(j)(X (l)
ˆCσ(i)(X (l)
j ∈ N (l)
(cid:12)
(cid:12)
i = Mσ(i) for all
j )
(cid:12)
l ≥ L, establishing the second claim of Theorem 1.

i which requires that

Now, it remains to prove that the “good” event where (22) holds for all agents in every epoch

occurs with probability at least 1 − δ/2. To do so, for an agent i ∈ [N ] in epoch l, we have

(cid:12)
(cid:12)
(cid:12)

ˆCσ(i)(X (l)

i ) − Cσ(i)(K∗

(cid:12)
(cid:12)
(cid:12) ≤
σ(i))

(cid:12)
(cid:12)Cσ(i)(X (l)
(cid:12)

i ) − Cσ(i)(K∗

σ(i))

(cid:12)
(cid:12)
(cid:12) +

(cid:12)
(cid:12)
(cid:12)

ˆCσ(i)(X (l)

(cid:12)
i ) − Cσ(i)(X (l)
(cid:12)
(cid:12) .
i )

Therefore, to show (22), it suffices to show the following two guarantees for all agents in every
epoch:

(a)

(b)

(cid:12)
(cid:12)Cσ(i)(X (l)
(cid:12)
(cid:12)
ˆCσ(i)(X (l)
(cid:12)
(cid:12)

i ) − Cσ(i)(K∗
σ(i))
i ) − Cσ(i)(X (l)
i )

(cid:12)
(cid:12)
(cid:12) ≤ ∆l/8
(cid:12)
(cid:12)
(cid:12) ≤ ∆l/8.

(23)

Now, let us establish the claims in (23) via induction across epochs. Let us assume that for a fixed
epoch l − 1 ≥ 1, the claims in (23) hold for all agents i ∈ [N ] in every epoch k ≤ {1, 2, . . . , l − 1},
with probability at least (1 − (cid:80)l−1
δj
2 ). Denoting this event as El−1, in the following, we show that
j=1
the claims in (23) hold in epoch l, ∀i ∈ [N ] with probability at least (1 − δl
2 ) conditioned on the
event El−1.

In the following, fixing an agent i ∈ [N ], we show: (a)

(cid:12)
(cid:12)Cσ(i)(X (l)
(cid:12)
with probability at least 1−δl/(4N ) conditioned on the event El−1, and (b)

i ) − Cσ(i)(K∗
(cid:12)
ˆCσ(i)(X (l)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ ∆l/8
σ(i))
(cid:12)
i ) − Cσ(i)(X (l)
(cid:12)
(cid:12) ≤
i )

19

KANAKERI BAJAJ VERMA GUPTA MITRA

∆l/8 with probability at least 1 − δl/(4N ) conditioned on the intersection of the events El−1 and the
one where item (a) in (23) holds. The following lemma provides the convergence of the local policy
optimization sub-routine in epoch l which aids in establishing claim (a) from (23).

Lemma 9 (Local Policy Optimization.) For any agent i ∈ Mj, given a policy K0 ∈ G0
j , let KR be
the output of the localPO(K0, M, R, r) subroutine with step size η. Then, for any δ′ ∈ (0, 1/R),
with probability at least 1 − δ′R, KR ∈ G0

j and we have the following:

Cj(KR) − Cj(K∗

j ) ≤

(cid:16)

1 −

(cid:17)R

ηµ
4

(Cj(K0) − Cj(K∗

j )) +

(cid:32)

c(p,9)D
√
M

(cid:115)

log

(cid:18) 2D
δ′

(cid:19)(cid:33)

,

(24)

when η = c(p,1), M ≥

c(p,4)
∆2
0

log(2D/δ′), r = min{ρ, ˜r}, where ˜r =

(cid:18) c(p,5)√

M

(cid:113)

log (cid:0) 2D
δ′

(cid:19)1/2

(cid:1)

.

The proof of Lemma 9 is provided in Appendix B.1. We use Lemma 9 to analyze the local policy
(cid:12)
i ) − Cσ(i)(K∗
(cid:12)
optimization step in line 4 of the PCPO algorithm that helps in establishing
(cid:12) ≤
σ(i))
∆l/8 with probability at least 1 − δl/(4N ). More precisely, the settings for the hyperparameters
(η, Ml, r(loc)
, Rl) from Table 2 meet the requirement for the corresponding hyperparameters in
Lemma 9. Furthermore, conditioned on the event El−1, claim (a) in (23) implies that X (l−1)
σ(i).
Hence, the following holds due to Lemma 9 with probability at least 1−δ′Rl for some δ′ ∈ (0, 1/Rl):

(cid:12)
(cid:12)Cσ(i)(X (l)
(cid:12)

∈ G0

i

l

Cσ(i)(X (l)

i )−Cσ(i)(K∗

σ(i)) ≤

1 −

(cid:17)Rl

ηµ
4

(cid:16)

(cid:124)

(Cσ(i)(X (l−1)

i
(cid:123)(cid:122)
s1

) − Cσ(i)(K∗

σ(i)))
(cid:125)

+

(cid:32)

(cid:124)

c(p,9)D
√
Ml

(cid:115)

log

(cid:18) 2D
δ′

(cid:123)(cid:122)
s2

(cid:19)(cid:33)

.

(cid:125)

i

(cid:17)

(cid:17)

(cid:16) 16 ˜∆0
∆l

) − Cσ(i)(K∗

ensures s1 ≤ ∆l/16. Similarly, setting Ml =

Note that (Cσ(i)(X (l−1)
event El−1 where claim (a) of (23) holds. Hence, from Table 2, setting Rl = c(p,2) log

σ(i))) ≤ ∆l−1/8 ≤ ∆0 ≤ ˜∆0 as a result of conditioning on the
≥

(cid:16) c(p,3)N
∆2
l
log (cid:0) 2D
δ′
σ(i)) ≤
σ(i) with probability at least (1 − δl/(4N )), based on Lemma 9. Let us

4
ηµ log
ensures s2 ≤ ∆l/16. Finally, setting δ′ = δl/(4N Rl) provides us with Cσ(i)(X (l)
∆l/8, and hence X (l)
i ∈ G0
denote this event by ˜E(l,1).
(cid:12)
(cid:12)
i ) − Cσ(i)(X (l)
ˆCσ(i)(X (l)
(cid:12)
(cid:12)
(cid:12) ≤ ∆l/8 with probability at least 1 − δl/(4N )
i )
Now, we show that
(cid:12)
j ) and E[Cσ(i)(X (l)
conditioned on the event ˜E(l,1). We have ˆCσ(i)(X (l)
i ) = 1
Ml
, Z (i)
Cσ(i)(X (l)
i ∈ G0
j ) ≤ G∞ due to
(18). Using Hoeffding’s inequality, the following holds for all s ≥ 0:

(cid:80)Ml
j=1 Cσ(i)(X (l)
, Z (i)
j , we have Cσ(i)(X (l)

i ). Furthermore, since on event ˜E(l,1), X (l)

(p,9)D2
∆2
l
i )−Cσ(i)(K∗

log (cid:0) 2D
δ′

c(p,4)
∆2
l

(cid:1) ≥

162c2

(cid:1)

i

i

i

, Z (i)

j )] =

P





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
Ml

Ml(cid:88)

j=1

Cσ(i)(X (l)
i

, Z (i)

j ) − Cσ(i)(X (l)
i )



≥ s

 ≤ 2 exp

(cid:18) −2s2Ml
G2
∞

(cid:19)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Setting s = ∆l/8, and requiring the failure probability on the R.H.S to be lesser than δl/(4N ) leads
to the requirement: Ml ≥ 36G2
which is satisfied by our choice of Ml from Table 2.
∆2
l

(cid:16) 8N
δl

log

(cid:17)

∞

20

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

(cid:12)
ˆCσ(i)(X (l)
(cid:12)
Hence,
(cid:12)
this event as ˜E(l,2).

i ) − Cσ(i)(X (l)
i )

(cid:12)
(cid:12)
(cid:12) ≤ ∆l/8 with probability at least 1 − δl/(4N ). Let us denote

Now, to show the two claims in (23), let us define an event ˜El = ˜E(l,1) ∩ ˜E(l,2). Then,
P( ˜El|El−1) = P( ˜E(l,2)| ˜E(l,1), El−1)P( ˜E(l,1)|El−1) ≥ (1 − δl/(4N ))(1 − δl/(4N )) ≥ 1 − δl/(2N ).
(cid:12)
Therefore, on event ˜El, both the guarantees: (a)
(cid:12)
(cid:12) ≤ ∆l/8 and (b)
(cid:12)
(cid:12)
ˆCσ(i)(X (l)
(cid:12)
(cid:12)
(cid:12) ≤ ∆l/8 hold with probability at least 1 − δl/(2N ). Union bounding
(cid:12)
over all the agents, with probability at least 1 − δl/2, both claims in (23) hold for all agents i ∈ [N ]
on the event ˜El after conditioning on the event El−1.

i ) − Cσ(i)(X (l)
i )

(cid:12)
(cid:12)Cσ(i)(X (l)
(cid:12)

i ) − Cσ(i)(K∗

σ(i))

Finally, defining an event El = ˜El ∩ El−1, we have,

P(El) = P( ˜El|El−1)P(El−1) ≥ (1 − δl/2)

1 −



l−1
(cid:88)

j=1

δj
2





 ≥

1 −



 .

l
(cid:88)

j=1

δj
2

Since δl = δ/(2l2), we have (cid:80)l

j=1

B.1. Proof of Lemma 9

δj

2 = (cid:80)l

j=1

δ
4j2 ≤ δ/2. This completes the proof of Theorem 1.

In this section, we prove Lemma 9 which provides the convergence of the localPO subroutine. Fix
a system j ∈ [H] and let Kt denote the controller in the t-th iteration of localPO ∀t = 0, 1, . . . , R.
Note that the localPO sub-routine proceeds as follows: starting with a controller K0 ∈ G0
j , in
every iteration t, Kt is updated as Kt+1 = Kt − ηg(Kt), where g(Kt) = ZO(Kt, M, r) is the
M -minibatched zeroth-order gradient estimate with a smoothing radius r. We prove the statement
via induction. Given the base case K0 ∈ G0
j and δ′ ∈ (0, 1/R), let us assume that in the t-th iteration
the following holds for all τ ∈ {1, 2, . . . , t} with probability at least (1 − δ′t) :

Kτ ∈ G0
j

Cj(Kτ ) − Cj(K∗

j ) ≤

(cid:16)

1 −

(cid:17)

ηµ
4

(Cj(Kτ −1) − Cj(K∗

j )) +

(cid:32)

ηµ
4

c(p,9)D
√
M

(cid:115)

log

(cid:19)(cid:33)

.

(cid:18) 2D
δ′

(25)

Let us denote the event where both the claims in (25) hold by Et. Now, conditioned on the event Et,
in the following, we will show that with probability at least 1 − δ′, Kt+1 ∈ G0

j and

Cj(Kt+1) − Cj(K∗

j ) ≤

(cid:16)

1 −

(cid:17)

ηµ
4

(Cj(Kt) − Cj(K∗

j )) +

(cid:32)

ηµ
4

c(p,9)D
√
M

(cid:115)

log

(cid:19)(cid:33)

.

(cid:18) 2D
δ′

In what follows, we omit the subscript notation j for convenience. Conditioned on the event Et,

we begin by analyzing the one-step progress in the (t + 1)-th iteration of localPO.

From Lemma 7, as the event Et ensures that Kt ∈ G0, we have ∥g(Kt) − ∇Cr(Kt)∥ ≤
(cid:1) with probability at least (1 − δ′). Let us denote this event by ˜Et. Define et :=

(cid:113)

log (cid:0) 2D
δ′

c(p,8)D
√
M
r

21

KANAKERI BAJAJ VERMA GUPTA MITRA

g(Kt) − ∇C(Kt). Conditioned on the event ˜Et ∩ Et, we have

∥et∥ = ∥g(Kt) − ∇Cr(Kt) + ∇Cr(Kt) − ∇C(Kt)∥

(a)
≤ ∥g(Kt) − ∇Cr(Kt)∥ + ∥∇Cr(Kt) − ∇C(Kt)∥

(b)
≤

c(p,8)D
√
M
r

(cid:115)

log

(cid:19)

(cid:18) 2D
δ′

+ ϕr,

(26)

where (a) follows from the triangle inequality and the (b) due to the event ˜Et ∩ Et and (17). Let us
(cid:18) c(p,8)D

(cid:19)1/2

(cid:113)

(cid:113)

, r = min{ρ, ˜r}, and define

log (cid:0) 2D
δ′

(cid:1)

log (cid:0) 2D
δ′

define cp =

c(p,8)D
√
M
r
(cid:113)
log (cid:0) 2D
Z :=
δ′
following sequence of bounds on cp :

c(p,8)D
√
M

ϕ
(cid:1). Based on the choice M ≥

(cid:1) + ϕr. Set ˜r =

√

M

c(p,4)
∆2
0

log(2D/δ′), we have Z ≤ 1, yielding the

cp =

Z
r

+ ϕr
(cid:26) Z
˜r
(cid:40)

≤ max

+ ϕ˜r,

(a)
≤ max

2(cid:112)Zϕ,

(cid:40)

2(cid:112)Zϕ,

= max

(cid:27)

+ ϕρ

(cid:41)

+ ϕ˜r

(cid:41)

+ (cid:112)Zϕ

Z
ρ
√

Z
ρ
√

Z
ρ

√

≤ 2

Z max

(cid:26)

(cid:112)ϕ,

(b)
≤ 2 max

(cid:26)

(cid:112)ϕ,

(cid:27)

1
ρ

(cid:27)

1
ρ

,

(27)

(28)

where (a) and (b) follow from Z ≤ 1. Based on the above, we have

η∥g(Kt)∥ ≤ η(∥∇C(Kt)∥) + ∥et∥

(a)
≤ η(λ + cp)
(cid:18)
(b)
≤ η

λ + 2 max

(cid:26)

(cid:112)ϕ,

(cid:27)(cid:19)

,

1
ρ

where (a) follows from Lemma 4 and (b) follows from (28). Setting the RHS ≤ ρ leads to
(cid:111) which is satisfied by setting η = c(p,1). This ensures that
the requirement η ≤

ρ
(cid:110)√

λ+2 max

ϕ, 1
ρ

22

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

∥Kt+1 − Kt∥ ≤ ρ. Using the local smoothness property (Lemma 5), we then have

C(Kt+1) − C(Kt) ≤ ⟨∇C(Kt), Kt+1 − Kt⟩ +

∥Kt+1 − Kt∥2

= −η⟨∇C(Kt), g(Kt)⟩ +

= −η⟨∇C(Kt), ∇C(Kt) + et⟩ +

∥∇C(Kt) + et∥2

ϕ
2
ϕη2
2

∥g(Kt)∥2
ϕη2
2

(a)
≤ −η∥∇C(Kt)∥2 − η⟨∇C(Kt), et⟩ + ϕη2∥∇C(Kt)∥2 + ϕη2∥et∥2
(b)
≤ −η(1 − ϕη)∥∇C(Kt)∥2 +

∥et∥2 + ϕη2∥et∥2

η
2
(1 − 2ϕη)∥∇C(Kt)∥2 +

η
2
(1 + 2ϕη)∥et∥2

∥∇C(Kt)∥2 +
η
2

= −

(c)
≤ −

η
2
η
4

∥∇C(Kt)∥2 +

3η
4

c2
p.

In the above, we used ∥A + B∥2 ≤ 2∥A∥2 + 2∥B∥2 in (a), and −2⟨A, B⟩ ≤ ∥A∥2 + ∥B∥2 in (b)
where A and B are any matrices in Rm×n. In (c), we used η ≤ 1/(4ϕ) (satisfied by our choice
η = c(p,1)) and ∥et∥ ≤ cp. Denoting the suboptimality gap as St = C(Kt) − C(K∗), and using the
PL condition (15) in the above, we obtain the following with probability at least 1 − δ′:

St+1 ≤

≤

(cid:16)

(cid:16)

1 −

1 −

(cid:17)

(cid:17)

ηµ
4
ηµ
4

St +

St +

3η
4
ηµ
4

c2
p
(cid:18) 3
µ

(cid:19)

.

c2
p

(29)

On event Et, since we have Kt ∈ G0

j , St ≤ 10 ˜∆0. Furthermore, due to (27), we have

3
µ

c2
p ≤

12
µ

(cid:18)

(cid:26)

(cid:112)ϕ,

max

1
ρ

(cid:27)(cid:19)2 c(p,8)D
√
M

(cid:115)

log

(cid:18) 2D
δ′

(cid:19)
.

(cid:16)

(cid:110)√

(cid:111)(cid:17)2

12c(p,8)
µ

max

Defining c(p,9) :=
by M ≥
based on (29), we have St+1 ≤ (cid:0)1 − ηµ
event ˜Et ∩ Et, Kt+1 ∈ G0

c(p,4)
∆2
0

j and

4

ϕ, 1
ρ

and setting M ≥

(p,9)D2
c2
∆2
0

log (cid:0) 2D
(cid:1), which is satisfied
δ′
p ≤ ∆0 ≤ 10 ˜∆0. Hence,
µ c2
4 10 ˜∆0 ≤ 10 ˜∆0. Therefore, conditioned on the

(cid:1) 10 ˜∆0 + ηµ

log(2D/δ′) from the statement of Lemma 9, ensures that 3

St+1 ≤

(cid:16)

1 −

(cid:17)

ηµ
4

St +

ηµ
4

(cid:32)

c(p,9)D
√
M

(cid:115)

log

(cid:19)(cid:33)

.

(cid:18) 2D
δ′

Now, let us define Et+1 := ˜Et ∩ Et. We have P( ˜Et ∩ Et) = P( ˜Et|Et)P(Et) ≥ (1 − δ′)(1 − δ′t) ≥
1 − δ′(t + 1). This completes the induction step. To prove the statement of Lemma 9, since
η = c(p,1) ≤ 8/µ, for any R ≥ 1, we can unroll the recursion on the event ER which occurs with

23

KANAKERI BAJAJ VERMA GUPTA MITRA

probability 1 − δ′R. Doing so, we obtain the following which completes the proof:

SR ≤

(cid:16)

1 −

(cid:16)

≤

1 −

ηµ
4

ηµ
4

(cid:17)R

S0 +

R−1
(cid:88)

(cid:16)

1 −

(cid:17)R

S0 +

k=0
(cid:32)

c(p,9)D
√
M

ηµ
4
(cid:115)

(cid:17)k ηµ
4

(cid:32)

c(p,9)D
√
M

(cid:115)

log

(cid:18) 2D
δ′

(cid:19)(cid:33)

(cid:19)(cid:33)

.

(cid:18) 2D
δ′

(30)

log

24

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

Appendix C. Proof of Theorem 2

We prove Theorem 2 by conditioning on the event where the claims in (23) hold for all agents in
every epoch. In Appendix B, we showed that such an event occurs with probability at least 1 − δ/2
and let us denote it by EThm1. Furthermore, under this event, the claims of Theorem 1 hold as shown
in Appendix B. In particular, the true clusters are always contained in the neighborhood sets and
correct clustering takes place at the latest during the L-th epoch, ensuring that the agents collaborate
solely within their own clusters from the (L + 1)-th epoch onward. With that in mind, we consider
the following approach to prove Theorem 2. First, we take for granted that the last epoch occurs
after the correct clustering takes place, i.e, ¯L > L, and later show that this is indeed true if the total
number of rollouts T ≥ ˜O(1/∆2). Next, we show that for any l > L (note that at least one such
epoch exists in light of ¯L > L,) the policy at the start of the collaborative policy optimization remains
in the corresponding restricted domain with high probability, i.e, ˆK(l−1)
σ(i). Then, we focus
on the last epoch ¯L and provide the convergence guarantee, and finally conclude by analyzing the
number of rollouts needed to ensure ¯L > L.

∈ G0

Conditioned on the event EThm1, we follow an induction based argument to show that ˆK(l−1)
∈
σ(i) for all agents i ∈ [N ] in every epoch l > L. Let us define ˜L as the first epoch where correct
G0
clustering takes place. Due to Theorem 1, since the correct clustering takes place at the latest during
the Lth epoch, ˜L ≤ L, and moreover, since the neighborhood sets are sequentially pruned with
no new agents getting added to the neighborhood sets, we have Mσ(i) = N (l)
for all agents in
i
every epoch l ≥ ˜L. Furthermore, ˜L being the first epoch where correct clustering takes place,
i = Mσ(i) for some agent i ∈ [N ], hence causing reinitializtion as shown in (8). After
N
this reinitialization, the global sequences for all the agents are updated by collaborating within their
respective clusters, and hence the global sequences for two agents within a cluster evolve identically
in light of (6). In other words, for all agents i, j, if Mσ(i) = Mσ(j), then for all l ≥ ˜L, we have the
following on the event EThm1:

̸= N ˜L

˜L−1
i

i

i

i = ˆK(l)
ˆK(l)
i = N (l)
Mσ(i) = N (l)

j

j = Mσ(j)

(31)

i

Taking this into account, we show that ˆK(l−1)
σ(i) for all agents i ∈ [N ] in every epoch l > ˜L
∈ G0
via induction across epochs. For the base case l = ˜L + 1, as a consequence of reinitialization during
the ˜Lth epoch, and since X ( ˜L)
i ∈ G0
σ(i) for all agents i ∈ [N ] as a result of conditioning on the event
EThm1, we have ˆK( ˜L)
σ(i) for all agents i ∈ [N ]. Let us assume that for an epoch l ≥ ˜L + 1,
i
with probability at least (1 − (cid:80)l−1
δj
4 ), we have ˆK(t−1)
∈ G0
σ(i) for all agents i ∈ [N ] and for
j=1
all t ∈ { ˜L + 1, ˜L + 2, . . . , l}. With a slight abuse of notation, let us denote this event by El−1.
Next, conditioned on the event EThm1 ∩ El−1, we show that ˆK(l)
σ(i) for all agents i ∈ [N ] with
probability at least 1 − δl/4.

i ∈ G0

∈ G0

i

In what follows we fix an agent i ∈ [N ] and omit the notation i and σ(i) for convenience. Given
that K(l−1) ∈ G0 on the event EThm1 ∩ El−1, we focus on analyzing the iterates {Y (k)}0≤k<Rl in
the lth epoch. The iterates are updated as follows: Y (k+1) = Y (k) − ηG(Y (k)) with Y (0) = ˆK(l−1),
where we used G(Y (k)) to denote the averaged zeroth-order gradient estimate as shown in (6) in the
kth iteration. Note that in the light of Assumption 1, the averaged gradient estimate is an unbiased

25

KANAKERI BAJAJ VERMA GUPTA MITRA

estimate of ∇Cr(Y (k)) with an effective minibatch size of Ml|M|. Therefore, we follow an approach
similar to the one from the proof of Lemma 9 in Appendix B.1 to analyze the one-step progress and
to show that Y (Rl) = ˆKl ∈ G0. More specifically, we follow an induction based approach across
iterations and establish one-step recursion similar to (25) and finally unroll the recursion to obtain
something similar to (30). However, a key difference arises from the fact that the second term in the
RHS of both (25) and (30) will now enjoy a variance reduction effect due to collaboration in light of
Assumption 1 as shown in Corollary 8.

In particular, following the induction approach from the proof of Lemma 9 in Appendix B.1,
in the kth iteration, we have the following concentration with probability at least (1 − δ′) after
conditioning on the event where the previous iterations satisfy similar guarantees as in (25):

∥G(Y (k)) − ∇Cr(Y (k))∥ ≤

c(p,8)D
r(cid:112)Ml|M|

(cid:115)

log

(cid:18) 2D
δ′

(cid:19)
.

Conditioned on the event where the gradient estimate is concentrated as above, and defining ek :=
(cid:1) + ϕr following the arguments up to
G(Y (k)) − ∇C(Y (k)), we have ∥ek∥ ≤

c(p,8)D
√

(cid:113)

log (cid:0) 2D
δ′

(26). Now, let us define cp =

c(p,8)D
√

Ml|M|

r

r
(cid:113)

Ml|M|
log (cid:0) 2D
δ′

(cid:1) + ϕr. Setting ˜r =

(cid:19)1/2

(cid:18) c(p,8)D
Ml

√

ϕ

(cid:113)

(cid:1)

log (cid:0) 2D
δ′
(cid:113)
log (cid:0) 2D
δ′

c(p,8)D
√
Ml

˜r

r = min{ρ,
which is ensured by our setting for Ml in Table 2.

|M|1/4 }, we obtain the following bound on cp provided Z :=

cp =

Z
r(cid:112)|M|

+ ϕr

(cid:26) Z

˜r|M|1/4
√

+ ϕ

˜r
|M|1/4
√

,

Z
ρ|M|1/2

(cid:27)

+ ϕρ

(cid:41)

˜r
|M|1/4

+ ϕ

≤ max

≤ max

(cid:40)
2

√

Z
|M|1/4
(cid:26)

≤ 2 max

≤ 2

max

,

Zϕ
|M|1/4
(cid:26)

Z
ρ|M|1/4
(cid:27)

(cid:112)ϕ,

1
ρ

(cid:112)ϕ,

(cid:27)

.

1
ρ

and

(cid:1) ≤ 1

(32)

(33)

Based on the above, we choose η = c(p,1) ≤

(cid:111) to ensure that ∥Y (k+1) − Y (k)∥ ≤ ρ.

λ+2 max
Defining Sk = C(Y (k)) − C(K∗) and following the analysis from Appendix B.1 up to (29) and
using the bound on cp from (32), we obtain

ρ
(cid:110)√

ϕ, 1
ρ

Sk+1 ≤

(cid:32)

1 −

(cid:16)

(cid:124)

(cid:17)

ηµ
4
(cid:123)(cid:122)
s1

+

Sk
(cid:125)

ηµ
4
(cid:124)

(cid:115)

log

c(p,9)D
(cid:112)Ml|M|
(cid:123)(cid:122)
s2

(cid:18) 2D
δ′

(cid:19)(cid:33)

,

(cid:125)

with probability at least 1 − δ′. Note that since |M| ≥ 1, the term s2 is not greater than the
4 10 ˜∆0. Meanwhile, the term s1 ≤
corresponding term from (25), and hence s2 ≤ ηµ

4 ∆0 ≤ ηµ

26

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

(cid:1) 10 ˜∆0 as we have conditioned on the event where Y (k) ∈ G0 similar to the proof of
(cid:0)1 − ηµ
4
Lemma 9. This ensures that Y (k+1) ∈ G0. Now, unrolling the recursion, we have with probability at
least 1 − δ′Rl, Y (Rl) = ˆK(l) ∈ G0 and the following:

C( ˆK(l)) − C(K∗) ≤

(cid:16)

1 −

(cid:17)Rl

ηµ
4

(C( ˆK(l−1)) − C(K∗)) +

(cid:32)

c(p,9)D
(cid:112)Ml|M|

(cid:19)(cid:33)

(cid:115)

log

(cid:18) 2D
δ′

.

(34)

Setting δ′ = δl/(4RlN ) and applying an union bound over all agents, we have the above guarantee
for all agents with probability at least 1 − δl/4.

Let us denote this event by ˜El. Defining El = ˜El ∩ El−1, we have the following:

P(El|EThm1) = P( ˜El|El−1, EThm1)P(El−1|EThm1) ≥ (1 − δl/4)

1 −



l−1
(cid:88)

j=1

δj
4





 ≥

1 −



 .

l
(cid:88)

j=1

δj
4

This completes the induction argument. Hence, we have established that ˆK(l−1)
i
(cid:16)
(34) holds for all agents i ∈ [N ] in every epoch l ≥ ˜L with probability at least

∈ G0
1 − (cid:80)l

σ(i) and that
(cid:17)
δj
4

j=1

.

Next, we analyze the final convergence guarantee in the last epoch ¯L. Conditioned on the event
E ¯L−1 ∩ EThm1, we obtain (34) as shown in the following with probability at least 1 − δ ¯L/4 for all
agents i ∈ [N ]:

Cσ(i)( ˆK( ¯L)

i

) − Cσ(i)(K∗

σ(i)) ≤

1 −

(cid:17)R ¯L

ηµ
4

(cid:16)

(cid:124)

) − Cσ(i)(K∗

σ(i)))
(cid:125)

(Cσ(i)( ˆK( ¯L−1)
i
(cid:123)(cid:122)
s1
(cid:18) 8DN R ¯L
δ ¯L

log

(cid:115)

(cid:123)(cid:122)
s2

(cid:19)(cid:33)

.

(cid:125)

+

(cid:32)

(cid:124)

c(p,9)D
(cid:112)M ¯L|M|

In the above, the settings of R ¯L and M ¯L from Table 2 ensures the following:
4
ηµ log

, note that the term

(cid:18) c(p,4)N 10 ˜∆0
∆2
¯L

(cid:19)

from R ¯L ≥

(cid:18)

s1 ≤ exp

−

(cid:19)

ηµR ¯L
4

(cid:18)

S0 ≤ exp

−

(cid:19)

ηµR ¯L
4

10 ˜∆0 ≤

∆2
¯L
c(p,4)N

.

Using M ¯L =

M ¯L =

c(p,4)
∆2
¯L

log

c(p,4)
log
∆2
¯L
(cid:16) 8DN R ¯L
δ ¯L

(cid:17)

(cid:16) 8DN R ¯L
δ ¯L

(cid:17)

≥ log

in the above, we have s1 ≤
(cid:16) 8DN R ¯L
δ ¯L

, we have

(cid:17)

log

(cid:19)

(cid:18) 8DN R ¯L
δ ¯L
M ¯LN

. Furthermore, since

(cid:114)

(cid:16) 8DN R ¯L
log
δ ¯L
(cid:112)M ¯LN

(cid:17)

≤

(cid:114)

(cid:16) 8DN R ¯L
log
δ ¯L
(cid:112)M ¯LN

(cid:17)

≤

(cid:114)

(cid:16) 8DN R ¯L
log
δ ¯L
(cid:112)M ¯L|M|

(cid:17)

.

s1 ≤

27

KANAKERI BAJAJ VERMA GUPTA MITRA

Together with the term s2 we obtain the following with probability at least 1 − δ ¯L/4 for all agents
i ∈ [N ]:

Cσ(i)( ˆK( ¯L)

i

) − Cσ(i)(K∗

σ(i)) ≤

2 max{1, c(p,9)D}

(cid:113)

M ¯L|Mσ(i)|

(cid:115)

log

(cid:18) 8DN R ¯L
δ

(cid:19)
.

(35)

The above holds on the event E ¯L conditioned on the event EThm1. We have P(E ¯L|EThm1) ≥

(cid:16)

1 − (cid:80) ¯L

j=1

δj
4

(cid:17)

(cid:16)

=

1 − (cid:80) ¯L

j=1

δ
8j2

(cid:17)

≥ 1 − δ/4. Therefore,

P(EE ¯L ∩ EThm1) = P(EE ¯L|EThm1)P(EThm1) ≥ (1 − δ/4)(1 − δ/2) ≥ 1 − (δ/4 + δ/2) ≥ 1 − δ.

Note that the guarantee in (35) provides a rate ˜O

. It remains to show that
M ¯L = ˜Ω(T ). Since ∆l = ∆0/4l and Rl ≥ 1 for all l ∈ {1, 2, . . . , ¯L}, consider the following as
Ml ≤ T

M ¯L|Mσ(i)|

1/

(cid:16)

(cid:113)

(cid:17)

Ml ≤ T =⇒ 4l ≤

(cid:1)

T ∆2
0
c(p,10) log (cid:0) 8DN
δ
T ∆2
0
c(p,10) log (cid:0) 8DN

(cid:32)

δ

(cid:33)

(cid:1)

=⇒ l ≤ log

= log(c(p,11)T ),

(36)

where we defined c(p,11) :=
follows:

(cid:18)

∆2
0
c(p,10) log( 8DN
δ )

(cid:19)

. Now, we use the upper bound on l to bound Rl as

Rl =

4
ηµ
(cid:32)

(a)
≤ l

(cid:32)

(cid:32)

log

(cid:32)

4
ηµ

log

(cid:33)

c(p,10)N ˜∆2
0
∆2
0
c(p,10)N ˜∆2
0
∆2
0

(cid:32)

(cid:33)

+ l log(4)

(cid:33)

(cid:33)(cid:33)

+ log(4)

(b)
≤ c(p,12) log(c(p,11)T ),

28

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS

where (a) follows as l ≥ 1, and (b) follows from (36) with c(p,12) := 4
ηµ
Therefore, the overall sample complexity has the following bound:

(cid:18)

log

(cid:18) c(p,10)N ˜∆2
∆2
0

0

(cid:19)

+ log(4)

(cid:19)
.

¯L
(cid:88)

T =

(2MlRl + Ml)

l=1
¯L
(cid:88)

(3MlRl)

(a)
≤

(b)
≤

l=1
¯L
(cid:88)

l=1

3c(p,12) log(c(p,11)T )

= 4c(p,12) log(c(p,11)T )

(cid:33)

(cid:1)

4l

(cid:32)

δ

c(p,10) log (cid:0) 8DN T
∆2
0
c(p,10) log (cid:0) 8DN T
∆2
0

(cid:33)

(cid:1)

δ

(cid:32)

¯L
4

=⇒

T

4c(p,12) log(c(p,11)T )

(cid:18) c(p,10) log( 8DN T
∆2
0

δ

)

(cid:19) ≤ 4

¯L.






(37)

In the above, (a) follows as Rl ≥ 1 and (b) as we used Rl ≤ T in Ml. Using the lower bound

on 4 ¯L as obtained above in M ¯L, we obtain M ¯L ≥
(35), we have the following with probability 1 − δ:

T log

(cid:16) 8DN R ¯L
δ

(cid:17)

4c(p,12) log(c(p,11)T ) log( 8DN T

δ

. Using this bound in

)

Cσ(i)( ˆK( ¯L)

i

) − Cσ(i)(K∗

σ(i)) ≤

=

4D max{1, c(p,9)}

(cid:113)

c(p,12) log(c(p,11)T ) log (cid:0) 8DN T
(cid:113)

δ

T |Mσ(i)|

Dc(p,13)
(cid:113)

(cid:113)

log (cid:0) 8DN T

δ

(cid:1)

,

T |Mσ(i)|

(cid:1)

(38)

where we defined c(p,13) := 4 max{1, c(p,9)}

(cid:113)

c(p,12) log(c(p,11)T ).

Finally, it remains to show that ¯L > L when T ≥ ˜O(1/∆2). To ensure, ¯L > L, consider the
number of rollouts required up to (L + 1)th epoch. From (37) with the summation from 1 to L + 1
we have:

L+1
(cid:88)

(2MlRl + Ml) ≤ 4c(p,12) log(c(p,11)T )

l=1

(cid:32)

c(p,10) log (cid:0) 8DN T
∆2
0

δ

(cid:33)

(cid:1)

4L+1.

In the above, setting T ≥ RHS to ensure ¯L > L, we have

T ≥ 4c(p,12) log(c(p,11)T )

(cid:32)

(a)
≥ 4c(p,12) log(c(p,11)T )

δ

c(p,10) log (cid:0) 8DN T
∆2
0
c(p,10) log (cid:0) 8DN T
∆2
0

δ

(cid:32)

(cid:1)

(cid:33)

4L+1

(cid:1)

(cid:33) (cid:18) ∆0
∆

(cid:19)2

.

29

KANAKERI BAJAJ VERMA GUPTA MITRA

In the above, since L = min{l ∈ 1, 2, . . . : ∆l ≤ ∆/2}, (a) follows from the fact that ∆L+1 =
∆0/(2L+1) ≤ ∆/2. Hence, when T ≥ ˜O(1/∆2), we have ¯L > L. This completes the proof of
Theorem 2.

Appendix D. Proof of Corollary 3

In this section, we analyze the total communication complexity of the PCPO algorithm. In every
epoch, each agent communicates with the server once in every iteration of the collaborative policy
optimization subroutine, and once to send the local policy to update the neighborhood sets. Hence,
the overall communication complexity is (cid:80) ¯L
l=1(Rl + 1) ≤ (R ¯L + 1) ¯L

since R ¯L = c(p,2) log

≥ c(p,2) log

(cid:19)

(cid:18) 2 ¯Lc(p,3)N
∆2
0

l=1(Rl + 1). Note that (cid:80) ¯L
(cid:17)
(cid:16) 2lc(p,3)N
∆2
0

= Rl.

From (37), ¯L is logarithmic in T . Furthermore, R ¯L = c(p,2) log

(cid:18) 2 ¯Lc(p,3)N
∆2
0

(cid:19)

is logarithmic in

the number of agents N and T . Finally, since T ≥ ˜O(1/∆2), the overall communication complexity
is logarithmic in T , N and 1/∆.

30

