{
  "id": "2312.00752",
  "content": "4\n2\n0\n2\n\ny\na\nM\n1\n3\n\n]\n\nG\nL\n.\ns\nc\n[\n\n2\nv\n2\n5\n7\n0\n0\n.\n2\n1\n3\n2\n:\nv\ni\nX\nr\na\n\nMamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nAlbert Gu\u22171 and Tri Dao\u22172\n\n1Machine Learning Department, Carnegie Mellon University\n2Department of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\n\nAbstract\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\nTransformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses\ntheir weakness with discrete modalities, allowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though this change prevents the use of efficient\nconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a\nsimplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves\non real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art\nperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model\noutperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream\nevaluation.\n\n1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged\nas an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on\narbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever,\nVinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are\npredominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention\nlayer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely\nwithin a context window, allowing it to model complex data. However, this property brings fundamental drawbacks:\nan inability to model anything outside of a finite window, and quadratic scaling with respect to the window length.\nAn enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay,\nDehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these\nvariants have been shown to be empirically effective at scale across domains.\n\nRecently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have\nemerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of\nrecurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space\nmodels (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with\nlinear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range\ndependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range\n\n\u2217Alphabetical by first name.\n\n1\n\n \n \n \n \n \n \n\fArena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu,\nand Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been\nsuccessful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022;\nSaon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such\nas text.\n\nWe propose a new class of selective state space models, that improves on prior work on several axes to achieve the\nmodeling power of Transformers while scaling linearly in sequence length.\n\nFirst, we identify a key limitation of prior models: the ability to efficiently select data in an\nSelection Mechanism.\ninput-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic\ntasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM\nparameters based on the input. This allows the model to filter out irrelevant information and remember relevant information\nindefinitely.\n\nHardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in\nfact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this\nwith a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not\nmaterialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The\nresulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to\npseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3\u00d7 faster on A100 GPUs).\n\nArchitecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures\n(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous\narchitecture design (Mamba) incorporating selective state spaces.\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them\nsuitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong\nperformance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory\nscales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only\nconstant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency\ntogether yield performance improvements on real data up to sequence length 1M.\n\nWe empirically validate Mamba\u2019s potential as a general sequence FM backbone, in both pretraining quality and domain-\nspecific task performance, on several types of modalities and settings:\n\n\u2022 Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to\nlarge language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long (>1M tokens).\n\u2022 Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers\non modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing\nFID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer\ncontext up to million-length sequences.\n\n\u2022 Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance,\nboth in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba\nexceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based\non LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7 generation throughput compared to Transformers\nof similar size, and Mamba-3B\u2019s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common\nsense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\n\nModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\n\n2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related\nto RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n2\n\n\fFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. \ud835\udc37 = 5) of an input \ud835\udc65 to output \ud835\udc66 through a higher\ndimensional latent state \u210e (e.g. \ud835\udc41 = 4). Prior SSMs avoid materializing this large effective state (\ud835\udc37\ud835\udc41 , times batch size \ud835\udc35 and sequence\nlength \ud835\udc3f) through clever alternate computation paths requiring time-invariance: the (\u0394, \ud835\udc68, \ud835\udc69, \ud835\udc6a) parameters are constant across time. Our\nselection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize\nthe expanded states in more efficient levels of the GPU memory hierarchy.\n\n1-dimensional function or sequence \ud835\udc65 (\ud835\udc61) \u2208 R \u21a6\u2192 \ud835\udc66 (\ud835\udc61) \u2208 R through an implicit latent state \u210e(\ud835\udc61) \u2208 R\ud835\udc41 .\n\nConcretely, S4 models are defined with four parameters (\u0394, \ud835\udc68, \ud835\udc69, \ud835\udc6a), which define a sequence-to-sequence transformation\nin two stages.\n\n\u210e\u2032 (\ud835\udc61) = \ud835\udc68\u210e(\ud835\udc61) + \ud835\udc69\ud835\udc65 (\ud835\udc61)\n\ud835\udc66 (\ud835\udc61) = \ud835\udc6a\u210e(\ud835\udc61)\n\n(1a)\n\n(1b)\n\n\u210e\ud835\udc61 = \ud835\udc68\u210e\ud835\udc61 \u22121 + \ud835\udc69\ud835\udc65\ud835\udc61\n\ud835\udc66\ud835\udc61 = \ud835\udc6a\u210e\ud835\udc61\n\n(2a)\n\n(2b)\n\n\ud835\udc58\n\ud835\udc72 = (\ud835\udc6a\ud835\udc69, \ud835\udc6a\ud835\udc68\ud835\udc69, . . . , \ud835\udc6a\ud835\udc68\n\ud835\udc66 = \ud835\udc65 \u2217 \ud835\udc72\n\n\ud835\udc69, . . . )\n\n(3a)\n\n(3b)\n\nDiscretization. The first stage transforms the \u201ccontinuous parameters\u201d (\u0394, \ud835\udc68, \ud835\udc69) to \u201cdiscrete parameters\u201d (\ud835\udc68, \ud835\udc69) through\nfixed formulas \ud835\udc68 = \ud835\udc53\ud835\udc34 (\u0394, \ud835\udc68) and \ud835\udc69 = \ud835\udc53\ud835\udc35 (\u0394, \ud835\udc68, \ud835\udc69), where the pair (\ud835\udc53\ud835\udc34, \ud835\udc53\ud835\udc35) is called a discretization rule. Various rules can\nbe used such as the zero-order hold (ZOH) defined in equation (4).\n\n\ud835\udc68 = exp(\u0394\ud835\udc68)\n\n\ud835\udc69 = (\u0394\ud835\udc68) \u22121(exp(\u0394\ud835\udc68) \u2212 \ud835\udc70 ) \u00b7 \u0394\ud835\udc69\n\n(4)\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such\nas resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized\n(Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms of RNNs (Gu,\nGulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from a mechanical point\nof view discretization can simply be viewed as the first step of the computation graph in the forward pass of an SSM.\nAlternate flavors of SSMs can bypass the discretization step and parameterize (\ud835\udc68, \ud835\udc69) directly instead (Zhang et al. 2023),\nwhich may be easier to reason about.\n\nComputation. After the parameters have been transformed from (\u0394, \ud835\udc68, \ud835\udc69, \ud835\udc6a) \u21a6\u2192 (\ud835\udc68, \ud835\udc69, \ud835\udc6a), the model can be computed\nin two ways, either as a linear recurrence (2) or a global convolution (3).\nCommonly, the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequence\nis seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where the inputs are\nseen one timestep at a time).\n\nLinear Time Invariance (LTI). An important property of equations (1) to (3) is that the model\u2019s dynamics are constant\nthrough time. In other words (\u0394, \ud835\udc68, \ud835\udc69, \ud835\udc6a), and consequently (\ud835\udc68, \ud835\udc69) as well, are fixed for all time-steps. This property is\n\n3\n\nProjectDiscretize\ud835\udc65!\u210e!\"#\u210e!\ud835\udc66!\ud835\udc34\ud835\udc36!\ud835\udc35!Selection MechanismGPU SRAMGPU HBM\u2206!Selective State Space ModelwithHardware-aware State Expansion\fcalled linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think of\nLTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these\nclasses of models.\n\nThus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints,\ndiscussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling\ncertain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiency\nbottlenecks.\n\nFinally, we note that structured SSMs are so named because computing them efficiently\nStructure and Dimensions.\nalso requires imposing structure on the \ud835\udc68 matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022;\nGupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.\n\nIn this case, the \ud835\udc68 \u2208 R\ud835\udc41 \u00d7\ud835\udc41 , \ud835\udc69 \u2208 R\ud835\udc41 \u00d71, \ud835\udc6a \u2208 R1\u00d7\ud835\udc41 matrices can all be represented by \ud835\udc41 numbers. To operate over an input\nsequence \ud835\udc65 of batch size \ud835\udc35 and length \ud835\udc3f with \ud835\udc37 channels, the SSM is applied independently to each channel. Note that in\nthis case, the total hidden state has dimension \ud835\udc37\ud835\udc41 per input, and computing it over the sequence length requires \ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41 )\ntime and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.\n\nGeneral State Space Models. We note that the term state space model has a very broad meaning which simply represents\nthe notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in different\ndisciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal\nmodeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)),\nhidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes\nconvolutional) models at large (deep learning).\n\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4 models (Gu,\nGoel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington,\nand Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such\nmodels, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto\net al. 2023; Poli et al. 2023), and clarify nuances when necessary.\n\nSSMs are standalone sequence transformations that can be incorporated into end-to-end neural\nSSM Architectures.\nnetwork architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear\nconvolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our\nprimary baselines.\n\n\u2022 Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be\n\nviewed as a degenerate linear SSM.\n\n\u2022 H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM\nsandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a\nshift-SSM, before the main SSM layer.\n\n\u2022 Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global\n\nconvolution (Romero et al. 2021).\n\n\u2022 RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative\n\nparallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.\n\n\u2022 RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approxi-\nmation, the attention-free Transformer (S. Zhai et al. 2021). Its main \u201cWKV\u201d mechanism involves LTI recurrences and\ncan be viewed as the ratio of two SSMs.\n\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight\nin particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which\nwe view as the most closely related methods to our core selective SSM.\n\n4\n\n\f3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate\nthis mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting\na technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits\nthe memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or\neven MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5).\n\n3.1 Motivation: Selection as a Means of Compression\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can\nview the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and\ninefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference\nrequires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and\nquadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state,\nimplying constant-time inference and linear-time training. However, their effectiveness is limited by how well this state\nhas compressed the context.\n\nTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\n\n\u2022 The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the\nposition of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens\n(colored) and filter out the irrelevant ones (white).\n\n\u2022 The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning\nabilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in\nthe appropriate context (black).\n\nThese tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the (\ud835\udc68, \ud835\udc69)\ntransitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed\nalong the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can\nsolve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty\nwith the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between\ninputs-to-outputs is varying and cannot be modeled by static convolution kernels.\n\nIn summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress\ntheir state: efficient models must have a small state, while effective models must have a state that contains all necessary\ninformation from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity:\nor the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism\ncontrols how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion).\n\n3.2 Improving SSMs with Selection\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along\nthe sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be input-dependent.\n\nAlgorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several\nparameters \u0394, \ud835\udc69, \ud835\udc6a functions of the input, along with the associated changes to tensor shapes throughout. In particular, we\nhighlight that these parameters now have a length dimension \ud835\udc3f, meaning that the model has changed from time-invariant\nto time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3)\nwith implications for its efficiency, discussed next.\n\nWe specifically choose \ud835\udc60\ud835\udc35 (\ud835\udc65) = Linear\ud835\udc41 (\ud835\udc65), \ud835\udc60\ud835\udc36 (\ud835\udc65) = Linear\ud835\udc41 (\ud835\udc65), \ud835\udc60\u0394 (\ud835\udc65) = Broadcast\ud835\udc37 (Linear1(\ud835\udc65)), and \ud835\udf0f\u0394 = softplus,\nwhere Linear\ud835\udc51 is a parameterized projection to dimension \ud835\udc51. The choice of \ud835\udc60\u0394 and \ud835\udf0f\u0394 is due to a connection to RNN gating\nmechanisms explained in Section 3.5.\n\n5\n\n\fFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily\nsolved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random\nspacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content.\n(Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key\nability for LLMs.\n\nAlgorithm 1 SSM (S4)\nInput: \ud835\udc65 : (B, L, D)\nOutput: \ud835\udc66 : (B, L, D)\n1: \ud835\udc68 : (D, N) \u2190 Parameter\n\nAlgorithm 2 SSM + Selection (S6)\nInput: \ud835\udc65 : (B, L, D)\nOutput: \ud835\udc66 : (B, L, D)\n1: \ud835\udc68 : (D, N) \u2190 Parameter\n\n\u22b2 Represents structured \ud835\udc41 \u00d7 \ud835\udc41 matrix\n\n\u22b2 Represents structured \ud835\udc41 \u00d7 \ud835\udc41 matrix\n\n2: \ud835\udc69 : (D, N) \u2190 Parameter\n3: \ud835\udc6a : (D, N) \u2190 Parameter\n4: \u0394 : (D) \u2190 \ud835\udf0f\u0394 (Parameter)\n5: \ud835\udc68, \ud835\udc69 : (D, N) \u2190 discretize(\u0394, \ud835\udc68, \ud835\udc69)\n6: \ud835\udc66 \u2190 SSM(\ud835\udc68, \ud835\udc69, \ud835\udc6a)(\ud835\udc65)\n\n\u22b2 Time-invariant: recurrence or convolution\n\n2: \ud835\udc69 : (B, L, N) \u2190 \ud835\udc60\ud835\udc35 (\ud835\udc65)\n3: \ud835\udc6a : (B, L, N) \u2190 \ud835\udc60\ud835\udc36 (\ud835\udc65)\n4: \u0394 : (B, L, D) \u2190 \ud835\udf0f\u0394 (Parameter+\ud835\udc60\u0394 (\ud835\udc65))\n5: \ud835\udc68, \ud835\udc69 : (B, L, D, N) \u2190 discretize(\u0394, \ud835\udc68, \ud835\udc69)\n6: \ud835\udc66 \u2190 SSM(\ud835\udc68, \ud835\udc69, \ud835\udc6a)(\ud835\udc65)\n\n7: return \ud835\udc66\n\n7: return \ud835\udc66\n\n\u22b2 Time-varying: recurrence (scan) only\n\n3.3 Efficient Implementation of Selective SSMs\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau,\nCho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on\nmodern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate\nspecial cases of selection, such as letting \u0394 vary over time in recurrent SSMs (Gu, Dao, et al. 2020). However, as previously\nmentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives\nused LTI (non-selective) models, most commonly in the form of global convolutions.\n\n3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods.\n\n\u2022 At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in\nSection 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize\nhidden state dimension without paying speed and memory costs.\n\n\u2022 Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding\nthe former (2) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and\nmaterializing the latent state \u210e with shape (B, L, D, N), which is much larger (by a factor of \ud835\udc41 , the SSM state dimension)\nthan the input \ud835\udc65 and output \ud835\udc66 of shape (B, L, D). Thus the more efficient convolution mode was introduced which could\nbypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D).\n\n\u2022 Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by\n\na factor of \ud835\udc41 (\u2248 10 \u2212 100), much larger than traditional RNNs, without efficiency penalties.\n\n6\n\nInputOutput?OutputCopyingSelective CopyingInputInduction HeadsSolutionPerfectly solved by LTI (e.g.convolutional) models that do not need to look at the actual inputs\f3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to\nrevisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and\nrecomputation. We make two main observations:\n\n\u2022 The naive recurrent computation uses \ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41 ) FLOPs while the convolutional computation uses \ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc37 log(\ud835\udc3f)) FLOPs,\nand the former has a lower constant factor. Thus for long sequences and not-too-large state dimension \ud835\udc41 , the recurrent\nmode can actually use fewer FLOPs.\n\n\u2022 The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like\n\nthe convolutional mode, we can attempt to not actually materialize the full state \u210e.\n\nThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state \u210e only in more efficient\nlevels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory\nbandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our\nscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared\nto a standard implementation.\n\nConcretely, instead of preparing the scan input (\ud835\udc68, \ud835\udc69) of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load\nthe SSM parameters (\u0394, \ud835\udc68, \ud835\udc69, \ud835\udc6a) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM,\nand then write the final outputs of size (B, L, D) back to HBM.\n\nTo avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient\nparallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023).\n\nFinally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply\nthe classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but\nrecomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan\nlayer has the same memory requirements as an optimized transformer implementation with FlashAttention.\n\nDetails of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated\nin Figure 1.\n\n3.4 A Simplified SSM Architecture\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into\nneural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are\ngenerally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We\nsimplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is\ninspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention.\n\nThis architecture involves expanding the model dimension \ud835\udc37 by a controllable expansion factor \ud835\udc38. For each block, most\nof the parameters (3\ud835\udc38\ud835\udc37 2) are in the linear projections (2\ud835\udc38\ud835\udc37 2 for input projections, \ud835\udc38\ud835\udc37 2 for output projection) while\nthe inner SSM contributes less. The number of SSM parameters (projections for \u0394, \ud835\udc69, \ud835\udc6a, and the matrix \ud835\udc68) are much\nsmaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form\nthe Mamba architecture. We always fix to \ud835\udc38 = 2 in our experiments and use two stacks of the block to match the 12\ud835\udc37 2\nparameters of a Transformer\u2019s interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation\nfunction (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP\nbecomes the popular \u201cSwiGLU\u201d variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023).\nFinally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)),\nmotivated by RetNet\u2019s usage of a normalization layer in a similar location (Y. Sun et al. 2023).\n\n3.5 Properties of Selection Mechanisms\nThe selection mechanism is a broader concept that can be applied in different ways, such as to more traditional RNNs or\nCNNs, to different parameters (e.g. \ud835\udc68 in Algorithm 2), or using different transformations \ud835\udc60 (\ud835\udc65).\n\n7\n\n\fFigure 3: (Architecture.) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, with\nthe ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block\nhomogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to\nthe MLP block, Mamba adds an SSM to the main branch. For \ud835\udf0e we use the SiLU / Swish activation (Hendrycks and Gimpel 2016;\nRamachandran, Zoph, and Quoc V Le 2017).\n\n3.5.1 Connection to Gating Mechanisms\n\nWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection\nmechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systems\nis well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement of\nGu, Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof in\nAppendix C). More broadly, \u0394 in SSMs can be seen to play a generalized role of the RNN gating mechanism. In line with\nprior work, we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms.\nTheorem 1. When \ud835\udc41 = 1, \ud835\udc68 = \u22121, \ud835\udc69 = 1, \ud835\udc60\u0394 = Linear(\ud835\udc65), and \ud835\udf0f\u0394 = softplus, then the selective SSM recurrence (Algorithm 2)\ntakes the form\n\n\ud835\udc54\ud835\udc61 = \ud835\udf0e (Linear(\ud835\udc65\ud835\udc61 ))\n\u210e\ud835\udc61 = (1 \u2212 \ud835\udc54\ud835\udc61 )\u210e\ud835\udc61 \u22121 + \ud835\udc54\ud835\udc61\ud835\udc65\ud835\udc61 .\n\n(5)\n\nAs mentioned in Section 3.2, our specific choices of \ud835\udc60\u0394, \ud835\udf0f\u0394 is from this connection. In particular, note that if a given input \ud835\udc65\ud835\udc61\nshould be completely ignored (as necessary in the synthetic tasks), all \ud835\udc37 channels should ignore it, and so we project the\ninput down to 1 dimension before repeating/broadcasting with \u0394.\n\n3.5.2 Interpretation of Selection Mechanisms\n\nWe elaborate on three particular mechanistic effects of selection.\n\nSelectivity allows filtering out irrelevant noise tokens that may occur between inputs of interest.\nVariable Spacing.\nThis is exemplified by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly for\ndiscrete data \u2013 for example the presence of language fillers such as \u201cum\u201d. This property arises because the model can\nmechanistically filter out any particular input \ud835\udc65\ud835\udc61 , for example in the gated RNN case (Theorem 1) when \ud835\udc54\ud835\udc61 \u2192 0.\n\nIt has been empirically observed that many sequence models do not improve with longer context (F.\nFiltering Context.\nShi et al. 2023), despite the principle that more context should lead to strictly better performance. An explanation is\nthat many sequence models cannot effectively ignore irrelevant context when necessary; an intuitive example are global\nconvolutions (and general LTI models). On the other hand, selective models can simply reset their state at any time\nto remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g.\nSection 4.3.2).\n\n8\n\nH3Gated MLPMambaLinear projectionSequence transformationNonlinearity (activation or multiplication)XXX!XConvSSMX!!ConvSSM\u2a02\fIn settings where multiple independent sequences are stitched together, Transformers can keep\nBoundary Resetting.\nthem separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences.\nSelective SSMs can also reset their state at boundaries (e.g. \u0394\ud835\udc61 \u2192 \u221e, or Theorem 1 when \ud835\udc54\ud835\udc61 \u2192 1). These settings may\noccur artificially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in\nreinforcement learning (Lu et al. 2023)).\n\nAdditionally, we elaborate on effects of each selective parameter.\n\nInterpretation of \u0394.\nIn general, \u0394 controls the balance between how much to focus or ignore the current input \ud835\udc65\ud835\udc61 . It\ngeneralizes RNN gates (e.g. \ud835\udc54\ud835\udc61 in Theorem 1): mechanically, a large \u0394 resets the state \u210e and focuses on the current input \ud835\udc65,\nwhile a small \u0394 persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system\ndiscretized by a timestep \u0394, and in this context the intuition is that large \u0394 \u2192 \u221e represents the system focusing on the\ncurrent input for longer (thus \u201cselecting\u201d it and forgetting its current state) while a small \u0394 \u2192 0 represents a transient\ninput that is ignored.\n\nInterpretation of \ud835\udc68. We remark that while the \ud835\udc68 parameter could also be selective, it ultimately affects the model\nonly through its interaction with \u0394 via \ud835\udc68 = exp(\u0394\ud835\udc68) (the discretization (4)). Thus selectivity in \u0394 is enough to ensure\nselectivity in (\ud835\udc68, \ud835\udc69), and is the main source of improvement. We hypothesize that making \ud835\udc68 selective in addition to (or\ninstead of) \u0394 would have similar performance, and leave it out for simplicity.\n\nInterpretation of \ud835\udc69 and \ud835\udc6a. As discussed in Section 3.1, the most important property of selectivity is filtering out\nirrelevant information so that a sequence model\u2019s context can be compressed into an efficient state. In an SSM, modifying\n\ud835\udc69 and \ud835\udc6a to be selective allows finer-grained control over whether to let an input \ud835\udc65\ud835\udc61 into the state \u210e\ud835\udc61 , or the state into the\noutput \ud835\udc66\ud835\udc61 . These can be interpreted as allowing the model to modulate the recurrent dynamics based on content (input)\nand context (hidden states) respectively.\n\n3.6 Additional Model Details\nReal vs. Complex. Most prior SSMs use complex numbers in their state \u210e, which is necessary for strong performance\non many tasks in perceptual modalities (Gu, Goel, and R\u00e9 2022). However, it has been empirically observed that completely\nreal-valued SSMs seem to work fine, and possibly even better, in some settings (Ma et al. 2023). We use real values as\nthe default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoff is related to the\ncontinuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g. audio,\nvideo) but not discrete (e.g. text, DNA).\n\nInitialization. Most prior SSMs also suggest special initializations, particularly in the complex-valued case, which can\nhelp in several settings such as low-data regimes. Our default initialization for the complex case is S4D-Lin and for the real\ncase is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu, Dao, et al. 2020). These define the \ud835\udc5b-th\nelement of \ud835\udc68 as \u22121/2 + \ud835\udc5b\ud835\udc56 and \u2212(\ud835\udc5b + 1) respectively. However, we expect many initializations to work fine, particularly in\nthe large-data and real-valued SSM regimes; some ablations are considered in Section 4.6.\n\nParameterization of \u0394. We defined the selective adjustment to \u0394 as \ud835\udc60\u0394 (\ud835\udc65) = Broadcast\ud835\udc37 (Linear1(\ud835\udc65)), which was\nmotivated by the mechanics of \u0394 (Section 3.5). We observe that it can be generalized from dimension 1 to a larger\ndimension R. We set this to be a small fraction of D, which uses a negligible number of parameters compared to the main\nLinear projections in the block. We additionally note that the broadcasting operation can instead be viewed as another\nLinear projection, initialized to a specific pattern of 1\u2019s and 0\u2019s; if this projection is trainable, this leads to the alternative\n\ud835\udc60\u0394 (\ud835\udc65) = Linear\ud835\udc37 (Linear\ud835\udc45 (\ud835\udc65)), which can be viewed as a low-rank projection.\nIn our experiments, the \u0394 parameter (which can be viewed as a bias term) is initialized to \ud835\udf0f \u22121\nfollowing prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023).\n\n\u0394 (Uniform([0.001, 0.1])),\n\nRemark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models, because they are\nS4 models with a selection mechanism and computed with a scan.\n\n9\n\n\f4 Empirical Evaluation\n\nIn Section 4.1 we test Mamba\u2019s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on three\ndomains, each evaluated on autoregressive pretraining as well as downstream tasks.\n\n\u2022 Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.\n\n\u2022 Section 4.3: DNA sequence pretraining, and fine-tuning on a long-sequence classification task.\n\n\u2022 Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.\n\nFinally, Section 4.5 shows Mamba\u2019s computational efficiency at both training and inference time, and Section 4.6 ablates\nvarious components of the architecture and selective SSMs.\n\n4.1 Synthetic Tasks\nFull experiment details for these tasks including task details and training protocol are in Appendix E.1.\n\n4.1.1 Selective Copying\n\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\nthe memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and global\nconvolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for example, by\nconstructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated in earlier work on\nglobal convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut by randomizing the spacing\nbetween tokens. Note that this task has been introduced before as the Denoising task (Jing et al. 2019).\n\nNote that many previous works argue that adding architecture gating (multiplicative interactions) can endow models with\n\u201cdata-dependence\u201d and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al. 2023). However, we find this explanation\ninsufficient intuitively because such gating does not interact along the sequence axis, and cannot affect the spacing between\ntokens. In particular architecture gating is not an instance of a selection mechanism (Appendix A).\n\nTable 1 confirms that gated architectures such as H3 and Mamba only partially improve performance, while the selec-\ntion mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more powerful\narchitectures.\n\n4.1.2 Induction Heads\n\nInduction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021) that is\nsurprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative recall and copy:\nfor example, if the model has seen a bigram such as \u201cHarry Potter\u201d in the sequence, then the next time \u201cHarry\u201d appears in\nthe same sequence, the model should be able to predict \u201cPotter\u201d by copying from history.\n\nDataset. We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is\ncomparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate\ngeneralization and extrapolation abilities by evaluating on a range of sequence lengths from 26 = 64 up to 220 = 1048576 at\ntest time.\n\nFollowing established work on induction heads, we use 2 layer models, which allows attention to mechanistically\nModels.\nsolve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positional\nencodings) and SSM variants. We use a model dimension \ud835\udc37 of 64 for Mamba and 128 for the other models.\n\nResults. Table 2 shows that Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly\nbecause of its ability to selectively remember the relevant token while ignoring everything else in between. It generalizes\nperfectly to million-length sequences, or 4000\u00d7 longer than it saw during training, while no other method goes\nbeyond 2\u00d7.\n\n10\n\n\fModel\n\nArch.\n\nLayer\n\nAcc.\n\nS4\n-\n\nNo gate\nNo gate\n\nS4\nS6\n\nH3\nHyena\n-\n\nH3\nH3\nH3\n\nMamba\n-\n-\nMamba\nMamba Mamba\n\nS4\nHyena\nS6\n\nS4\nHyena\nS6\n\n18.3\n97.0\n\n57.0\n30.1\n99.7\n\n56.4\n28.4\n99.8\n\nTable 1: (Selective Copying.)\nAccuracy for combinations of architectures\nand inner sequence layers.\n\nTable 2: (Induction Heads.) Models are trained on sequence length 28 =\n256, and tested on increasing sequence lengths of 26 = 64 up to 220 =\n1048576. Full numbers in Table 11.\n\nFigure 4: (Scaling Laws.) Models of size \u2248 125\ud835\udc40 to \u2248 1.3\ud835\udc35 parameters, trained on the Pile. Mamba scales better than all other\nattention-free models and is the first to match the performance of a very strong \u201cTransformer++\u201d recipe that has now become standard,\nparticularly as the sequence length grows.\n\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly\nbetter than the others; also note that all attention models were only tested up to sequence length 214 = 16384 due to\nmemory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the findings in Poli et al. (2023).\n\n4.2 Language Modeling\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both\npretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3\nspecifications. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown\net al. (2020). All training details are in Appendix E.2.\n\n4.2.1 Scaling Laws\n\nFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the strongest\nTransformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa architectures (e.g.\nrotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). We also\ncompare against other recent subquadratic architectures (Figure 4). All model details are in Appendix E.2.\n\nFigure 4 shows scaling laws under the standard Chinchilla (Hoffmann et al. 2022) protocol, on models from \u2248 125\ud835\udc40 to \u2248 1.3\ud835\udc35\nparameters. Mamba is the first attention-free model to match the performance of a very strong Transformer\nrecipe (Transformer++) that has now become standard, particularly as the sequence length grows. (We note\nthat full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that\ncan also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealistic\ncomputation requirements.)\n\n11\n\n\u0000\u0015\u0000\u0014\u0000\u0016\u0000\u0015\u0000\u0014\u0000\u0017\u0000\u0015\u0000\u0014\u0000\u0018\u0000\u0015\u0000\u0014\u0000\u0019\u0000\u0015\u0000\u0014\u0000\u001a\u00008\u0000I\u0000W\u0000X\u0000\u0004\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0014\u0000\u0012\u0000\u0014\u0000\u0014\u0000\u0012\u0000\u0016\u0000\u0014\u0000\u0012\u0000\u0018\u0000\u0014\u0000\u0012\u0000\u001a\u0000\u0014\u0000\u0012\u0000\u001c\u0000\u0015\u0000\u0012\u0000\u0014\u0000%\u0000G\u0000G\u0000Y\u0000V\u0000E\u0000G\u0000]\u0000-\u0000R\u0000H\u0000Y\u0000G\u0000X\u0000M\u0000S\u0000R\u0000\u0004\u0000,\u0000I\u0000E\u0000H\u0000W\u0000\u0004\u0000)\u0000\\\u0000X\u0000V\u0000E\u0000T\u0000S\u0000P\u0000E\u0000X\u0000M\u0000S\u0000R\u00001\u0000,\u0000%\u0000\u0011\u0000%\u0000F\u0000W\u0000S\u0000P\u0000Y\u0000X\u0000I\u00001\u0000,\u0000%\u0000\u0011\u00006\u0000S\u00004\u0000)\u00001\u0000,\u0000%\u0000\u0011\u0000\\\u00004\u0000S\u0000W\u0000,\u0000\u0017\u0000,\u0000]\u0000I\u0000R\u0000E\u00001\u0000E\u0000Q\u0000F\u0000E\u00006\u0000E\u0000R\u0000H\u0000S\u0000Q\u00008\u0000V\u0000E\u0000M\u0000R\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u001d\u0000\u0015\u0000\u0014\u0000\u0016\u0000\u0014\u0000*\u00000\u00003\u00004\u0000W\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u001a\u0000\u0082\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u0016\u0000\u0082\u0000\u0015\u0000\u0014\u0000\u0015\u00004\u0000I\u0000V\u0000T\u0000P\u0000I\u0000\\\u0000M\u0000X\u0000]\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u00007\u0000G\u0000E\u0000P\u0000M\u0000R\u0000K\u0000\u0004\u00000\u0000E\u0000[\u0000W\u0000\u0004\u0000S\u0000R\u0000\u0004\u00008\u0000L\u0000I\u0000\u0004\u00004\u0000M\u0000P\u0000I\u0000\u0004\u0000\f\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0004\u0000\u0016\u0000\u0014\u0000\u0018\u0000\u001c\u0000\r\u0000,\u0000]\u0000I\u0000R\u0000E\u00006\u0000;\u0000/\u0000:\u00008\u0000V\u0000E\u0000R\u0000W\u0000J\u0000S\u0000V\u0000Q\u0000I\u0000V\u00006\u0000I\u0000X\u00002\u0000I\u0000X\u0000,\u0000\u0017\u0000\u000f\u0000\u000f\u00008\u0000V\u0000E\u0000R\u0000W\u0000J\u0000S\u0000V\u0000Q\u0000I\u0000V\u0000\u000f\u0000\u000f\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u001d\u0000\u0015\u0000\u0014\u0000\u0016\u0000\u0014\u0000*\u00000\u00003\u00004\u0000W\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u001a\u0000\u0082\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u0016\u0000\u0082\u0000\u0015\u0000\u0014\u0000\u0015\u00004\u0000I\u0000V\u0000T\u0000P\u0000I\u0000\\\u0000M\u0000X\u0000]\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u00007\u0000G\u0000E\u0000P\u0000M\u0000R\u0000K\u0000\u0004\u00000\u0000E\u0000[\u0000W\u0000\u0004\u0000S\u0000R\u0000\u0004\u00008\u0000L\u0000I\u0000\u0004\u00004\u0000M\u0000P\u0000I\u0000\u0004\u0000\f\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0004\u0000\u001c\u0000\u0015\u0000\u001d\u0000\u0016\u0000\r\u0000,\u0000]\u0000I\u0000R\u0000E\u00006\u0000;\u0000/\u0000:\u00008\u0000V\u0000E\u0000R\u0000W\u0000J\u0000S\u0000V\u0000Q\u0000I\u0000V\u00006\u0000I\u0000X\u00002\u0000I\u0000X\u0000,\u0000\u0017\u0000\u000f\u0000\u000f\u00008\u0000V\u0000E\u0000R\u0000W\u0000J\u0000S\u0000V\u0000Q\u0000I\u0000V\u0000\u000f\u0000\u000f\u00001\u0000E\u0000Q\u0000F\u0000E\f4.2.2 Downstream Evaluations\n\nTable 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare\nagainst the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and\nRWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our\nmodels. (Note that Mamba and Pythia are trained with context length 2048, while RWKV was trained with context length\n1024.)\n\nTable 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare against open source LMs with various tokenizers,\ntrained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer\n(GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches baselines at twice\nthe model size.\n\nModel\n\nToken.\n\nHybrid H3-130M GPT2\nNeoX\nPythia-160M\nNeoX\nMamba-130M\n\nHybrid H3-360M GPT2\nNeoX\nPythia-410M\nNeoX\nMamba-370M\n\nPythia-1B\nMamba-790M\n\nGPT-Neo 1.3B\nHybrid H3-1.3B\nOPT-1.3B\nPythia-1.4B\nRWKV-1.5B\nMamba-1.4B\n\nGPT-Neo 2.7B\nHybrid H3-2.7B\nOPT-2.7B\nPythia-2.8B\nRWKV-3B\nMamba-2.8B\n\nGPT-J-6B\nOPT-6.7B\nPythia-6.9B\nRWKV-7.4B\n\nNeoX\nNeoX\n\nGPT2\nGPT2\nOPT\nNeoX\nNeoX\nNeoX\n\nGPT2\nGPT2\nOPT\nNeoX\nNeoX\nNeoX\n\nGPT2\nOPT\nNeoX\nNeoX\n\nPile\nppl \u2193\n\n\u2014\n29.64\n10.56\n\n\u2014\n9.95\n8.28\n\n7.82\n7.33\n\n\u2014\n\u2014\n\u2014\n7.51\n7.70\n6.80\n\n\u2014\n\u2014\n\u2014\n6.73\n7.00\n6.22\n\n\u2013\n\u2013\n6.51\n6.31\n\nLAMBADA LAMBADA HellaSwag\nppl \u2193\n\nacc \u2191\n\nacc \u2191\n\nPIQA Arc-E Arc-C WinoGrande Average\nacc \u2191\n\nacc \u2191\n\nacc \u2191\n\nacc \u2191\n\nacc \u2191\n\n89.48\n38.10\n16.07\n\n12.58\n10.84\n8.14\n\n7.92\n6.02\n\n7.50\n11.25\n6.64\n6.08\n7.04\n5.04\n\n5.63\n7.92\n5.12\n5.04\n5.24\n4.23\n\n4.10\n4.25\n4.45\n4.38\n\n25.77\n33.0\n44.3\n\n48.0\n51.4\n55.6\n\n56.1\n62.7\n\n57.2\n49.6\n58.0\n61.7\n56.4\n64.9\n\n62.2\n55.7\n63.6\n64.7\n63.9\n69.2\n\n68.3\n67.7\n67.1\n67.2\n\n31.7\n30.2\n35.3\n\n41.5\n40.6\n46.5\n\n47.2\n55.1\n\n48.9\n52.6\n53.7\n52.1\n52.5\n59.1\n\n55.8\n59.7\n60.6\n59.3\n59.6\n66.1\n\n66.3\n67.2\n64.0\n65.5\n\n64.2\n61.4\n64.5\n\n68.1\n66.9\n69.5\n\n70.7\n72.1\n\n71.1\n71.3\n72.4\n71.0\n72.4\n74.2\n\n72.1\n73.3\n74.8\n74.0\n73.7\n75.2\n\n75.4\n76.3\n75.2\n76.1\n\n44.4\n43.2\n48.0\n\n51.4\n52.1\n55.1\n\n57.0\n61.2\n\n56.2\n59.2\n56.7\n60.5\n60.5\n65.5\n\n61.1\n65.6\n60.8\n64.1\n67.8\n69.7\n\n67.0\n65.6\n67.3\n67.8\n\n24.2\n24.1\n24.3\n\n24.7\n24.6\n28.0\n\n27.1\n29.5\n\n25.9\n28.1\n29.6\n28.5\n29.4\n32.8\n\n30.2\n32.3\n31.3\n32.9\n33.1\n36.3\n\n36.6\n34.9\n35.5\n37.5\n\n50.6\n51.9\n51.9\n\n54.1\n53.8\n55.3\n\n53.5\n56.1\n\n54.9\n56.9\n59.5\n57.2\n54.6\n61.5\n\n57.6\n61.4\n61.0\n59.7\n59.6\n63.5\n\n64.1\n65.5\n61.3\n61.0\n\n40.1\n40.6\n44.7\n\n48.0\n48.2\n50.0\n\n51.9\n57.1\n\n52.4\n53.0\n55.0\n55.2\n54.3\n59.7\n\n56.5\n58.0\n58.7\n59.1\n59.6\n63.3\n\n63.0\n62.9\n61.7\n62.5\n\n4.3 DNA Modeling\nMotivated by the success of large language models, there has been recent exploration into using the foundation model\nparadigm for genomics. DNA has been likened to language in that it consists of sequences of discrete tokens with a finite\nvocabulary. It is also known for requiring long-range dependencies to model (Avsec et al. 2021). We investigate Mamba as\na FM backbone for pretraining and fine-tuning in the same setting as recent works on long-sequence models for DNA\n(Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling laws across model size and sequence\nlength (Figure 5), and a difficult downstream synthetic classification task requiring long context (Figure 6).\n\nFor pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training and\nmodel details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen, Poli, et al. 2023),\nwhich uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5 billion tokens (DNA base\npairs) in the training split.\n\n12\n\n\fFigure 5: (DNA Scaling Laws.) Pretraining on the HG38 (human genome) dataset. (Left) Fixing short context length 210 = 1024 and\nincreasing size from \u2248 200\ud835\udc3e to \u2248 40\ud835\udc40 parameters, Mamba scales better than baselines. (Right) Fixing model size and increasing sequence\nlengths while keeping tokens/batch and total training tokens fixed. Unlike baselines, the selection mechanism of Mamba facilitates\nbetter performance with increasing context length.\n\n4.3.1 Scaling: Model Size\n\nIn this experiment, we investigate the scaling properties of genomics foundation models with various model backbones\n(Figure 5 Left).\n\nTraining. To advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we expect\nresults to favor Mamba even more at longer sequence lengths. We fix a global batch size of 1024, for a total of 220 \u2248 1\ud835\udc40\ntokens per batch. Models were trained for 10\ud835\udc3e gradient steps for a total of 10\ud835\udc35 tokens.\n\nFigure 5 (Left) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and that Mamba\nResults.\nscales better than both HyenaDNA and Transformer++. For example, at the largest model size of \u2248 40\ud835\udc40 parameters, the\ncurve shows that Mamba can match the Transformer++ and HyenaDNA models with roughly 3\u00d7 to 4\u00d7 fewer\nparameters.\n\n4.3.2 Scaling: Context Length\n\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We only\ncompare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence\nlengths. We pretrain models on sequence lengths 210 = 1024, 212 = 4096, 214 = 16384, 216 = 65536, 218 = 262144,\n220 = 1048576. We fix a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20\ud835\udc3e\ngradient steps for a total of \u2248 330\ud835\udc35 tokens. The longer sequence lengths used sequence length warmup similar to (Nguyen,\nPoli, et al. 2023).\n\nFigure 5 (Right) shows that Mamba is able to make use of longer context even up to extremely long\nResults.\nsequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand, the\nHyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of the\nselection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a\nvery long convolution kernel is aggregating all information across a long sequence which may be very noisy. Note that\nwhile HyenaDNA claims to improve with longer context, their results do not control for computation time.\n\n4.3.3 Synthetic Species Classification\n\nWe evaluate models on a downstream task of classifying between 5 different species by randomly sampling a contiguous\nsegment of their DNA. This task is adapted from HyenaDNA, which used the species {human, lemur, mouse, pig, hippo}.\nWe modify the task to be significantly more challenging by classifying between the five great apes species\n{human, chimpanzee, gorilla, orangutan, bonobo}, which are known to share 99% of their DNA.\n\n13\n\n\u0000\u0015\u0000\u0014\u0000\u001a\u0000\u0015\u0000\u0014\u0000\u001b\u00004\u0000E\u0000V\u0000E\u0000Q\u0000I\u0000X\u0000I\u0000V\u0000W\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u0000\u0016\u0000\u0012\u0000\u001b\u0000\u0016\u0000\u0012\u0000\u001c\u0000\u0016\u0000\u0012\u0000\u001d\u0000\u0017\u0000\u0012\u0000\u0014\u0000\u0017\u0000\u0012\u0000\u0015\u00004\u0000I\u0000V\u0000T\u0000P\u0000I\u0000\\\u0000M\u0000X\u0000]\u00007\u0000G\u0000E\u0000P\u0000M\u0000R\u0000K\u0000\u0004\u00000\u0000E\u0000[\u0000W\u0000\u0004\u0000S\u0000R\u0000\u0004\u0000X\u0000L\u0000I\u0000\u0004\u0000,\u0000Y\u0000Q\u0000E\u0000R\u0000\u0004\u0000+\u0000I\u0000R\u0000S\u0000Q\u0000I\u0000\u0004\u0000\f\u0000,\u0000+\u0000\u0017\u0000\u001c\u0000\r\u0000,\u0000]\u0000I\u0000R\u0000E\u0000(\u00002\u0000%\u00001\u0000E\u0000Q\u0000F\u0000E\u00008\u0000V\u0000E\u0000R\u0000W\u0000J\u0000S\u0000V\u0000Q\u0000I\u0000V\u0000\u000f\u0000\u000f\u0000\u0015\u0000\u0014\u0000\u0017\u0000\u0015\u0000\u0014\u0000\u0018\u0000\u0015\u0000\u0014\u0000\u0019\u0000\u0015\u0000\u0014\u0000\u001a\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0016\u0000\u0012\u0000\u001b\u0000\u0019\u0000\u0016\u0000\u0012\u0000\u001c\u0000\u0014\u0000\u0016\u0000\u0012\u0000\u001c\u0000\u0019\u0000\u0016\u0000\u0012\u0000\u001d\u0000\u0014\u0000\u0016\u0000\u0012\u0000\u001d\u0000\u0019\u0000\u0017\u0000\u0012\u0000\u0014\u0000\u0014\u00004\u0000I\u0000V\u0000T\u0000P\u0000I\u0000\\\u0000M\u0000X\u0000]\u00007\u0000G\u0000E\u0000P\u0000M\u0000R\u0000K\u0000\u0004\u00000\u0000E\u0000[\u0000W\u0000\u0004\u0000\u0011\u0000\u0004\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0004\u0000\f\u0000,\u0000+\u0000\u0017\u0000\u001c\u0000\r\u0000,\u0000]\u0000I\u0000R\u0000E\u0000(\u00002\u0000%\u0000\u0004\u0000\u0015\u0000\u0012\u0000\u0018\u00001\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\u0015\u0000\u0012\u0000\u0018\u00001\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\u001b\u00001\fFigure 6: (Great Apes DNA Classification.) Accuracy after fine-\ntuning on sequences of length 210 = 1024 up to 220 = 1048576 using\npretrained models of the same context length. Numerical results in\nTable 13.\n\nFigure 7: (Audio Pretraining.) Mamba improves performance\nover prior state-of-the-art (Sashimi) in autoregressive audio model-\ning, while improving up to minute-long context or million-length\nsequences (controlling for computation).\n\n4.4 Audio Modeling and Generation\nFor the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel et al.\n2022). This model comprises:\n\n1. a U-Net backbone with two stages of pooling by a factor \ud835\udc5d that doubles the model dimension \ud835\udc37 per stage,\n\n2. alternating S4 and MLP blocks in each stage.\n\nWe consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4.\n\n4.4.1 Long-Context Autoregressive Pretraining\n\nWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standard\npiano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of 16000 Hz. Pretraining\ndetails largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the effect of increasing training\nsequence lengths from 213 = 8192 to 220 \u2248 106, while keeping computation fixed. (There are some slight edge cases to the\nway the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were available\nso the maximum sequence length is actually bounded by 60\ud835\udc60 \u00b7 16000\ud835\udc3b\ud835\udc67 = 960000.)\n\nBoth Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is\nbetter throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a constant\nfactor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.\n\nWe note one important detail: this is the only experiment in this paper in which we switched from the real parameterization\nto complex (Section 3.6). We show additional ablations in Appendix E.4.\n\n4.4.2 Autoregressive Speech Generation\n\nSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of\n1-second clips sampled at 16000 Hz of the digits \u201czero\u201d through \u201cnine\u201d with highly variable characteristics. We largely\nfollow the autoregressive training setup and generation protocol of Goel et al. (2022).\n\nTable 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022):\nWaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), DiffWave (Z.\nKong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GAN-\nand diffusion- based models. A larger model parameter-matched to the baselines further improves on fidelity metrics\ndramatically.\n\nTable 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages and\ncenter stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba > S4+MLP >\nMHA+MLP in the center blocks.\n\n14\n\n\u0000\u0015\u0000\u0014\u0000\u0017\u0000\u0015\u0000\u0014\u0000\u0018\u0000\u0015\u0000\u0014\u0000\u0019\u0000\u0015\u0000\u0014\u0000\u001a\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0014\u0000\u0012\u0000\u0016\u0000\u0014\u0000\u0012\u0000\u0017\u0000\u0014\u0000\u0012\u0000\u0018\u0000\u0014\u0000\u0012\u0000\u0019\u0000\u0014\u0000\u0012\u0000\u001a\u0000\u0014\u0000\u0012\u0000\u001b\u0000\u0014\u0000\u0012\u0000\u001c\u0000%\u0000G\u0000G\u0000Y\u0000V\u0000E\u0000G\u0000]\u0000*\u0000M\u0000R\u0000I\u0000X\u0000Y\u0000R\u0000M\u0000R\u0000K\u0000\u0004\u0000%\u0000G\u0000G\u0000Y\u0000V\u0000E\u0000G\u0000]\u0000\u0004\u0000\f\u00007\u0000T\u0000I\u0000G\u0000M\u0000I\u0000W\u0000\u0004\u0000(\u00002\u0000%\u0000\u0004\u0000'\u0000P\u0000E\u0000W\u0000W\u0000M\u0000J\u0000M\u0000G\u0000E\u0000X\u0000M\u0000S\u0000R\u0000\r\u0000,\u0000]\u0000I\u0000R\u0000E\u0000(\u00002\u0000%\u0000\u0004\u0000\u0015\u0000\u0012\u0000\u0018\u00001\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\u0015\u0000\u0012\u0000\u0018\u00001\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\u001b\u00001\u00006\u0000E\u0000R\u0000H\u0000S\u0000Q\u0000\u0015\u0000\u0014\u0000\u0018\u0000\u0015\u0000\u0014\u0000\u0019\u0000\u0015\u0000\u0014\u0000\u001a\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u0014\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u0016\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u0019\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u001b\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u0014\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u0016\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u0019\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u001b\u0000\u0019\u0000&\u0000M\u0000X\u0000W\u0000\u0004\u00004\u0000I\u0000V\u0000\u0004\u0000&\u0000]\u0000X\u0000I\u00007\u0000G\u0000E\u0000P\u0000M\u0000R\u0000K\u0000\u0004\u00000\u0000E\u0000[\u0000W\u0000\u0004\u0000\u0011\u0000\u0004\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0004\u0000\f\u0000=\u0000S\u0000Y\u00008\u0000Y\u0000F\u0000I\u00001\u0000M\u0000\\\u0000\r\u00007\u0000\u0018\u0000\u000f\u0000*\u0000*\u00002\u00001\u0000E\u0000Q\u0000F\u0000E\fTable 4: (SC09) Automated metrics for unconditional generation on\na challenging dataset of fixed-length speech clips. (Top to Bottom)\nAutoregressive baselines, non-autoregressive baselines, Mamba, and\ndataset metrics.\n\nModel\n\nParams\n\nNLL \u2193\n\nFID \u2193\n\nSampleRNN\nWaveNet\nSaShiMi\n\nWaveGAN\nDiffWave\n\n+ SaShiMi\n\nMamba\nMamba\n\nTrain\nTest\n\n35.0M\n4.2M\n5.8M\n\n19.1M\n24.1M\n23.0M\n\n6.1M\n24.3M\n\n-\n-\n\n2.042\n1.925\n1.873\n\n-\n-\n-\n\n1.852\n1.860\n\n-\n-\n\n8.96\n5.08\n1.99\n\n2.03\n1.92\n1.42\n\n0.94\n0.67\n\n0.00\n0.02\n\nIS \u2191\n\n1.71\n2.27\n5.13\n\n4.90\n5.26\n5.94\n\n6.26\n7.33\n\n8.56\n8.33\n\nmIS \u2191\n\nAM \u2193\n\n3.02\n5.80\n42.57\n\n36.10\n51.21\n69.17\n\n88.54\n144.9\n\n292.5\n257.6\n\n1.76\n1.47\n0.74\n\n0.80\n0.68\n0.59\n\n0.52\n0.36\n\n0.16\n0.19\n\nTable 5: (SC09 Model Ablations) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\nOuter\n\nCenter\n\nNLL \u2193\n\nFID \u2193\n\nS4+MLP\nS4+MLP\nS4+MLP\nMamba\nMamba\nMamba\n\nMHA+MLP\nS4+MLP\nMamba\nMHA+MLP\nS4+MLP\nMamba\n\n1.859\n1.867\n1.859\n1.850\n1.853\n1.852\n\n1.45\n1.43\n1.42\n1.37\n1.07\n0.94\n\nIS \u2191\n\n5.06\n5.42\n5.71\n5.63\n6.05\n6.26\n\nmIS \u2191\n\n47.03\n53.54\n56.51\n58.23\n73.34\n88.54\n\nAM \u2193\n\n0.70\n0.65\n0.64\n0.62\n0.55\n0.52\n\n4.5 Speed and Memory Benchmarks\nWe benchmark the speed of the SSM scan operation (state expansion \ud835\udc41 = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference\nthroughput than a 5\u00d7 smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark of\nmemory consumption.\n\nFigure 8: (Efficiency Benchmarks.) (Left) Training: our efficient scan is 40\u00d7 faster than a standard implementation. (Right) Inference:\nas a recurrent model, Mamba can achieve 5\u00d7 higher throughput than Transformers.\n\n4.6 Model Ablations\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with\nsize \u2248 350M models at Chinchilla token counts (same setting as Figure 4).\n\n4.6.1 Architecture\n\nTable 6 investigates the effects of the architecture (block) and its inner SSM layer (Figure 3). We find that\n\n\u2022 Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.\n\n\u2022 Replacing the complex-valued S4 variant from previous work with a real-valued one does not affect performance much,\nsuggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware efficiency.\n\n\u2022 Replacing any of these with a selective SSM (S6) significantly improves performance, validating the motivation of\n\nSection 3.\n\n15\n\n\u0000\u0019\u0000\u0015\u0000\u0016\u0000\u0015\u0000O\u0000\u0016\u0000O\u0000\u0018\u0000O\u0000\u001c\u0000O\u0000\u0015\u0000\u001a\u0000O\u0000\u0017\u0000\u0016\u0000O\u0000\u001a\u0000\u0018\u0000O\u0000\u0015\u0000\u0016\u0000\u001c\u0000O\u0000\u0016\u0000\u0019\u0000\u001a\u0000O\u0000\u0019\u0000\u0015\u0000\u0016\u0000O\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u0000P\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0014\u0000\u0012\u0000\u0015\u0000\u0015\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u0014\u00008\u0000M\u0000Q\u0000I\u0000\u0004\u0000\f\u0000Q\u0000W\u0000\r\u00007\u0000G\u0000E\u0000R\u0000\u0004\u0000Z\u0000W\u0000\u0004\u0000'\u0000S\u0000R\u0000Z\u0000S\u0000P\u0000Y\u0000X\u0000M\u0000S\u0000R\u0000\u0004\u0000Z\u0000W\u0000\u0004\u0000%\u0000X\u0000X\u0000I\u0000R\u0000X\u0000M\u0000S\u0000R\u0000\u0004\u0000X\u0000M\u0000Q\u0000I\u0000\u0004\u0000\f\u0000%\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u0004\u0000\u001c\u0000\u0014\u0000+\u0000&\u0000\u0004\u00004\u0000'\u0000-\u0000I\u0000\r\u0000*\u0000P\u0000E\u0000W\u0000L\u0000%\u0000X\u0000X\u0000I\u0000R\u0000X\u0000M\u0000S\u0000R\u0000\u0011\u0000\u0016\u0000'\u0000S\u0000R\u0000Z\u0000S\u0000P\u0000Y\u0000X\u0000M\u0000S\u0000R\u00007\u0000G\u0000E\u0000R\u0000\u0004\u0000\f\u00004\u0000]\u00008\u0000S\u0000V\u0000G\u0000L\u0000\r\u00007\u0000G\u0000E\u0000R\u0000\u0004\u0000\f\u0000S\u0000Y\u0000V\u0000W\u0000\r\u00003\u00003\u00001\u0000\u0015\u0000\u0016\u0000\u0018\u0000\u001c\u0000\u0015\u0000\u001a\u0000\u0017\u0000\u0016\u0000\u001a\u0000\u0018\u0000\u0015\u0000\u0016\u0000\u001c\u0000&\u0000E\u0000X\u0000G\u0000L\u0000\u0004\u0000W\u0000M\u0000^\u0000I\u0000\u0019\u0000\u0014\u0000\u0014\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u0014\u0000\u0015\u0000\u0019\u0000\u0014\u0000\u0014\u00008\u0000L\u0000V\u0000S\u0000Y\u0000K\u0000L\u0000T\u0000Y\u0000X\u0000\u0004\u0000\f\u0000X\u0000S\u0000O\u0000I\u0000R\u0000W\u0000\u0004\u0000\u0013\u0000\u0004\u0000W\u0000\r\u0000\u0015\u0000\u0018\u0000\u0014\u0000\u0016\u0000\u0018\u0000\u001b\u0000\u0018\u0000\u0018\u0000\u0015\u0000\u001b\u0000\u0018\u0000\u0018\u0000\u0015\u0000\u0014\u0000\u001c\u0000\u001d\u0000\u0015\u0000\u0018\u0000\u0018\u0000\u0019\u0000\u0015\u0000\u001a\u0000\u001c\u0000\u001c\u0000\u0015\u0000\u001c\u0000\u0015\u0000\u0018\u0000\u001b\u0000\u001d\u0000\u0015\u0000\u0017\u0000\u0016\u0000\u0015\u0000\u001d\u0000\u001d\u0000\u0016\u0000\u001a\u0000\u0019\u0000\u0017\u0000\u0016\u0000\u0017\u0000\u0017\u0000\u001a\u0000\u0018\u00003\u00003\u00001\u00003\u00003\u00001\u0000\u0019\u0000\u001c\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u0015\u0000\u001b\u0000\u0016\u0000\u0016\u0000\u001a\u0000\u0015\u0000\u0017\u0000\u001a\u0000\u0018\u0000\u0018\u0000\u0018\u0000\u0017\u0000\u0018\u0000\u001d\u0000\u0014\u0000\u0019\u0000\u0015\u0000\u0019\u0000\u0018\u0000\u001a\u0000\u001a\u0000\u001a\u0000\u001d\u0000\u0015\u0000\u0015\u0000\u0014\u0000\u001d\u0000\u0015\u0000\u0016\u0000\u0014\u00003\u00003\u00001\u00003\u00003\u00001\u00003\u00003\u00001\u0000-\u0000R\u0000J\u0000I\u0000V\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u0000X\u0000L\u0000V\u0000S\u0000Y\u0000K\u0000L\u0000T\u0000Y\u0000X\u0000\u0004\u0000S\u0000R\u0000\u0004\u0000%\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u0004\u0000\u001c\u0000\u0014\u0000+\u0000&\u0000\u0004\u0000\f\u0000T\u0000V\u0000S\u0000Q\u0000T\u0000X\u0000\u0004\u0000P\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0004\u0000\u0016\u0000\u0014\u0000\u0018\u0000\u001c\u0000\r\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\u0015\u0000\u0012\u0000\u0018\u0000&\u00008\u0000V\u0000E\u0000R\u0000W\u0000J\u0000S\u0000V\u0000Q\u0000I\u0000V\u0000\u0004\u0000\u0015\u0000\u0012\u0000\u0017\u0000&\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\u001a\u0000\u0012\u0000\u001d\u0000&\u00008\u0000V\u0000E\u0000R\u0000W\u0000J\u0000S\u0000V\u0000Q\u0000I\u0000V\u0000\u0004\u0000\u001a\u0000\u0012\u0000\u001b\u0000&\fTable 6: (Ablations: Architecture and SSM layer.) The Mamba block performs similarly to H3 while being simpler. In the inner layer,\nthere is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More\nspecifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\n\nModel Arch.\n\nSSM Layer\n\nPerplexity\n\nModel Arch.\n\nSSM Layer\n\nPerplexity\n\nHyena H3\nH3\nH3\nH3\n-\nH3\n-\n\nHyena\nS4 (complex)\nS4 (real)\nS6\n\n10.24\n10.30\n10.34\n8.95\n\nMamba Hyena\n-\nMamba\n-\n-\nMamba\nMamba Mamba\n\nS4 (complex)\nS4 (real)\nS6\n\n10.75\n10.54\n10.56\n8.69\n\nTable 7: (Ablations: Selective parameters.) \u0394 is the most impor-\ntant parameter (Theorem 1), but using multiple selective parameters\ntogether synergizes.\n\nTable 8: (Ablations: Parameterization of \ud835\udc68.) The more\nstandard initializations based on S4D-Lin (Gu, Gupta, et al.\n2022) perform worse than S4D-Real or a random initialization,\nwhen the SSM is selective.\n\nSelective \u0394 Selective \ud835\udc69 Selective \ud835\udc6a\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\n\u2717\n\u2713\n\u2717\n\u2717\n\u2713\n\n\u2717\n\u2717\n\u2713\n\u2717\n\u2713\n\nPerplexity\n\n10.93\n10.15\n9.98\n9.81\n8.71\n\n2 + \ud835\udc5b\ud835\udc56\n\n\ud835\udc68\ud835\udc5b Initialization Field\n\ud835\udc68\ud835\udc5b = \u2212 1\n\ud835\udc68\ud835\udc5b = \u22121/2\n\ud835\udc68\ud835\udc5b = \u2212(\ud835\udc5b + 1)\n\ud835\udc68\ud835\udc5b \u223c exp(N (0, 1))\n\nComplex\nReal\nReal\nReal\n\nPerplexity\n\n9.16\n8.85\n8.71\n8.71\n\n\u2022 The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selective\n\nlayer).\n\nWe also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid\nattention architecture) in Appendix E.2.2.\n\n4.6.2 Selective SSM\nTable 7 ablates the selective SSM layer by considering different combinations of selective \u0394, \ud835\udc69, and \ud835\udc6a parameters (Algo-\nrithm 2), showing that \u0394 is the most important parameter due to its connection to RNN gating (Theorem 1).\n\nTable 8 considers different initializations of the SSM, which have been shown to make a large difference in some data\nmodalities and settings (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022). On language modeling, we find that simpler\nreal-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued parameterizations (S4D-Lin,\nrow 1) perform better. Random initializations also work well, consistent with findings from prior work (Mehta et al.\n2023).\n\nTable 9 and Table 10 consider varying the dimension of the \u0394 and (\ud835\udc69, \ud835\udc6a) projections respectively. Changing them from\nstatic to selective provides the most benefit, while increasing the dimensions further generally improves performance\nmodestly with a small increase in parameter count.\n\nOf particular note is the dramatic improvement of the selective SSM when the state size \ud835\udc41 is increased, with over a 1.0\nperplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1\nand 3.3.\n\n5 Discussion\n\nWe discuss related work, limitations, and some future directions.\n\nRelated Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an\nextended related work of SSMs and other related models.\n\n16\n\n\fTable 9: (Ablations: Expressivity of \u0394.)\nThe selection mechanism of \u0394 constructs it\nwith a projection of the input. Projecting it\neven to dim. 1 provides a large increase in\nperformance; increasing it further provides\nfurther improvements at the cost of a mod-\nest increase in parameters. State size fixed\nto \ud835\udc41 = 16.\n\nTable 10: (Ablations: SSM state dimension.) (Top) Constant \ud835\udc69 and \ud835\udc6a (Bottom) Selective\n\ud835\udc69 and \ud835\udc6a. Increasing the SSM state dimension \ud835\udc41 , which can be viewed as an expansion\nfactor on the dimension of the recurrent state, can significantly improve performance for\na negligible cost in parameters/FLOPs, but only when \ud835\udc69 and \ud835\udc6a are also selective. Size of\n\u0394 projection fixed to 64.\n\nState dimension \ud835\udc41\n\nParams (M)\n\nPerplexity\n\nSize of \u0394 proj.\n\nParams (M)\n\nPerplexity\n\n-\n1\n2\n4\n8\n16\n32\n64\n\n358.9\n359.1\n359.3\n359.7\n360.5\n362.1\n365.2\n371.5\n\n9.12\n8.97\n8.97\n8.91\n8.83\n8.84\n8.80\n8.71\n\n1\n2\n4\n8\n16\n\n1\n2\n4\n8\n16\n\n367.1\n367.4\n368.0\n369.1\n371.5\n\n367.1\n367.4\n368.0\n369.1\n371.5\n\n9.88\n9.86\n9.82\n9.82\n9.81\n\n9.73\n9.40\n9.09\n8.84\n8.71\n\nStructured SSMs were originally defined as discretizations of\nNo Free Lunch: Continuous-Discrete Spectrum.\ncontinuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual\nsignals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on\ndiscrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel\non. Our ablations on audio waveforms examine this tradeoff in more detail.\n\nDownstream Affordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of proper-\nties and modes of interaction with pretrained models, such as fine-tuning, adaptation, prompting, in-context learning,\ninstruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such\nas SSMs have similar properties and affordances.\n\nScaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs\n(e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun\net al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still\ncompares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and\nadjustments to the model that are not discussed in this paper.\n\n6 Conclusion\n\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba\nachieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong\nTransformer models. We are excited about the broad applications of selective state space models to build foundation models\nfor different domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our\nresults suggest that Mamba is a strong candidate to be a general sequence model backbone.\n\nAcknowledgments\n\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\n\nReferences\n\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. \u201cUnitary Evolution Recurrent Neural Networks\u201d. In: The Interna-\n\ntional Conference on Machine Learning (ICML). 2016, pp. 1120\u20131128.\n\n17\n\n\f[2] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \u201cEffective Gene Expression Prediction from\nSequence by Integrating Long-range Interactions\u201d. In: Nature Methods 18.10 (2021), pp. 1196\u20131203.\nJimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. \u201cUsing Fast Weights to Attend\nto the Recent Past\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016).\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \u201cLayer Normalization\u201d. In: arXiv preprint arXiv:1607.06450\n(2016).\n\n[4]\n\n[3]\n\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \u201cNeural Machine Translation by Jointly Learning to\n\nAlign and Translate\u201d. In: The International Conference on Learning Representations (ICLR). 2015.\n\n[6] David Balduzzi and Muhammad Ghifary. \u201cStrongly-typed Recurrent Neural Networks\u201d. In: International Conference\n\non Machine Learning. PMLR. 2016, pp. 1292\u20131300.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. \u201cPythia: A Suite for Analyzing\nLarge Language Models across Training and Scaling\u201d. In: The International Conference on Machine Learning (ICML).\nPMLR. 2023, pp. 2397\u20132430.\n\n[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. \u201cPIQA: Reasoning about Physical Commonsense in\n\nNatural Language\u201d. In: Proceedings of the AAAI conference on Artificial Intelligence. Vol. 34. 2020.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\nKyle McDonell, Jason Phang, et al. \u201cGpt-NeoX-20B: An Open-source Autoregressive Language Model\u201d. In: arXiv\npreprint arXiv:2204.06745 (2022).\n\n[10] Guy E Blelloch. \u201cPrefix Sums and Their Applications\u201d. In: (1990).\n[11]\n\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. \u201cQuasi-recurrent Neural Networks\u201d. In:\narXiv preprint arXiv:1611.01576 (2016).\n\n[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. \u201cLanguage Models are Few-shot Learners\u201d. In: Advances in\nNeural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877\u20131901.\n\n[13] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. \u201cScaling Transformer to 1M tokens and Beyond with RMT\u201d.\n\n[7]\n\n[9]\n\nIn: arXiv preprint arXiv:2304.11062 (2023).\n\n[14] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. \u201cGenerating Long Sequences with Sparse Transformers\u201d.\n\nIn: arXiv preprint arXiv:1904.10509 (2019).\n\n[15] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \u201cRethinking Attention with Performers\u201d. In: The\nInternational Conference on Learning Representations (ICLR). 2021.\n\n[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. \u201cPaLM: Scaling Language Modeling with Pathways\u201d.\nIn: Journal of Machine Learning Research 24.240 (2023), pp. 1\u2013113. url: http://jmlr.org/papers/v24/22-\n1144.html.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \u201cEmpirical Evaluation of Gated Recurrent\nNeural Networks on Sequence Modeling\u201d. In: arXiv preprint arXiv:1412.3555 (2014).\n\n[17]\n\n[18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n\n\u201cThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\u201d. In: arXiv preprint arXiv:1803.05457\n(2018).\n\n[19] Tri Dao. \u201cFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\u201d. In: The International\n\nConference on Learning Representations (ICLR). 2024.\n\n[20] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cFlashAttention: Fast and Memory-Efficient\nExact Attention with IO-Awareness\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2022.\n[21] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. \u201cHungry Hungry Hippos:\nTowards Language Modeling with State Space Models\u201d. In: The International Conference on Learning Representations\n(ICLR). 2023.\n\n[22] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. \u201cLanguage Modeling with Gated Convolutional\n\nNetworks\u201d. In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933\u2013941.\n[23] DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017.\n[24]\n\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. \u201cLongNet:\nScaling Transformers to 1,000,000,000 Tokens\u201d. In: arXiv preprint arXiv:2307.02486 (2023).\n\n18\n\n\f[25] Chris Donahue, Julian McAuley, and Miller Puckette. \u201cAdversarial Audio Synthesis\u201d. In: The International Conference\n\non Learning Representations (ICLR). 2019.\n\n[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. \u201cAn Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale\u201d. In: The International Conference on Learning Representations (ICLR).\n2020.\n\n[27] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao\nBai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,\nAndy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. \u201cA Mathematical Framework for Transformer Circuits\u201d. In: Transformer Circuits\nThread (2021). https://transformer-circuits.pub/2021/framework/index.html.\n\n[28] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. \u201cBlock-State\n\nTransformer\u201d. In: arXiv preprint arXiv:2306.09539 (2023).\n\n[29] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,\nYangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. \u201cMulti-Head State Space Model for Speech\nRecognition\u201d. In: Proc. INTERSPEECH 2023. 2023, pp. 241\u2013245. doi: 10.21437/Interspeech.2023-1036.\n[30] Karl J Friston, Lee Harrison, and Will Penny. \u201cDynamic Causal Modelling\u201d. In: Neuroimage 19.4 (2003), pp. 1273\u2013\n\n1302.\n\n[31] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher\nR\u00e9. \u201cSimple Hardware-efficient Long Convolutions for Sequence Modeling\u201d. In: The International Conference on\nMachine Learning (ICML) (2023).\n\n[32] Ken-ichi Funahashi and Yuichi Nakamura. \u201cApproximation of Dynamical Systems by Continuous Time Recurrent\n\nNeural Networks\u201d. In: Neural Networks 6.6 (1993), pp. 801\u2013806.\n\n[33] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. \u201cThe Pile: An 800GB Dataset of Diverse Text for\nLanguage Modeling\u201d. In: arXiv preprint arXiv:2101.00027 (2020).\n\n[34] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin\nWang, and Andy Zou. A Framework for Few-shot Language Model Evaluation. Version v0.0.1. Sept. 2021. doi:\n10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628.\n\n[35] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. \u201cIt\u2019s Raw! Audio Generation with State-Space Models\u201d.\n\nIn: The International Conference on Machine Learning (ICML). 2022.\n\n[36] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cHIPPO: Recurrent Memory with Optimal\n\nPolynomial Projections\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2020.\n\n[37] Albert Gu, Karan Goel, and Christopher R\u00e9. \u201cEfficiently Modeling Long Sequences with Structured State Spaces\u201d.\n\nIn: The International Conference on Learning Representations (ICLR). 2022.\n\n[38] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. \u201cImproving the Gating Mechanism\n\nof Recurrent Neural Networks\u201d. In: The International Conference on Machine Learning (ICML). 2020.\n\n[39] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. \u201cOn the Parameterization and Initialization of Diagonal\n\nState Space Models\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2022.\n\n[40] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. \u201cCombining Recurrent,\nConvolutional, and Continuous-time Models with the Linear State Space Layer\u201d. In: Advances in Neural Information\nProcessing Systems (NeurIPS). 2021.\n\n[41] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. \u201cHow to Train Your HIPPO: State Space\nModels with Generalized Basis Projections\u201d. In: The International Conference on Learning Representations (ICLR).\n2023.\n\n[42] Ankit Gupta, Albert Gu, and Jonathan Berant. \u201cDiagonal State Spaces are as Effective as Structured State Spaces\u201d.\n\nIn: Advances in Neural Information Processing Systems 35 (2022), pp. 22982\u201322994.\n\n[43] Ankit Gupta, Harsh Mehta, and Jonathan Berant. \u201cSimplifying and Understanding State Space Models with Diagonal\n\nLinear RNNs\u201d. In: arXiv preprint arXiv:2212.00768 (2022).\n\n[44] David Ha, Andrew Dai, and Quoc V. Le. \u201cHyperNetworks\u201d. In: The International Conference on Learning Representa-\n\ntions (ICLR). 2017.\n\n[45] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. \u201cDream to Control: Learning Behaviors by\n\nLatent Imagination\u201d. In: The International Conference on Learning Representations (ICLR). 2020.\n\n19\n\n\f[46] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. \u201cLiquid\nStructural State-Space Models\u201d. In: The International Conference on Learning Representations (ICLR). 2023.\n[47] Mikael Henaff, Arthur Szlam, and Yann LeCun. \u201cRecurrent Orthogonal Networks and Long-Memory Tasks\u201d. In:\n\nThe International Conference on Machine Learning (ICML). 2016.\n\n[48] Dan Hendrycks and Kevin Gimpel. \u201cGaussian Error Linear Units (GELUs)\u201d. In: arXiv preprint arXiv:1606.08415\n\n[49]\n\n[50]\n\n[51]\n\n[52]\n\n(2016).\nSepp Hochreiter. \u201cUntersuchungen zu dynamischen neuronalen Netzen\u201d. In: Diploma, Technische Universit\u00e4t\nM\u00fcnchen 91.1 (1991), p. 31.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J\u00fcrgen Schmidhuber, et al. Gradient Flow in Recurrent Nets: The\nDifficulty of Learning Long-term Dependencies. 2001.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. \u201cLong Short-Term Memory\u201d. In: Neural Computation 9.8 (1997), pp. 1735\u2013\n1780.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. \u201cAn Empirical Analysis of Compute-\nOptimal Large Language Model Training\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 35 (2022),\npp. 30016\u201330030.\n\n[53] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. \u201cTransformer Quality in Linear Time\u201d. In: The International\n\nConference on Machine Learning (ICML). PMLR. 2022, pp. 9099\u20139117.\n\n[54] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. \u201cDeep\nLearning for Time Series Classification: A Review\u201d. In: Data Mining and Knowledge Discovery 33.4 (2019), pp. 917\u2013\n963.\n\n[55] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. \u201cData Movement is All You Need: A\nCase Study on Optimizing Transformers\u201d. In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711\u2013732.\n[56] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. \u201cGated\n\nOrthogonal Recurrent Units: On Learning to Forget\u201d. In: Neural Computation 31.4 (2019), pp. 765\u2013783.\n\n[57] Rudolph Emil Kalman. \u201cA New Approach to Linear Filtering and Prediction Problems\u201d. In: (1960).\n[58] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. \u201cTransformers are RNNs: Fast\nAutoregressive Transformers with Linear Attention\u201d. In: International Conference on Machine Learning. PMLR. 2020,\npp. 5156\u20135165.\nShiva Kaul. \u201cLinear Dynamical Systems as a Core Computational Primitive\u201d. In: Advances in Neural Information\nProcessing Systems 33 (2020), pp. 16808\u201316820.\n\n[59]\n\n[60] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. \u201cDiffWave: A Versatile Diffusion Model\n\nfor Audio Synthesis\u201d. In: International Conference on Learning Representations. 2021.\n\n[61] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. \u201cTime-Parameterized Convolutional Neural\n\nNetworks for Irregularly Sampled Time Series\u201d. In: arXiv preprint arXiv:2308.03210 (2023).\n\n[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. \u201cImageNet Classification with Deep Convolutional Neural\n\nNetworks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012).\n\n[63] Tao Lei. \u201cWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Compute\u201d. In: Proceedings\n\nof the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633\u20137648.\n\n[64] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. \u201cSimple Recurrent Units for Highly Parallelizable\n\nRecurrence\u201d. In: arXiv preprint arXiv:1709.02755 (2017).\n\n[65] Mario Lezcano-Casado and David Mart\u00ednez-Rubio. \u201cCheap Orthogonal Constraints in Neural Networks: A Simple\nParametrization of the Orthogonal and Unitary Group\u201d. In: The International Conference on Machine Learning\n(ICML). 2019.\n\n[66] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. \u201cWhat Makes Convolutional Models Great\non Long Sequence Modeling?\u201d In: The International Conference on Learning Representations (ICLR). 2023.\n[67] Vasileios Lioutas and Yuhong Guo. \u201cTime-aware Large Kernel Convolutions\u201d. In: The International Conference on\n\nMachine Learning (ICML). PMLR. 2020, pp. 6172\u20136183.\n\n[68] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani.\n\u201cStructured State Space Models for In-Context Reinforcement Learning\u201d. In: Advances in Neural Information Processing\nSystems (NeurIPS). 2023.\nShahar Lutati, Itamar Zimerman, and Lior Wolf. \u201cFocus Your Attention (with Adaptive IIR Filters)\u201d. In: arXiv preprint\narXiv:2305.14952 (2023).\n\n[69]\n\n20\n\n\f[70] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke\nZettlemoyer. \u201cMega: Moving Average Equipped Gated Attention\u201d. In: The International Conference on Learning\nRepresentations (ICLR). 2023.\n\n[71] Eric Martin and Chris Cundy. \u201cParallelizing Linear Recurrent Neural Nets Over Sequence Length\u201d. In: The Interna-\n\n[72]\n\ntional Conference on Learning Representations (ICLR). 2018.\nSoroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and\nYoshua Bengio. \u201cSampleRNN: An Unconditional End-to-End Neural Audio Generation Model\u201d. In: The International\nConference on Learning Representations (ICLR). 2017.\n\n[73] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. \u201cLong Range Language Modeling via Gated\n\nState Spaces\u201d. In: The International Conference on Learning Representations (ICLR). 2023.\n\n[74] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. \u201cEfficient Orthogonal Parametrisation\nof Recurrent Neural Networks using Householder Reflections\u201d. In: International Conference on Machine Learning.\nPMLR. 2017, pp. 2401\u20132409.\n\n[75] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher\nR\u00e9. \u201cS4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces\u201d. In: Advances in Neural\nInformation Processing Systems (NeurIPS). 2022.\n\n[76] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel,\nClayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. \u201cHyenaDNA: Long-range Genomic Sequence Modeling\nat Single Nucleotide Resolution\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2023.\n\n[77] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,\nScott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,\nJared Kaplan, Sam McCandlish, and Chris Olah. \u201cIn-context Learning and Induction Heads\u201d. In: Transformer Circuits\nThread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\n[78] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,\nAndrew Senior, and Koray Kavukcuoglu. \u201cWaveNet: A Generative Model for Raw Audio\u201d. In: arXiv preprint\narXiv:1609.03499 (2016).\n\n[79] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.\n\u201cResurrecting Recurrent Neural Networks for Long Sequences\u201d. In: The International Conference on Machine Learning\n(ICML). 2023.\n\n[80] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle,\nMarco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. \u201cThe LAMBADA Dataset: Word Prediction Requiring a Broad\nDiscourse Context\u201d. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2016,\npp. 1525\u20131534.\n\n[81] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. \u201cOn the Difficulty of Training Recurrent Neural Networks\u201d.\n\nIn: International Conference on Machine Learning. 2013, pp. 1310\u20131318.\n\n[82] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung,\nMatteo Grella, Kranthi Kiran GV, et al. \u201cRWKV: Reinventing RNNs for the Transformer Era\u201d. In: arXiv preprint\narXiv:2305.13048 (2023).\n\n[83] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. \u201cRandom Feature\n\nAttention\u201d. In: The International Conference on Learning Representations (ICLR). 2021.\n\n[84] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,\nand Christopher R\u00e9. \u201cHyena Hierarchy: Towards Larger Convolutional Language Models\u201d. In: The International\nConference on Machine Learning (ICML). 2023.\n\n[85] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and\nYiran Zhong. \u201cToeplitz Neural Network for Sequence Modeling\u201d. In: The International Conference on Learning\nRepresentations (ICLR). 2023.\n\n[86] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. \u201cThe devil in\n\nlinear transformer\u201d. In: arXiv preprint arXiv:2210.10340 (2022).\n\n[87] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran\nZhong. \u201cCosFormer: Rethinking Softmax in Attention\u201d. In: The International Conference on Learning Representations\n(ICLR). 2022.\n\n[88] Ali Rahimi and Benjamin Recht. \u201cRandom Features for Large-Scale Kernel Machines\u201d. In: Advances in Neural\n\nInformation Processing Systems (NeurIPS) 20 (2007).\n\n21\n\n\f[89] Prajit Ramachandran, Barret Zoph, and Quoc V Le. \u201cSwish: A Self-gated Activation Function\u201d. In: arXiv preprint\n\narXiv:1710.05941 7.1 (2017), p. 5.\n\n[90] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. \u201cCKConv: Continuous\n\nKernel Convolution For Sequential Data\u201d. In: arXiv preprint arXiv:2102.02611 (2021).\n\n[91] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. \u201cWinogrande: An Adversarial Winograd\n\nSchema Challenge at Scale\u201d. In: Communications of the ACM 64.9 (2021), pp. 99\u2013106.\n\n[93]\n\n[92] George Saon, Ankit Gupta, and Xiaodong Cui. \u201cDiagonal State Space Augmented Transformers for Speech Recogni-\ntion\u201d. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.\n2023, pp. 1\u20135.\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. \u201cLinear Transformers are Secretly Fast Weight Programmers\u201d.\nIn: The International Conference on Machine Learning (ICML). PMLR. 2021, pp. 9355\u20139366.\nJ\u00fcrgen Schmidhuber. \u201cLearning to control fast-weight memories: An alternative to dynamic recurrent networks\u201d.\nIn: Neural Computation 4.1 (1992), pp. 131\u2013139.\n\n[94]\n\n[95] Noam Shazeer. \u201cGLU Variants Improve Transformer\u201d. In: arXiv preprint arXiv:2002.05202 (2020).\n[96]\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny\nZhou. \u201cLarge Language Models can be Easily Distracted by Irrelevant Context\u201d. In: The International Conference on\nMachine Learning (ICML). PMLR. 2023, pp. 31210\u201331227.\nJiaxin Shi, Ke Alexander Wang, and Emily Fox. \u201cSequence Modeling with Multiresolution Convolutional Memory\u201d.\nIn: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31312\u201331327.\nJimmy TH Smith, Andrew Warrington, and Scott W Linderman. \u201cSimplified State Space Layers for Sequence\nModeling\u201d. In: The International Conference on Learning Representations (ICLR). 2023.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \u201cRoformer: Enhanced Transformer\nwith Rotary Position Embedding\u201d. In: arXiv preprint arXiv:2104.09864 (2021).\n\n[97]\n\n[98]\n\n[99]\n\n[100] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. \u201cRetentive\nnetwork: A successor to transformer for large language models\u201d. In: arXiv preprint arXiv:2307.08621 (2023).\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. \u201cSequence to Sequence Learning with Neural Networks\u201d. In: Advances\nin Neural Information Processing Systems (NeurIPS) 27 (2014).\n\n[101]\n\n[102] Corentin Tallec and Yann Ollivier. \u201cCan Recurrent Neural Networks Warp Time?\u201d In: The International Conference\n\non Learning Representations (ICLR). 2018.\n\n[103] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\nRuder, and Donald Metzler. \u201cLong Range Arena: A Benchmark for Efficient Transformers\u201d. In: International\nConference on Learning Representations (ICLR). 2021.\n\n[104] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. \u201cEfficient Transformers: A Survey\u201d. In: ACM Computing\n\nSurveys 55.6 (2022), pp. 1\u201328.\n\n[105] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste\nRozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. \u201cLlama: Open and Efficient Foundation Language Models\u201d.\nIn: arXiv preprint arXiv:2302.13971 (2023).\n\n[106] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. \u201cAttention Is All You Need\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2017.\n[107] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. \u201cOn Orthogonality and Learning Recurrent\nNetworks with Long Term Dependencies\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 3570\u2013\n3578.\nJue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. \u201cSelective Structured\nState-Spaces for Long-form Video Understanding\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 2023, pp. 6387\u20136397.\n\n[108]\n\n[109] Pete Warden. \u201cSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition\u201d. In: ArXiv abs/1804.03209\n\n[110]\n\n(2018).\nSamuel Williams, Andrew Waterman, and David Patterson. \u201cRoofline: An Insightful Visual Performance Model for\nMulticore Architectures\u201d. In: Communications of the ACM 52.4 (2009), pp. 65\u201376.\n\n[111] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. \u201cCondConv: Conditionally Parameterized Convolu-\ntions for Efficient Inference\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019).\n[112] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. \u201cHellaSwag: Can a Machine Really Finish\nYour Sentence?\u201d In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.\n\n22\n\n\f[113]\n\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.\n\u201cAn Attention Free Transformer\u201d. In: arXiv preprint arXiv:2105.14103 (2021).\n\n[114] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \u201cEffectively Modeling Time\nSeries with Simple Discrete State Spaces\u201d. In: The International Conference on Learning Representations (ICLR). 2023.\n[115] Lin Zheng, Chong Wang, and Lingpeng Kong. \u201cLinear complexity randomized self-attention mechanism\u201d. In:\n\n[116]\n\nInternational Conference on Machine Learning. PMLR. 2022, pp. 27011\u201327041.\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \u201cEfficient Long\nSequence Modeling via State Space Augmented Transformer\u201d. In: arXiv preprint arXiv:2212.08136 (2022).\n\n23\n\n\fA Discussion: Selection Mechanism\n\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It\ncan also be viewed as related to \u201cfast weights\u201d (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs with\nthe mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept\nthat is worth clarifying.\n\nGating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber\n1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism\nfor controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal\nthrough time and causes inputs to interact along the sequence length dimension.\n\nHowever, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction\n(often with an activation function). For example, elementwise multiplicative components of neural network architectures\n(that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta\net al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of RNN\ngating versus the popular usage of multiplicative gating actually have a very different semantic meaning.\n\nHypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller\nneural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose\nrecurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber\n1992).\n\nData-dependence.\nmodel depend on the data (Poli et al. 2023).\n\nSimilar to hypernetworks, data-dependence can refer to any notion where some parameters of the\n\nExample: GLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear layer \ud835\udc66 = \ud835\udc6b\ud835\udc65,\nwhere \ud835\udc6b is a diagonal weight parameter. Now suppose that \ud835\udc6b is itself generated from a linear transformation of \ud835\udc65,\nwith an optional nonlinearity: \ud835\udc6b = \ud835\udf0e (\ud835\udc7e\ud835\udc65). Since it is diagonal, the multiplication becomes an elementwise product:\n\ud835\udc66 = \ud835\udf0e (\ud835\udc7e\ud835\udc65) \u25e6 \ud835\udc65.\nThis is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative\n\u201cbranch\u201d), hypernetworks (since the parameter \ud835\udc6b is generated by another layer), and data-dependent (since \ud835\udc6b depends\non the data \ud835\udc65). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an\nactivation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer.\n\nSelection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating,\nhypernetworks, or data-dependence, so can an enormous range of other constructions\u2014essentially anything with a\nmultiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as\nwell\u2014and we find it uninformative to think of them as such.\n\nInstead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1)\nand also has a deeper history of connections to SSMs through variable (input-dependent) discretization of \u0394 (Funahashi\nand Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term \u201cgating\u201d in favor of selection to\nclarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select\nor ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated\nRNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and\nGuo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention.\n\nB Related Work\n\nWe overview several prior works related to our methods. We mention that some of the most closely related models include\nrecurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV.\n\n24\n\n\fB.1 S4 Variants and Derivatives\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our\nmethod.\n\n\u2022 S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonal\nstructure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a\nconnection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020).\n\n\u2022 DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximat-\n\ning the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022).\n\n\u2022 S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the\nfirst S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state\ndimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to\nMIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO\ndimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the\ncomputation issue, (iii) adding the selection mechanism.\n\nLu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their\nmechanism can be viewed as a particular hard-coded instance of a selection mechanism, where \ud835\udc68 is manually set to 0,\ninstead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically\nto this setting and probe if the model has learned to automatically reset its state on episode boundaries.\n\n\u2022 Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of\nbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization step\nof SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show that\nreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.\n\n\u2022 Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this\nperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally\nand close to LTI.\n\n\u2022 SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox\n2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of\nS4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do\nfast autoregressive inference directly.\n\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually\nstrictly LTI (linear time invariant).\n\nB.2 SSM Architectures\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating\none of the previous SSMs as a black box layer.\n\n\u2022 GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the\nmodel dimension in order to increase the state size, based on the motivation in Section 3.1.\n\n\u2022 Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient\n\nattention approximation.\n\n\u2022 H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is\nthe first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later\narchitectures.\n\n\u2022 Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the\ninput. While sharing the \u201cselection\u201d name, we consider this an architectural modification that is closer to architectural\ngating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\n25\n\n\fCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones\n(indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).\n\n\u2022 RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a\nspecial case where the state dimension is \ud835\udc41 = 1. Although not framed as such, its recurrence can be viewed as a special\ncase of a linear SSM.\n\nIts primary source of improvement is using a linear attention with large head dimension, which can be viewed as another\nmethod to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention\nvariants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.\nRetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention\ninstead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.\n\n\u2022 RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free\nTransformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \u201cWKV\u201d mechanism involves LTI recurrences\nand can be seen as the ratio of two SSMs.\n\nWe also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformer\u2019s\nMHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP\nblocks.\n\nB.3 Relationship to RNNs\nRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state.\nSeveral older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016),\nand simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities.\nBecause of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs,\nand are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:\n\n\u2022 They do not use state expansion (\ud835\udc41 = 1) or selective \ud835\udc69, \ud835\udc6a parameters, both of which are important for performance\n\n(Section 4.6).\n\n\u2022 They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +\ndiscretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and\ninitializations (Section 3.6).\n\nAdditionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;\nHochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The former\ncould be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latter\nwas difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more careful\nparameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson,\nGoel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023)), or direct analysis (Gupta, Mehta, and Berant 2022; Kaul 2020; Orvieto\net al. 2023)).\n\nWe also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Henaff, Szlam,\nand LeCun 2016; Lezcano-Casado and Mart\u00ednez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017) which are\nmotivated by constraining the \ud835\udc68 transition matrix to be orthogonal or unitary, in order to control its eigenvalues and\nprevent the vanishing gradient problem. However, these had other limitations; we believe that these stem from the fact\nthat orthogonal/unitary RNNs are also LTI. For example, they are almost always evaluated on the Copying task which they\ncan solve perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019).\n\nB.4 Linear Attention\nThe Linear Attention (LA) (Katharopoulos et al. 2020) framework is an important result popularizing kernel attention and\nshowing how it relates to recurrent autoregressive models. Many variants have proposed alternative kernels and other\nmodifications. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax\nattention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels (Rahimi and\nRecht 2007). Performer (Choromanski et al. 2021) finds an approximation to the exponential kernel involving only positive\n\n26\n\n\ffeatures, which also allows the softmax normalization term. TransNormer (Qin, Han, W. Sun, D. Li, et al. 2022) showed\nthat the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al.\n2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality.\nLinear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance\nsampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed\nnumerator).\n\nAside from kernel attention, many other variants of efficient attention exist; the survey Tay, Dehghani, Bahri, et al. (2022)\noffers an extensive categorization of many of these.\n\nB.5 Long Context Models\nLong context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences.\nHowever, these are often from a computational standpoint and have not been extensively validated. These include:\n\n\u2022 Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformer\nbackbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result\nis similar to our Induction Heads extrapolation experiment (Table 2).\n\n\u2022 LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100\ud835\udc3e for actual tasks.\n\n\u2022 Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context. How-\never, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality\nimprovements at 1M context are due to context length or due to more data and computation.\n\n\u2022 Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer to\nmodel audio waveforms of length 220 = 1048576, although did not discuss performance tradeoffs when controlling for\ncomputation and model size.\n\nIn contrast, we believe this work presents one of the first approaches to meaningfully demonstrate increasing performance\nwith longer context.\n\nC Mechanics of Selective SSMs\n\nProof of Theorem 1. Consider a selective SSM (Algorithm 2) with \ud835\udc41 = 1, \ud835\udc68 = \u22121, \ud835\udc69 = 1, \ud835\udc60\u0394 = Linear(\ud835\udc65), \ud835\udf0f\u0394 = softplus. The\ncorresponding continuous-time SSM (1) is\n\nwhich is also called a leaky integrator.\nThe discretization step size is\n\n\u210e(\ud835\udc61) = \u2212\u210e(\ud835\udc61) + \ud835\udc65 (\ud835\udc61)\n\n\u0394\ud835\udc61 = \ud835\udf0f\u0394 (Parameter + \ud835\udc60\u0394 (\ud835\udc65\ud835\udc61 ))\n\n= softplus(Parameter + Linear(\ud835\udc65\ud835\udc61 ))\n= softplus(Linear(\ud835\udc65\ud835\udc61 ))\n\nwhere we observe that the parameter can be viewed as a learnable bias and folded into the linear projection.\n\nNow applying the zero-order hold (ZOH) discretization formulas:\n\n\ud835\udc68\ud835\udc61 = exp(\u0394\ud835\udc68) =\n\n1\n1 + exp(Linear(\ud835\udc65\ud835\udc61 ))\n\n= \ud835\udf0e (\u2212Linear(\ud835\udc65\ud835\udc61 ))\n\n= 1 \u2212 \ud835\udf0e (Linear(\ud835\udc65\ud835\udc61 ))\n\n\ud835\udc69\ud835\udc61 = (\u0394\ud835\udc68) \u22121 (exp(\u0394\ud835\udc68) \u2212 \ud835\udc70 ) \u00b7 \u0394\ud835\udc69 = \u2212(exp(\u0394\ud835\udc68) \u2212 \ud835\udc70 ) = 1 \u2212 \ud835\udc68\n\n= \ud835\udf0e (Linear(\ud835\udc65\ud835\udc61 )).\n\n27\n\n\fThus the final discrete recurrence (2a) is\n\n\ud835\udc54\ud835\udc61 = \ud835\udf0e (Linear(\ud835\udc65\ud835\udc61 ))\n\u210e\ud835\udc61 = (1 \u2212 \ud835\udc54\ud835\udc61 )\u210e\ud835\udc61 \u22121 + \ud835\udc54\ud835\udc61\ud835\udc65\ud835\udc61\n\nas desired.\n\n\u25a1\n\nD Hardware-aware Algorithm For Selective SSMs\n\nWithout input-dependent selectivity, SSMs can be efficiently implemented as a convolution (Dao, Fu, Saab, et al. 2023; Gu,\nGoel, and R\u00e9 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity, SSMs are no-longer\nequivalent to convolution, but we leverage the parallel associative scan. While SSM scans are theoretically efficient\n(\ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41 ) FLOPs, scaling linear in \ud835\udc3f), training foundation models with selective SSMs requires them to be efficient on\nmodern hardware (GPUs) as well. We describe how we use kernel fusion and recomputation to make SSM scan fast and\nmemory-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5,\nshowing that it is up to 7\u00d7 times faster than attention at sequence length 32K, and is as memory-efficient as the best\nattention implementation (FlashAttention).\n\nSpeed. On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memory-\nbandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This the case with our\nscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to significant speedup compared to\na standard implementation.\n\nThe standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input \ud835\udc68, \ud835\udc69 of size (\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41 ) in GPU\nHBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation to\nwrite the scan output of size (\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41 ) to GPU HBM, then multiply that scan output with \ud835\udc6a to produce an output of size\n(\ud835\udc35, \ud835\udc3f, \ud835\udc37). However, this requires the number of memory reads/writes on the order of \ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41 ). We can instead fuse the\ndiscretization step, the scan, and the multiplication with \ud835\udc6a into one kernel:\n\n1. We read in \ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc37 + \ud835\udc37\ud835\udc41 ) bytes of memory (\u0394, \ud835\udc68, \ud835\udc69, \ud835\udc6a) from slow HBM to fast SRAM.\n2. We discretize to produce \ud835\udc68, \ud835\udc69 of size (\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41 ) in SRAM.\n3. We perform a parallel associative scan, yielding intermediate states of size (\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41 ) in SRAM.\n\n4. We multiply and sum with \ud835\udc6a, producing outputs of size (\ud835\udc35, \ud835\udc3f, \ud835\udc37) and write it to HBM.\n\nThis way, we reduce IOs by a factor of \ud835\udc42 (\ud835\udc41 ) (the state dimension), which in practice speeds up the operation by 20-40\ntimes (Section 4.5).\n\nFor sequence length \ud835\udc3f too long where we cannot fit the sequence in SRAM (which is much smaller than HBM), we split the\nsequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate scan states, we can\ncontinue the scan with the next chunk.\n\nMemory. We describe how we use the classical technique of recomputation to reduce the total amount of memory\nrequired to train selective SSM layers.\n\nFrom the way we fuse the forward pass, we do not save the intermediate states of size (\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41 ) to avoid memory blowup.\nHowever, these intermediate states are necessary for the backward pass to compute gradients. We instead recompute those\nintermediate states in the backward pass. Since the inputs \u0394, \ud835\udc68, \ud835\udc69, \ud835\udc6a and output gradient read from HBM to SRAM are\nof size \ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc41 + \ud835\udc37\ud835\udc41 ), and the input gradients are also of size \ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc41 + \ud835\udc37\ud835\udc41 ), recomputation avoids the cost of reading\n\ud835\udc42 (\ud835\udc35\ud835\udc3f\ud835\udc41 \ud835\udc37) elements from HBM. This means that recomputation of the SSM states in the backward pass speeds up the\ncomputation compared to storing them and reading them from HBM.\n\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the\nmemory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output projection).\nIn particular, we do not save intermediate activations that take a lot of memory but are fast to recompute (e.g. output of\nactivation function or short convolution). As a result, the selective SSM layer has the same memory requirement as an\n\n28\n\n\fTable 11: (Induction heads.) Models are trained on sequence length 28 = 256, and tested on various sequence lengths of 26 = 64 up to\n220 = 1048576. \u2713 denotes perfect generalization accuracy, while \u2717 denotes out of memory.\n\nModel\n\nParams\n\nTest Accuracy (%) at Seqence Length\n\nMHA-Abs\nMHA-RoPE\nMHA-xPos\nH3\nHyena\nMamba\n\n137K\n137K\n137K\n153K\n69M\u2217\n74K\n\n26\n\n27\n\n\u2713\n99.6\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n97.7 \u2713\n\u2713\n\u2713\n\n29\n\n28\n100.0\n100.0\n100.0\n100.0\n100.0 \u2713\n100.0 \u2713\n\n58.6\n83.6\n99.6\n80.9\n\n210\n\n26.6\n31.3\n67.6\n39.5\n44.1\n\u2713\n\n211\n\n18.8\n18.4\n25.4\n23.8\n12.5\n\u2713\n\n212\n\n9.8\n8.6\n7.0\n14.8\n6.6\n\u2713\n\n213\n\n10.9\n9.0\n9.0\n8.2\n5.1\n\u2713\n\n214\n\n7.8\n5.5\n7.8\n5.9\n7.0\n\u2713\n\n215\n\n\u2717\n\u2717\n\u2717\n6.6\n5.9\n\u2713\n\n216\n\n\u2717\n\u2717\n\u2717\n8.2\n6.6\n\u2713\n\n217\n\n\u2717\n\u2717\n\u2717\n4.7\n6.6\n\u2713\n\n218\n\n\u2717\n\u2717\n\u2717\n8.2\n5.9\n\u2713\n\n219\n\n\u2717\n\u2717\n\u2717\n6.3\n6.3\n\u2713\n\n220\n\n\u2717\n\u2717\n\u2717\n7.4\n9.8\n\u2713\n\n\u2217 Most of the parameters are in learnable positional encodings.\n\noptimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) stores\naround 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of\n32 bytes ((assuming mixed-precision training in FP16 or BF16)). Each selective SSM stores around 16 bytes of activations\nper token. Hence two layers of selective SSMs have around the same activation memory as an attention layer and an MLP\nlayer.\n\nE Experimental Details and Additional Results\n\nE.1 Synthetic Tasks\nSelective Copying. Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including the\nwhite \u201cnoise\u201d token from Figure 2) and requiring models to memorize 16 \u201cdata\u201d tokens. We use 2 layer models with a model\ndimension of \ud835\udc37 = 64.\n\nModels are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64.\n\nInduction Heads. Training consists of randomly generating data every step, with a batch size of 8. We choose an\n\u201cepoch\u201d size of 8192 steps, and track the accuracy on fixed validation sets (also randomly generated) of each target sequence\nlength. For the MHA-Abs and Mamba models, results are reported after the 25th epoch (8192 \u00d7 25 = 204800 steps). For the\nMHA-RoPE and MHA-xPos models, results are reported after the 50th epoch (8192 \u00d7 50 = 409600 steps). For the LTI H3\nand Hyena models, results are reported after the 10th epoch (81920 steps) because they had converged by then and failed\nto improve further.\n\nWe use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2\ud835\udc52 \u2212 4 and 1\ud835\udc52 \u2212 3, and\nthe better results are reported for each model (2\ud835\udc52 \u2212 4 for all models except Mamba). The attention and Hyena models did\nnot learn at LR 1\ud835\udc52 \u2212 3. H3 learned at both LRs, but interestingly generalized better to shorter sequences at the smaller LR of\n2\ud835\udc52 \u2212 4. Mamba learned at both LRs, but extrapolated better at the larger LR of 1\ud835\udc52 \u2212 3.\n\nE.2 Language Modeling\n\nE.2.1 Scaling Law Details\n\nScaling law experiments generally followed the GPT3 recipe. All models were trained on the Pile with the GPT2 tok-\nenizer.\n\nModel Sizes. Table 12 specifies the model sizes we use for scaling laws. This is taken directly from the GPT3 specifi-\ncations (Brown et al. 2020), with very minor modifications. First, we changed the batch size of the 1.3B model from 1M\ntokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed the\nnumber of training steps and total tokens to roughly match Chinchilla scaling laws (Hoffmann et al. 2022), which specify\nthat training tokens should increase proportionally to model size.\n\nTraining Recipes. All models used the AdamW optimizer with\n\n29\n\n\fTable 12: (Scaling Law Model Sizes.) Our model sizes and hyperparameters for scaling experiments. (Model dimension and number of\nheads applies only to Transformer models.)\n\nParams\n\nn_layers\n\nd_model\n\nn_heads / d_head\n\nTraining steps\n\nLearning Rate\n\nBatch Size\n\nTokens\n\n125M\n350M\n760M\n1.3B\n\n12\n24\n24\n24\n\n768\n1024\n1536\n2048\n\n12 / 64\n16 / 64\n16 / 96\n32 / 64\n\n4800\n13500\n29000\n50000\n\n6e-4\n3e-4\n2.5e-4\n2e-4\n\n0.5M tokens\n0.5M tokens\n0.5M tokens\n0.5M tokens\n\n2.5B\n7B\n15B\n26B\n\n\u2022 gradient clip value 1.0\n\n\u2022 weight decay 0.1\n\n\u2022 no dropout\n\n\u2022 linear learning rate warmup with cosine decay\n\nBy default, the peak learning rate is the GPT3 specification.\n\nWe give several models an \u201cimproved recipe\u201d, inspired by changes adopted by popular large language models such as\nPaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include:\n\n\u2022 linear learning rate warmup with cosine decay to 1\ud835\udc52 \u2212 5, with a peak value of 5\u00d7 the GPT3 value\n\n\u2022 no linear bias terms\n\n\u2022 RMSNorm instead of LayerNorm\n\n\u2022 AdamW hyperparameter \ud835\udefd = (.9, .95) (the GPT3 value) instead of the PyTorch default of \ud835\udefd = (.9, .999)\n\nArchitecture and Training Details. Our models are:\n\u2022 Transformer: The standard Transformer based on GPT3 (Table 12).\n\u2022 Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) and\n\nSwiGLU MLP (Shazeer 2020), and the improved training recipe above.\n\n\u2022 Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with\nstandard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly\nincreased by 1.5\u00d7 to preserve parameter count.\n\n\u2022 H3++: The H3 architecture with a few modifications, including (i) using the same \u201cthin\u201d Hyena dimensions above (ii) the\n\nimproved training recipe above (iii) a linear attention head dimension of 8.\n\n\u2022 RWKV: The default RWKV model from B. Peng et al. (2023), including its modified MLP block. We also used as much of\n\nits specified training recipe as possible, such as increasing the learning rates by 2\u00d7 or 3\u00d7 on certain parameters.\n\n\u2022 RetNet: The default RetNet model from Y. Sun et al. (2023). We also gave it the improved training recipe above.\n\u2022 Mamba: The standard Mamba architecture, with the improved training recipe.\n\nE.2.2 Additional Scaling Law Ablations\n\nWe perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in\nFigure 4 (Left).\n\nMamba Architecture: Interleaving Blocks. We test the effect of different architectural blocks combined with the\nMamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra\nconv \u2192 SSM path added. This leads to two natural ablations:\n\u2022 What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be\n\ninterpreted as taking Mamba and removing half of the SSMs.\n\n30\n\n\fFigure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead of\n\n\u2022 What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking\n\na Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.\n\nFigure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither\nchange matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models except\nTransformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact\nthat many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao,\nFu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022).\n\nH3 Architecture: Training Recipes. Next we ablate differences between the Hyena and H3++ models, our weakest\nand strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.\n\n\u2022 Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).\n\u2022 Hyena+: The same architecture but with the improved training recipe described above.\n\u2022 H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.\n\u2022 H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside the SSM\n\nrecurrence but does not increase parameters.\n\nOur general convention is that \u201cModel+\u201d represents the base model with the improved training recipe, and \u201cModel++\u201d also\nallows for architectural changes.\n\nFigure 9 (Right) shows that\n\u2022 A large improvement is achieved by the improved training recipe, which was used for many of the models in the main\n\nFigure 4 (RetNet, H3++, Transformer++, Mamba).\n\n\u2022 The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.\n\n\u2022 The head dimension expansion improves performance, consistent with one of our main themes that expanded state\n\ndimension improves performance for SSMs (Section 3).\n\nE.2.3 Downstream Evaluation Details\n\nThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens and with the GPT-NeoX\ntokenizer (Black et al. 2022) instead of GPT2 tokenizer. For the 1.3B model, we use a batch size of 1M tokens to be consistent\nwith the GPT3 specifications. We report the perplexity on the Pile validation set, and for this metric only compare to\nmodels trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV.\n\nFor downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021), as done by most\nwork in this area. We evaluate on the following tasks/datasets that measure common sense reasoning:\n\n\u2022 LAMBADA (Paperno et al. 2016)\n\n\u2022 HellaSwag (Zellers et al. 2019)\n\n31\n\n\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u001d\u0000\u0015\u0000\u0014\u0000\u0016\u0000\u0014\u0000*\u00000\u00003\u00004\u0000W\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u001b\u0000\u0082\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u001c\u0000\u0082\u0000\u0015\u0000\u0014\u0000\u0014\u0000\u001d\u0000\u0082\u0000\u0015\u0000\u0014\u0000\u0014\u00004\u0000I\u0000V\u0000T\u0000P\u0000I\u0000\\\u0000M\u0000X\u0000]\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u00007\u0000G\u0000E\u0000P\u0000M\u0000R\u0000K\u0000\u0004\u00000\u0000E\u0000[\u0000W\u0000\u0004\u0000S\u0000R\u0000\u0004\u00008\u0000L\u0000I\u0000\u0004\u00004\u0000M\u0000P\u0000I\u0000\u0004\u0000\f\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0004\u0000\u0016\u0000\u0014\u0000\u0018\u0000\u001c\u0000\r\u00001\u0000E\u0000Q\u0000F\u0000E\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0011\u00001\u00000\u00004\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0011\u00001\u0000,\u0000%\u0000\u0015\u0000\u0014\u0000\u0015\u0000\u001d\u0000\u0015\u0000\u0014\u0000\u0016\u0000\u0014\u0000*\u00000\u00003\u00004\u0000W\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u0000\u0015\u0000\u0014\u0000\u0015\u00004\u0000I\u0000V\u0000T\u0000P\u0000I\u0000\\\u0000M\u0000X\u0000]\u0000\u0004\u0000\f\u0000P\u0000S\u0000K\u0000\u0004\u0000W\u0000G\u0000E\u0000P\u0000I\u0000\r\u00007\u0000G\u0000E\u0000P\u0000M\u0000R\u0000K\u0000\u0004\u00000\u0000E\u0000[\u0000W\u0000\u0004\u0000S\u0000R\u0000\u0004\u00008\u0000L\u0000I\u0000\u0004\u00004\u0000M\u0000P\u0000I\u0000\u0004\u0000\f\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0004\u0000\u0016\u0000\u0014\u0000\u0018\u0000\u001c\u0000\r\u0000,\u0000]\u0000I\u0000R\u0000E\u0000,\u0000]\u0000I\u0000R\u0000E\u0000\u000f\u0000,\u0000\u0017\u0000\u000f\u0000,\u0000\u0017\u0000\u000f\u0000\u000f\f\u2022 PIQA (Bisk et al. 2020)\n\n\u2022 ARC-challenge (P. Clark et al. 2018)\n\n\u2022 ARC-easy: an easy subset of ARC-challenge\n\n\u2022 WinoGrande (Sakaguchi et al. 2021)\n\nWe report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for\nHellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task).\n\nE.3 DNA Modeling\n\nE.3.1 Pretraining Details\n\nWe describe the dataset and training procedure of the HG38 pretraining task in more detail.\n\nThe dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains a\ntotal of \ud835\udc46 = 34021 segments of length 217 = 131072 that cover the genome, for a total of approximately 4.5 billion tokens\n(DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if\nnecessary (e.g. to get longer segments).\n\nWe deviate from HyenaDNA when the training sequence length is not 217. HyenaDNA always takes a fixed sub-segment\n(e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is fixed\nto 34021 samples and doesn\u2019t necessarily go through the whole genome. On the other hand, we use the entire training\ndata:\n\n\u2022 When the context length \ud835\udc3f is less than (or equal to) 217, we divide up each segment into non-overlapping sub-segments\n\nof length \ud835\udc3f, so that there are \ud835\udc46 \u00d7 217\n\n\ud835\udc3f total samples and \ud835\udc46 \u00d7 217 \u2248 4.5\ud835\udc35 tokens per epoch.\n\n\u2022 When the context length \ud835\udc3f is greater than 217, we turn each segment into two samples, one that begins with the prescribed\nsegment and one that ends with the prescribed segment. Thus each epoch has 2\ud835\udc46 items and 2\ud835\udc46\ud835\udc3f tokens per epoch. For\nexample, at sequence length 218 = 262144 there are 4\u00d7 as many tokens as the default, and at sequence length 220 there\nare 16\u00d7 as many tokens.\n\nOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2). For\nexample, we use the AdamW with (\ud835\udefd1, \ud835\udefd2) = (0.9, 0.95), no dropout, weight decay 0.1. We use a cosine learning rate\nscheduler with linear warmup for 10% of total steps.\n\nE.3.2 Scaling: Model Size Details\nModels. The models we consider are:\n\u2022 Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al.\n2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017).\n\n\u2022 HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with\n\nthe MHA block replaced by an H3 block using a global convolution parameterized by an MLP.\n\n\u2022 Mamba: the standard Mamba architecture.\n\nModel Sizes. We use the following model sizes.\n\n4\nBlocks\nModel Dimension 64\nParams (Approx.)\n\n5\n96\n\n6\n128\n\n7\n192\n\n8\n256\n\n10\n384\n\n12\n512\n\n250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M\n\nNote that the number of blocks for Mamba is doubled, because one Transformer \u201clayer\u201d includes both the MHA and MLP\nblocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).\n\n32\n\n\fFor each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1\ud835\udc52 \u2212 3, 2\ud835\udc52 \u2212 3, 4\ud835\udc52 \u2212\nTraining.\n3, 8\ud835\udc52 \u2212 3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learning\nrate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stable\nand improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possible\nthat our results are still suboptimal.)\n\nNote that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. The\noptimal LR should go down for larger models, but we didn\u2019t find a noticeable effect at the small model sizes (at most a few\nmillion parameters) we considered.\n\nE.3.3 Scaling: Context Length Details\nWe use a total batch size of 224 \u2248 16\ud835\udc40 tokens per training step, for every sequence length (e.g. at length 220 there are\n16 segments per batch and at length 210 there are 16384 segments per batch). This is a large batch size relative to the\nmodel size by usual LM standards, but note that a batch size of 223 is the minimum possible on a machine with 8 GPUs and\nsequence length of 220, and that HyenaDNA used much larger batches of 228.\n\nThe learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rate\nof 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length.\n\nFollowing (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW) during\nSequence Length Warmup.\npretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 210 = 1024.\n(Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally.\nIn particular, each stage up to length 217 processes the same number of tokens, but 4\u00d7 as many tokens are processed at\nlength 218, 8\u00d7 as many at length 219, and 16\u00d7 as many at length 220.)\n\nUnlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively\nhalved as the sequence lengths are doubled in each stage.\n\nRemark E.1. We also note that the schedule was not tuned, and we never experimented with turning off sequence length\nwarmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similar\nlengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.\n\nE.3.4 Species (Great Apes) Classification\n\nModels are causal and therefore only the last element (across the sequence length) of the model\u2019s output is used for the\nclassification head. Note that we control for the total number of elements in the loss function per gradient step. The\npretraining objective includes all positions across the sequence length, so that batch_size \u00d7 sequence_length is held\nconstant; in other words, the batch size decreases as the sequence length increases. However, for a classification task, since\nonly the last position enters the loss, the batch size itself is held constant. Note that this also means that fine-tuning models\nwith longer sequence lengths is more computationally expensive.\n\nTraining consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which are all\nindependently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then uniformly\npicking a contiguous segment of DNA.\n\nFollowing (Nguyen, Poli, et al. 2023), models with a maximum context length greater than 214 = 16384 use sequence length\nwarmup with 1 epoch at length 214 = 16384, 1 epoch at length 215 = 32768, 1 epoch at length 216 = 65536, and so on up to\nthe maximum sequence length. For example, the model with 220 = 1048576 context undergoes 6 epochs of sequence length\nwarmup before 4 more epochs at its maximum sequence length.\n\nThe learning rate for all Hyena models is 4e \u2212 5, while the learning rate for all Mamba models is 1e \u2212 4. These were found\nby performing learning rate sweeps for each model among {1\ud835\udc52 \u2212 5, 2\ud835\udc52 \u2212 5, 4\ud835\udc52 \u2212 5, 1\ud835\udc52 \u2212 4, 2\ud835\udc52 \u2212 4} for the smaller sequence\nlengths (210, 212, 214, 216), and these values were consistently found to be the best for each model. An abridged learning rate\nsweep was done at length 218, which agreed with these values, and a single run at length 220 was performed (as described\nabove, the computational cost of these experiments is proportional to the sequence length). The learning rate followed\na cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate, and 5 epochs of\ncosine decay down to 1\ud835\udc52 \u2212 6. The unusually long learning rate warmup schedule was chosen because the sequence length\n\n33\n\n\fTable 13: (Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 using\npretrained models of the same context length. Random guessing is 20%.\n\nModel\n\nParams\n\nAccuracy (%) at Seqence Length\n\n210\n\n212\n\n214\n\n216\n\n218\n\n220\n\nHyenaDNA 1.4M\n1.4M\nMamba\n\n28.04\n31.47\n\n28.43\n27.50\n\n41.17\n27.66\n\n42.22\n40.72\n\n31.10\n42.41\n\nMamba\n\n7M\n\n30.00\n\n29.01\n\n31.48\n\n43.73\n\n56.60\n\n54.87\n71.67\n\n81.31\n\nTable 14: YouTubeMix length scaling sequence lengths and batch sizes.\n\nSeqence length Batch size\n\nTokens / batch\n\n468 \u00d7 2048 = 958464\n234 \u00d7 2048 = 479232\n117 \u00d7 2048 = 239616\n59 \u00d7 2048 = 120832\n30 \u00d7 2048 = 61440\n15 \u00d7 2048 = 30720\n8 \u00d7 2048 = 16384\n4 \u00d7 2048 = 8192\n\n1\n2\n4\n8\n16\n32\n64\n128\n\n958464\n958464\n958464\n966656\n983040\n983040\n1048576\n1048576\n\nwarmup was also long (e.g. comprising 6 out of 10 epochs for the model with context length 220); we did not experiment\nwith this choice.\n\nResults for the Species classification task are in Table 13.\n\nE.4 Audio Details\n\nE.4.1 YouTubeMix Audio Pretraining\nModel. We use a model with 3 blocks per stage (3 \u00d7 5 = 15 total Mamba blocks), pooling factor \ud835\udc5d = 16, and outer\ndimension \ud835\udc37 = 64, for about 3.5M parameters.\n\nDataset. The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of 256.\nThe dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into segments of any\ndesired sequence length. Since the architecture involves two stages of pooling by a factor of 16, and we want the resulting\nsequence length to be a a multiple of 8 for hardware efficiency, the longest possible sequence is 468 \u00d7 2048 = 958464. The\nrest of our sequence lengths are defined by successively halving this and rounding up to the nearest multiple of 2048.\n\nTable 14 lists the specifications used in Figure 7. Beyond the varying batch sizes, the number of valid segments in the\ntraining set varied between different sequence lengths (e.g. the number of training steps per epoch was not constant for\ndifferent points in the graph), which may have contributed to kinks in the scaling curves.\n\nTraining. Models were trained for 200\ud835\udc3e training steps with a maximum learning rate of 0.002, 20\ud835\udc3e (10%) warmup steps,\nand weight decay 0.1 (similar to our general pretraining recipe across domains).\n\nAdditional Ablations: SSM Parameterizations. We investigate SSM parameterizations on long-form audio waveform\npretraining in the setting of Figure 7. The setting is modified slightly to use larger models (8 layers and \ud835\udc37 = 64 for 6M\nparams, the SaShiMi default), shorter sequences (211 = 2048 to 218 = 262144 instead of 213 to 220), lower LR (0.001 from\n0.002), and shorter training cycles (100K instead of 200K steps).\n\nFigure 10 shows that the change from S4 \u2192 S6 (i.e. the selection mechanism) is not always beneficial. On long-form\naudio waveforms, it in fact significantly hampers performance, which may be intuitive from the point of view that audio\n\n34\n\n\fFigure 10: (Audio Pretraining (YouTubeMix) Ablations.) As a uniformly-sampled \u201ccontinuous\u201d signal modality, audio waveforms actu-\nally benefit from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same parameterization)\n(Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left.\n\nis uniformly sampled and very smooth, and therefore benefits from continuous linear time-invariant (LTI) methods.\nAfter ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To\ndisambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6.\n\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The\nperformance differences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio signal should\nbe LTI, but once they are \u201ctokenized\u201d and compressed by the outer layers, the inner layers no longer need to be LTI. In this\nsetting however, the real-valued SSM still underperforms the complex-valued one.\n\nE.4.2 SC09 Speech Generation\n\nAutoregressive training largely followed the autoregressive language modeling protocol, such as\n\n\u2022 Weight decay 0.1\n\n\u2022 Learning rate warmup for 10% of total steps\n\n\u2022 AdamW optimizer with \ud835\udefd = (0.9, 0.95)\n\n\u2022 Gradient clip value 0.1\n\nWe used a learning rate of 0.002 and 200000 training steps at a batch size of 16.\n\nThe large Mamba model in Table 4 has 15 layers per stage with an outer dimension of \ud835\udc37 = 96 and pooling factor 4. We\nnote that this dataset is small (training went through 100 epochs) and for this large model, there was significant overfitting\nof the BPB or NLL. However, automated metrics of generated samples continually improving throughout training.\n\nThe models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 and\npooling factor 4. The S4+MLP block has roughly 2\ud835\udc37 2 + 4\ud835\udc37 2 parameters (expansion factor 2 in the MLP). The Transformer\nblock has 4\ud835\udc37 2 + 2\ud835\udc37 2 parameters (expansion factor 1 in the MLP). The Mamba block has the usual \u2248 6\ud835\udc37 2 parameters. All\nmodels have roughly 6M total parameters.\n\nE.5 Efficiency Benchmark\nScan Operation. We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against\nconvolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations\noutside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the\nQKV projections in attention.\n\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the\nparameters \ud835\udc68, \ud835\udc69, \ud835\udc6a in HBM.\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large\nparameters in HBM.\n\n35\n\n\u0000\u0015\u0000\u0014\u0000\u0018\u0000\u0015\u0000\u0014\u0000\u0019\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0015\u0000\u0012\u0000\u0016\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0019\u0000\u0014\u0000&\u0000M\u0000X\u0000W\u0000\u0004\u00004\u0000I\u0000V\u0000\u0004\u0000&\u0000]\u0000X\u0000I\u0000%\u0000Y\u0000H\u0000M\u0000S\u0000\u0004\u0000;\u0000E\u0000Z\u0000I\u0000J\u0000S\u0000V\u0000Q\u0000W\u0000\u0004\u0000\u0011\u0000\u0004\u00007\u00007\u00001\u0000\u0004\u00004\u0000E\u0000V\u0000E\u0000Q\u0000I\u0000X\u0000I\u0000V\u0000M\u0000^\u0000E\u0000X\u0000M\u0000S\u0000R\u00007\u0000\u0018\u0000\u000f\u00001\u00000\u00004\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\f\u00007\u0000\u001a\u0000\r\u0000\u000f\u0000G\u0000S\u0000Q\u0000T\u0000P\u0000I\u0000\\\u0000\u0011\u0000W\u0000I\u0000P\u0000I\u0000G\u0000X\u0000M\u0000Z\u0000I\u0000\u0004\u0000&\u0000\u0013\u0000'\u0000\u0011\u0000W\u0000I\u0000P\u0000I\u0000G\u0000X\u0000M\u0000Z\u0000I\u0000\u0004\u0000\u0004\u0000\u0004\u0000\u0004\u0000\f\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0011\u00007\u0000\u0018\u0000\r\u0000\u0015\u0000\u0014\u0000\u0018\u0000\u0015\u0000\u0014\u0000\u0019\u00007\u0000I\u0000U\u0000Y\u0000I\u0000R\u0000G\u0000I\u0000\u0004\u00000\u0000I\u0000R\u0000K\u0000X\u0000L\u0000\u0015\u0000\u0012\u0000\u0016\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0017\u0000\u0019\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u0014\u0000\u0015\u0000\u0012\u0000\u0018\u0000\u0019\u0000&\u0000M\u0000X\u0000W\u0000\u0004\u00004\u0000I\u0000V\u0000\u0004\u0000&\u0000]\u0000X\u0000I\u0000%\u0000Y\u0000H\u0000M\u0000S\u0000\u0004\u0000;\u0000E\u0000Z\u0000I\u0000J\u0000S\u0000V\u0000Q\u0000W\u0000\u0004\u0000\u0011\u0000\u0004\u00007\u00007\u00001\u0000\u0004\u00004\u0000E\u0000V\u0000E\u0000Q\u0000I\u0000X\u0000I\u0000V\u0000M\u0000^\u0000E\u0000X\u0000M\u0000S\u0000R\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0004\u0000\f\u00007\u0000\u001a\u0000\r\u0000\u000f\u0000G\u0000S\u0000Q\u0000T\u0000P\u0000I\u0000\\\u0000\u0011\u0000W\u0000I\u0000P\u0000I\u0000G\u0000X\u0000M\u0000Z\u0000I\u0000\u0004\u0000&\u0000\u0013\u0000'\u0000\u0011\u0000W\u0000I\u0000P\u0000I\u0000G\u0000X\u0000M\u0000Z\u0000I\u0000\u0004\u0000\u0004\u0000\u0004\u0000\u0004\u0000\f\u00001\u0000E\u0000Q\u0000F\u0000E\u0000\u0011\u00007\u0000\u0018\u0000\r\fTable 15: (Memory benchmark.) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\n\nBatch size\n\nTransformer (w/ FlashAttention-2) Mamba\n\n1\n2\n4\n8\n16\n32\n\n4.6GB\n5.2GB\n6.9GB\n11.5GB\n20.7GB\n34.5GB\n\n4.8GB\n5.8GB\n7.3GB\n12.3GB\n23.1GB\n38.2GB\n\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the\nfilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity\nis \ud835\udc42 (\ud835\udc3f log(\ud835\udc3f)) for sequence length \ud835\udc3f.\n\nFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2024)), with causal\nmask. Note that FlashAttention-2 with causal mask is about 1.7\u00d7 faster than without causal mask, since approximately\nonly half of the attention entries are computed.\n\nWe use batch size of 1 and increase the sequence length from 29 = 512, 210 \u2248 1\ud835\udc3e, 211 \u2248 2\ud835\udc3e, up to 219 \u2248 500\ud835\udc3e (some of the\nbaselines run out of memory before reaching 500K). We use a model dimension of \ud835\udc37 = 1024 and state dimension \ud835\udc41 = 16.\nWe measure with BF16 inputs, which is the data type most commonly used for large scale training.\n\nEnd-to-end Inference. We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba 6.9B\nmodel, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard Transformer\nimplementation in the Huggingface transformers library.\n\nWe set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,\n32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s) as\nbatch size \u00d7 128/time taken. We repeat the measurements 3 times and take the average. Measurements are done on an\nA100 80GB PCIe GPU.\n\nMemory Benchmark. The memory usage simply scales proportionally to the size of the activation tensors, as with\nmost deep sequence models. We report measurements of the training memory requirements of 125M models on 1 A100\n80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-efficient Transformer\nimplementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2). Table 15 shows that\nMamba\u2019s memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation,\nand we expect further improvement in Mamba\u2019s memory footprint in the future.\n\n36\n\n\f"
}