{
  "id": "2511.17446",
  "content": "5\n2\n0\n2\n\nv\no\nN\n1\n2\n\n]\n\nG\nL\n.\ns\nc\n[\n\n1\nv\n6\n4\n4\n7\n1\n.\n1\n1\n5\n2\n:\nv\ni\nX\nr\na\n\nUnmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass\nSpectrometry\u22c6\n\nKyle M. Regana,\u2217, Michael McLoughlinc, Wayne A. Brydenc, Gonzalo R. Arceb\n\naCenter for Bioinformatics and Computational Biology, University of Delaware, Newark, 19713, Delaware, United States\nbDepartment of Electrical and Computer Engineering, University of Delaware, Newark, 19716, Delaware, United States\ncZeteo Tech Inc., Sykesville, 21784, Maryland, United States\n\nAbstract\nMatrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is essential for biomolecular analysis\nin human health protection, offering precise identification of pathogens through unique mass spectral signatures to\nsupport environmental monitoring and disease prevention. However, traditional MALDI-MS relies on labor-intensive\nsample preparation and multi-shot spectral averaging, confining it to laboratory settings and hindering its application\nin dynamic, real-world settings critical for public health surveillance. These limitations are amplified in emerging\nportable aerosol MALDI-MS systems, where autonomous sampling produces noisy, single-shot spectra from a mix-\nture of aerosol analytes, demanding novel computational detection methods to safeguard against infectious diseases.\nTo address this, we introduce the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a computational\nframework that processes raw, minimally prepared mass spectral data for accurate multi-label classification, directly\nenhancing real-time pathogen monitoring. Utilizing a transformer architecture to model long-range dependencies\nin spectral time-series, MS-DGFormer incorporates a novel dictionary encoder with Singular Value Decomposition\n(SVD)-derived denoised side information, empowering the model to extract vital biomolecular patterns from noisy\nsingle-shot spectra with high reliability. This approach achieves robust spectral identification in aerosol samples,\nsupporting autonomous, real-time analysis in field-deployable systems. By reducing preprocessing requirements,\nMS-DGFormer facilitates portable MALDI-MS deployment in high-risk areas like public spaces, transforming\nhuman health strategies through proactive environmental monitoring, early detection of biological threats, and rapid\nresponse to mitigate disease spread.\n\nPreprint Notice. This manuscript is a preprint and has been submitted to *Computer in Biology and Medicine*.\n\nKeywords: Pathogen Detection, Mass Spectrometry, Machine Learning, Bioaerosols, Public Health,\n\n1. Introduction\n\nMatrix Assisted Laser Desorption/Ionization Mass\nSpectrometry (MALDI-MS) has evolved to be a power-\nful tool to characterize and identify large biomolecules.\nMALDI-MS is a \u201csoft ionization\u201d method and typically\nutilizes a light absorbing matrix that, when mixed with\nan analytical sample and illuminated with a pulsed laser\n\n\u22c6This preprint has been submitted to Computer in Biology and\n\nMedicine.\n\n\u2217Corresponding author\nEmail addresses: regank@udel.edu (Kyle M. Regan ),\n\nmike.mcloughlin@zeteotech.com (Michael McLoughlin),\nwayne.bryden@zeteotech.com (Wayne A. Bryden),\narce@udel.edu (Gonzalo R. Arce)\n\nlight, will create ions from biomolecules to include pro-\nteins, peptides, and lipids [1]. For biodefense applica-\ntions, a major advantage of MALDI is that it can be\ncombined with Time-of-Flight analyzers [2], which do\nnot require a complex fluidic system and can be minia-\nturized for field applications [3]. To achieve high mass\nresolution and accuracy, multiple methods such as de-\nlayed extraction and ion reflectors have been developed\nto compensate for energy spread during the ionization\nprocess [4]. Because MALDI-MS uses a sample de-\nposited onto a substrate, deconvolution of a mixed sam-\nple can be quite complex and has motivated the devel-\nopment of single particle methods [5], [6]. A portable\nprototype MALDI-MS, introduced in \u201cdigitalMALDI:\n\n \n \n \n \n \n \n\fA Single-Particle\u2013Based Mass Spectrometric Detection\nSystem for Biomolecules\u201d [7] demonstrated the ability\nto produce spectra from environmental aerosol parti-\ncles. The spectrometer autonomously samples aerosol\nparticles, irradiates each by an ultraviolet laser creating\nions from individual particles, then analyzes the ions\nby time-of-flight. The portability of this spectrometer\nenables in-field measurements for rapid identification\nof possible biological threats such as airborne bacteria,\nfungi, viruses, toxins, or nonvolatile chemicals.\n\nThis research is driven by the urgent need to pro-\ntect communities from natural and engineered biolog-\nical threats, such as infectious outbreaks or bioterror-\nism. By enabling rapid detection of airborne pathogens,\nour work could curb disease spread and enhance pub-\nlic safety. Envision deploying compact devices in high-\ntraffic areas like airports, transit systems, or stadiums\nfor continuous, real-time scanning, to transform how we\nsafeguard against environmental biological risks.\n\nAt the heart of this approach lies the near autonomous\nsampling of atmospheric *aerosols, a technique that en-\nables continuous monitoring without relying on exten-\nsive human intervention. Yet, creating a threat detection\nsystem that is robust, accurate, and precise is not easy.\nFalse positives risk sparking unwarranted alarm, while\nfalse negatives could allow a silent spread of infec-\ntion. Unlike controlled laboratory conditions with care-\nfully prepared samples, environmental sampling cap-\ntures a complex mixture of aerosol particles\u2014primarily\nharmless background particles\u2014making it challenging\nto identify the unique mass spectra of pathogens. This\nwork tackles these challenges, laying the groundwork\nfor a reliable system capable of safeguarding society\nfrom biological threats.\n\nMALDI-MS is already FDA-cleared for identifying\nbacterial and fungal isolates in clinical labs, capable\nof distinguishing over 10,000 strains. While extensive\nresearch applies MALDI-MS to biological specimens,\nmost focus on meticulously prepared lab samples re-\nquiring days of culturing or separation. Even then,\nsingle-laser-shot spectra are noisy, so multi-shot aver-\naging is standard to boost signal-to-noise ratio before\ndatabase matching.\n\nThis works well in clinics where shots target the same\nanalyte, but environmental sampling involves diverse\nparticles per shot. Averaging can blur features, as shown\nin Fig. 1, which depicts a batch of spectra from 80 dust\naerosols mixed with five spectra each from Bacillus glo-\nbigii, E. coli, Insulin, and Ubiquitin. Averaging as-\nsumes uniform analytes [8][9], but mixtures yield mud-\ndled spectra (Fig. 1A). A single-shot method is essential\nto isolate individual analyte spectra (Fig. 1B-E).\n\nTo enhance single-shot analysis, we leverage the low-\nrank structure of spectra using Singular Value Decom-\nposition (SVD), a mathematical technique for denois-\ning data by identifying dominant patterns. For a noise-\nless spectrum z \u2208 Rl and noisy version s = z + \u03f5, we\nform a matrix S \u2208 Rn\u00d7l from n spectra of the same\nclass. SVD decomposes S = U\u03a3VT, where U \u2208 Rn\u00d7n\nand VT \u2208 Rl\u00d7l are orthogonal matrices and \u03a3 \u2208 Rn\u00d7l\nis a diagonal matrix containing the singular values in\ndescending order. The matrix S has a low-rank struc-\nture, with r \u226a min (n, l), indicating that z resides within\nan r-dimensional subspace of Rl spanned by the first r\ncolumns of V [10]. By retaining the top r singular val-\nues (with r \u226a min(n, l)) approximates the signal sub-\nspace, filtering out noise [11].\n\nThis assumes uniform analytes, so for mixtures, we\nbuild a dictionary of denoised sub-dictionaries per an-\nalyte class via SVD, creating a union of subspaces as\nside information for feature extraction. We then employ\na transformer encoder, a machine learning model adept\nat processing sequences like spectral peaks with posi-\ntional context, to generate embeddings for input spectra\nand dictionaries separately. This separation allows spec-\ntral embeddings to extract biologically relevant features\nfrom the dictionary during training, and dictionary re-\nmoval during inference, halving parameters for faster,\ndeployable predictions crucial for field use in biological\nmonitoring.\n\n2. Methods\n\n2.1. Aerosol MALDI-MS Data Acquisition\n\nIn this study, aerosol particles were ionized us-\ning ultraviolet (349 nm) laser pulses in a portable\nMALDI-Time-of-Flight (ToF) mass spectrometer, as\ndetailed in our prototype system [7].\nEach mass\nspectrum is represented as an intensity vector s =\n[s1, s2, . . . , sl]T \u2208 Rl paired with a corresponding m\nz vec-\ntor m = [m1, m2, . . . , ml]T \u2208 Rl. For batches of n parti-\ncles, the spectra form a matrix:\n\nS =\n\n\uf8ee\n\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\ns11\ns21\n...\nsn1\n\ns12\ns22\n...\nsn2\n\n. . .\n. . .\n. . .\n. . .\n\n\uf8f9\n\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\ns1l\ns2l\n...\nsnl\n\n\u2208 Rn\u00d7l\n\n.\n\nSince m is identical across measurements (unless ma-\nchine parameters change), a two-dimensional m matrix\nis unnecessary.\n\nDue to the prototype phase, our data collection is lim-\nited to a handful of targets. To simulate field-expected\n\n2\n\n\fFigure 1: (A) Top: An example batch of particles containing 80% dust particulate, with the remaining 20% evenly divided among the four biological\nmarkers. Each row represents a mass spectrum, and each column corresponds to a mass-to-charge ratio value. Bottom: The column-wise average.\n(B) Top: The heatmap of rows from (A) corresponding to B. globigii spectra. Bottom: the average spectrum. (C), (D), (E) same format as (A) but\nwith E. coli, Insulin, and Ubiquitin, respectively.\n\nspectral profiles for environmental pathogen monitor-\ning, we selected bacterial (multi-peak, noisy), protein\n(few-peak), and non-biological (peakless) types. Data\nincluded two bacteria (Bacillus globigii, Escherichia\ncoli), two proteins (insulin, ubiquitin) as positives, and\nArizona Road Dust as negative background. For safety,\nsamples were aerosolized and collected in a lab set-\nting, mimicking real-world conditions. Data acquisi-\ntions were performed in a class-specific manner, with\nm\nz restricted to the range [500, 10, 000] Da. Each class\nproduces a matrix of raw spectra, S, which is split into\n80% for training and 20% for testing. Consequently,\nour model is trained on individual raw spectra, simulat-\ning single-shot detection, to ensure robust performance\nin field scenarios where a matrix of spectra may con-\ntain multiple, co-occurring classes. Table 1 summarizes\nspectra counts and the training/testing split.\n\nClass\nA.R.Dust\nB.globigii\nE.coli\nInsulin\nUbiquitin\nTotal\n\nTable 1: Number of Spectra per Class\n# of Samples Training Samples Test Samples\n504\n1200\n1200\n1120\n1200\n5224\n\n630\n1500\n1500\n1400\n1500\n6530\n\n126\n300\n300\n280\n300\n1306\n\n2.2. Sparse Signal Processing\n\nA mass spectrum s can be modeled by a linear com-\nbination of columns, referred to as atoms, from a dictio-\nnary matrix D \u2208 Rl\u00d7\u03b1, such that s = Dx, where x \u2208 R\u03b1\nis a sparse coefficient vector (||x||0 \u226a \u03b1). This synthe-\nsis model leverages sparsity to denoise and extract rel-\nevant features from noisy spectra, crucial for real-time\n\n3\n\n\fpathogen detection in complex aerosol signatures.\n\nCommon dictionaries include orthogonal bases like\nCosines, Fourier, or Wavelets (where l = \u03b1), but\nover-complete dictionaries ((l \u226a \u03b1))\n[12] allow\nmore flexible representations, advancing applications\nin compressive sensing[13], dictionary learning[14],\nmedical\nimaging[15], classification[16], and object\ndetection[17].\n\nThese approaches often solve the relaxed convex op-\n\ntimization problem known as Basis Pursuit (BP)[18]:\n\n||x||1 s.t. s = Dx\n\nmin\nx\n\n(1)\n\nwhere the sparse vector, x, encodes rich task-specific\ninformation. This motivated data-driven dictionaries, as\ndemonstrated in the Face Recognition imaging problem\n[19], where training samples form columns based on\nthe principle that high-dimensional data from the same\nclass lie in a low-dimensional subspace. A test sam-\nple is thus represented as a sparse linear combination of\ntraining samples via Sparse Representation Classifica-\ntion (SRC)[19]. SRC has been applied to many applica-\ntions such as image classification [20], denoising [21],\nand deep learning [22].\n\n2.3. Proposed Dictionary Construction\n\nInspired by SRC, we constructed the dictionary D \u2208\nRl\u00d7\u03b1 using \u03b1 training spectra evenly distributed across\nc classes (\u03b1/c per class), arranged column-wise with\nsame-class spectra grouped into sub-dictionaries Di\n(i = 1, . . . , c). Thus, Di = [di,1, . . . , di,\u03b1/c], and D =\n[D1, . . . , Dc].\n\nThis class-specific clustering exploits the low-rank\nstructure inherent in spectra from the same biomolecu-\nlar class (rank r \u226a \u03b1/c). To denoise and capture essen-\ntial features, we applied Singular Value Decomposition\n(SVD) to each sub-dictionary:\n\n\u02dcDi = Ur\u03a3rVT\nr ,\n\nr \u226a\n\n\u03b1\nc\n\n,\n\nwhere Ur \u2208 Rl\u00d7r and Vr \u2208 R\u03b1/c\u00d7r are orthogonal matri-\nces of left and right singular vectors, and \u03a3r \u2208 Rr\u00d7r con-\ntains the top r singular values. The denoised dictionary\nwas then formed by stacking these low-rank approxima-\ntions:\n\n\u02dcD = [ \u02dcD1, \u02dcD2, . . . , \u02dcDc] \u2208 R\u03b1\u00d7l.\n\nThis creates a union of rank-r subspaces that efficiently\nrepresent key biomolecular patterns in noisy spectra, en-\nhancing multi-label classification for airborne pathogen\n\nFigure 2: Top: Heatmap of 200 mass spectra from Bacillus globigii.\nMiddle: A low-rank approximation via the Singular Value Decompo-\nsition (SVD) with rank r = 2. Bottom: The first 50 singular values\nfrom the SVD plotted on a y-axis log-scale.\n\ndetection in public health scenarios. The approach\u2019s ef-\nficacy is shown in Fig. 2, where SVD on Bacillus glo-\nbigii data reveals the first two singular values captur-\ning primary features, with subsequent values as noise; a\nrank-2 approximation sharpens peaks and reduces noise\nfor clearer identification.\n\n2.4. Transformers for Time-Series and Mass Spectrom-\n\netry\n\nTransformers, first introduced for natural language\nprocessing [23], have been extended to time-series anal-\nysis, including forecasting [24, 25, 26] and classifica-\ntion [27, 28]. In mass spectrometry (MS), they support\npeptide/protein identification and protein structure pre-\ndiction [29, 30], capitalizing on long-range dependen-\ncies among spectral peaks. Typically, these models use\ndenoised, high-resolution spectra from lab instruments,\nwith m\nz values as positional inputs. Recent MS advance-\nments include PowerNovo [31], an ensemble of trans-\nformer and BERT models for tandem MS peptide se-\nquencing, and a semi-autoregressive transformer frame-\nwork for rapid sequencing [32]. These methods acceler-\nate proteomics but often rely on preprocessed data, con-\ntrasting our focus on raw, noisy single-shot spectra from\n\n4\n\n\u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0014\u0000\u0011\u0000\u0019\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0018\u0000\u0011\u0000\u0018\u0000\u001b\u0000\u0011\u0000\u0016\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0014\u0000\u0018\u0000\u0013\u0000,\u0000R\u0000Q\u0000\u0003\u0000,\u0000Q\u0000W\u0000H\u0000Q\u0000V\u0000L\u0000W\u0000\\\u0000\u0003\u0000\u000b\u0000P\u00009\u0000\f\u0000%\u0000\u0011\u0000J\u0000O\u0000R\u0000E\u0000L\u0000J\u0000L\u0000L\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0016\u0000\u0011\u0000\u0019\u0000\u0017\u0000\u0011\u0000\u0013\u0000\u0017\u0000\u0011\u0000\u0017\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0017\u0000\u0013\u0000=\u0000R\u0000R\u0000P\u0000H\u0000G\u0000\u0003\u0000,\u0000Q\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0016\u0000\u0013\u0000\u0017\u0000\u0013\u0000\u0018\u0000\u0013\u0000,\u0000Q\u0000G\u0000H\u0000[\u0000\u0015\u0000\u00ee\u0000\u0014\u0000\u0013\u0000\u0016\u0000\u0016\u0000\u00ee\u0000\u0014\u0000\u0013\u0000\u0016\u0000\u0017\u0000\u00ee\u0000\u0014\u0000\u0013\u0000\u0016\u00006\u0000L\u0000Q\u0000J\u0000X\u0000O\u0000D\u0000U\u0000\u0003\u00009\u0000D\u0000O\u0000X\u0000H\u0000\u0003\u0000\u000b\u0000O\u0000R\u0000J\u0000\f\u00006\u0000L\u0000Q\u0000J\u0000X\u0000O\u0000D\u0000U\u0000\u0003\u00009\u0000D\u0000O\u0000X\u0000H\u0000V\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013\u0000\u0011\u0000\u0019\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0014\u0000\u0011\u0000\u0019\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0018\u0000\u0011\u0000\u0018\u0000\u001b\u0000\u0011\u0000\u0016\u0000P\u0000\u0012\u0000]\u0000\u0003\u0000\u000b\u0000N\u0000'\u0000D\u0000\f\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0014\u0000\u0018\u0000\u0013\u0000,\u0000R\u0000Q\u0000\u0003\u0000,\u0000Q\u0000W\u0000H\u0000Q\u0000V\u0000L\u0000W\u0000\\\u0000\u0003\u0000\u000b\u0000P\u00009\u0000\f\u00005\u0000D\u0000Q\u0000N\u0000\u0003\u0000\u0015\u0000\u0003\u0000$\u0000S\u0000S\u0000U\u0000R\u0000[\u0000\u0011\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0016\u0000\u0011\u0000\u0019\u0000\u0017\u0000\u0011\u0000\u0013\u0000\u0017\u0000\u0011\u0000\u0017\u0000P\u0000\u0012\u0000]\u0000\u0003\u0000\u000b\u0000N\u0000'\u0000D\u0000\f\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0017\u0000\u0013\u0000=\u0000R\u0000R\u0000P\u0000H\u0000G\u0000\u0003\u0000,\u0000Q\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013\u0000\u0011\u0000\u0019\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0013\fforming M \u2208 RN\u00d7\u03c1. A linear projection maps these to\nh:\n\nMpe = MWT + b \u2208 RN\u00d7h,\n\nwith W \u2208 Rh\u00d7\u03c1 and b \u2208 Rh. Adding Mpe to spectral\nembeddings P integrates intensity and m\nz positions, al-\nlowing learned extrapolation to h for better biomolecu-\nlar pattern recognition in noisy spectra.\n\nDictionary Embeddings. Similarly, each spectrum in\nthe denoised dictionary \u02dcD \u2208 R\u03b1\u00d7l (Section 2.3) un-\ndergoes convolutional embedding, producing patch se-\nquences \u02dcdp\ni \u2208 RN\u00d7h for i = 1, . . . , \u03b1, stacked into\n\u02dcDp \u2208 R\u03b1\u00d7N\u00d7h. The same positional embeddings Mpe\nare added to consistently encode m\nz positions. Sepa-\nrate learnable weights for input and dictionary spectra\ndistinguish noisy from denoised features, allowing tar-\ngeted extraction of clean biomolecular signatures for\nimproved detection accuracy.\n\nEncoder Blocks. Both input and dictionary pathways\nemploy transformer encoder blocks based on Vaswani\net al. [23], featuring multi-head self-attention, an MLP,\nlayer normalization, and residuals for gradient stability.\nFor the input, embeddings P (with added positional em-\nbeddings) are projected to queries (Q), keys (K), and\nvalues (V):\n\nQ = PWQ, K = PWK, V = PWV ,\n\nc\n\nwith scaled dot-product attention per head and concate-\nnation via WO. This captures long-range spectral de-\npendencies essential for noisy data. For the dictionary,\nembeddings \u02dcDp (with added positional embeddings)\nare processed per sub-dictionary \u02dcDp\ni \u2208 R \u03b1\nc \u00d7N\u00d7h. For\neach sub-dictionary, a learnable sequence \u02dcdL\ni \u2208 R1\u00d7N\u00d7h\nis concatenated,\nthe resulting tensor is permuted to\n+1)\u00d7h, and attention is applied slice-wise across se-\nRN\u00d7( \u03b1\nquences at each patch position (Fig. 5). The c sequence\ntokens gather information globally throughout the sub-\ndictionary, providing aggregated side information for\nthe input encoder\u2019s output. Keeping the input and dic-\ntionary encoders separate ensures denoised priors guide\nclassification without contaminating raw signals, facili-\ntating precise pathogen identification for biodefense ap-\nplications.\n\nSelection Attention. Following encoding, the input se-\nquence (shape N \u00d7 1 \u00d7 h) selectively extracts features\nfrom the c aggregated sub-dictionary sequences (shape\nN \u00d7 c \u00d7 h) via a multi-head cross-attention layer (Fig. 6).\nThe input acts as queries, with sub-dictionaries as keys\nand values, enabling class-specific feature integration\n\nFigure 3: The input spectral embedding layer creates a sequence of\nsmall overlapping patches from the mass spectrum s through one-\ndimensional convolution filters, transforming the 1D spectrum to 2D\nsequence.\n\nportable aerosol systems for real-time pathogen detec-\ntion in environmental health monitoring.\n\n2.5. Proposed MS-DGFormer Architecture\n\nThe Mass Spectral Dictionary-Guided Transformer\n(MS-DGFormer) processes raw input spectra and de-\nnoised dictionary spectra through separate embedding\nand encoding pathways, enabling robust multi-label\nclassification of biomolecular patterns in noisy aerosol\ndata for real-time public health monitoring (see Fig. 4\nfor model overview).\n\nInput Embedding. To handle the intensity vector s \u2208 Rl\nz values m \u2208 Rl, we adapt the\nand corresponding m\n\"patchification\" from Vision Transformers [33] for 1D\nspectra. Overlapping patches are extracted via 1D con-\nvolution with kernel size \u03c1, stride \u03b3, and h output chan-\nnels (hidden dimension), yielding N = l\u2212\u03c1\n+ 1 embed-\n\u03b3\ndings:\n\npi, j =\n\n\u03c1\u22121(cid:88)\n\nk=0\n\nw j,k s\u03b3i+k + b j,\n\nforming matrix P \u2208 RN\u00d7h. This convolutional method\ncaptures local peaks amid noise, outperforming linear\nprojections by reducing edge artifacts and enhancing ro-\nbustness (Fig. 3). Transformer attention is permutation-\ninvariant, focusing on pairwise token relationships with-\nout inherent order. Positional embeddings address this\nby encoding sequence positions, using methods like\nfixed sinusoids [23], rotary embeddings (RoPE) [34], or\nlearnable parameters. In mass spectrometry, the m\nz vec-\ntor m provides intrinsic positional data from time-of-\nflight. For overlapping patches projected to dimension\nh, we patch m similarly: Mi = m[\u03b3(i\u22121)+1:\u03b3(i\u22121)+\u03c1] \u2208 R\u03c1,\n\n5\n\n\fFigure 4: The Mass Spectral Dictionary-Guided Transformer (MS-DGFormer) architecture. (A) Input embedding module. (B) Dictionary Embed-\nding Module. (C) Selection Attention Mechanism. (D) Final peak prediction layer.\n\n(e.g., prioritizing the relevant low-rank subspace for a\ngiven pathogen). A residual connection preserves orig-\ninal input information, enhancing model stability and\naccuracy in noisy environmental samples.\n\nPeak Prediction. Known peak locations in training\nclasses form ground-truth yi \u2208 RN, where yi, j = 1 if\nclass i has a peak at patch j. An MLP processes the\nmodel output Pout \u2208 RN\u00d7h for binary predictions:\n\n\u02c6y = sigmoid\n\n(cid:16)\n\nReLU(PoutW(1) + b(1))W(2) + b(2)(cid:17)\n\n,\n\nwith W(1) \u2208 Rh\u00d7\u03d5, b(1) \u2208 R\u03d5, W(2) \u2208 R\u03d5\u00d71, and b(2) \u2208 R.\nThis outputs probabilities per patch, supporting multi-\nlabel classification for rapid biomolecular threat detec-\ntion in aerosols. The final predicted class \u02c6c is the one\nwith maximum cosine similarity to ground truth vectors\nyc for c \u2208 {1, . . . , 5}:\n\n\u02c6c = arg max\n\nc\n\n\u02c6y \u00b7 yc\n\u2225\u02c6y\u2225\u2225yc\u2225\n\n.\n\nTraining optimizes binary cross-entropy between \u02c6y and\ny, while class predictions inform evaluation metrics: ac-\ncuracy, precision, recall, and F1 score.\n\n3. Results\n\n3.1. Competing Models\n\nTo evaluate our model\u2019s performance, we bench-\nmarked it against recurrent baselines and a dictionary-\nablated variant, focusing on sequence modeling for\nnoisy mass spectra in pathogen detection. To ensure\na fair comparison, we strive to maintain consistency in\n\nmodel parameters where possible; however, due to ar-\nchitectural differences, exact parameter matching is not\nalways feasible.\n\nRecurrent Neural Network (RNN)[35], Long Short-\nTerm Memory (LSTM)[36], and Bidirectional LSTM\n(biLSTM)[37] models retained our input embedding\nand peak prediction layers for consistent processing and\noutput. Positional embeddings were omitted, as RNNs\nand LSTMs inherently capture sequential order. The\ndictionary, input encoder, dictionary encoder, and se-\nlection attention were replaced with RNN, LSTM, or\nbiLSTM blocks. This evaluation directly compares re-\ntransformer-based sequence handling in\ncurrent vs.\nbiomolecular classification.\n\nTo assess the dictionary\u2019s impact in our model, we\ntrained a model without\nthe dictionary embedding,\nencoder, and selection attention (MS-Former), which\nhalved the total parameter count.\nEssentially this\nmodel is a standard transformer. For fair comparison,\nwe trained variants with 3 input encoder layers (MS-\nFormer-3; 4.13M parameters) to match our core archi-\ntecture and 7 layers (MS-Former-7) to approximate to-\ntal parameters (8-9M across models), isolating the dic-\ntionary\u2019s role in enhancing accuracy for environmental\nhealth applications.\n\n3.2. Model and Dictionary Hyperparameters\n\nExperiments used convolutional embedding window\nsize \u03c1 = 100 and overlap \u03b3 = 50 (50%), yielding\nN = 1765 patches for spectra of length l = 88300. Cor-\nresponding m\nz values were patched identically. Patches\nmapped to hidden dimension h = 256. Multi-head at-\ntention (nheads = 8, dk = 32) was applied in the in-\n\n6\n\n\fc\n\nc \u00d7N\u00d7h, where each kernel encodes temporal peak information. A learnable token sequence \u02dcdL\n1\n\n]T are transformed into token sequences via convolution with overlapping kernels and h output channels, yielding \u02dcDp\n1\n\nFigure 5: The processing of a sub-dictionary is illustrated by exemplifying the first low-rank approximated sub-dictionary \u02dcD1 \u2208 R \u03b1\nspectra [ \u02dcd1,1, . . . , \u02dcd1, \u03b1\nR \u03b1\n\u02dcDp\n1\nattention mechanism aggregates information across the \u03b1\nc\nwith contextual information, are extracted to represent the aggregated temporal information.\n\nc \u00d7l. The\n\u2208\n1 , forming\n+1)\u00d7h, for attention to be computed independently across the N temporal positions. The\n1 , now enriched\n\n+ 1 sequences at each temporal location. Finally, the learnable tokens \u02dcdL\n\n+1)\u00d7N\u00d7h. This tensor is permuted to RN\u00d7( \u03b1\n\n\u2208 R1\u00d7N\u00d7h is concatenated with \u02dcDp\n\n\u2208 R( \u03b1\n\nc\n\nc\n\noverconfident in its peak predictions.\n\n3.3. MS-DGFormer Evaluation\n\nWe begin by examining the results obtained from\nMS-DGFormer before proceeding to a comparison with\ncompeting models. To understand the features captured\nin each sub-dictionary, we visualize the attention maps\nderived from each learnable sequence, \u02dcdL\ni . Figure 7 dis-\nplays the average attention scores, revealing that patches\ncontaining spectral peaks receive higher attention scores\ncompared to those without. Thus, \u02dcdL\ni effectively focuses\non the peaks found within the ith sub-dictionary.\n\nNext, to gain insight into how the model processes\nthe raw overlapping m\nz values and maps them to its em-\nbedding space, we extract and visualize the positional\nembeddings. For visualization purposes, we flatten both\nthe raw m\nz values and the corresponding positional em-\nz matrix M \u2208 R1765\u00d7100\nbeddings. Specifically, the raw m\nis flattened to a vector in R176500, and the positional\nembeddings P \u2208 R1765\u00d7256 are flattened to a vector in\nR451840. We then plot these flattened vectors, focusing\non the segments corresponding to the first 10 patches,\nas illustrated in Fig. 8. From this figure, it is evident\nthat the model preserves the structural characteristics of\nthe raw m\nz values, such as their overlapping nature (with\n50% overlap between adjacent patches), while mapping\nthem to a higher-dimensional embedding space (from\n100 to 256 dimensions per patch). Additionally, the\namplitude of the positional embeddings is adjusted to\na range of approximately \u00b15, aligning with the intensity\nscales observed in the mass spectra. To demonstrate the\nalignment between the positional embeddings and the\n\nFigure 6: An input spectral sequence P \u2208 RN\u00d7h is first encoded by\nthe input encoder. Each sub-dictionary\u2019s sequences are permuted to\nRN\u00d7( \u03b1\n+1)\u00d7h, processed by the dictionary encoder, and the respective\nc\nlearnable token sequences ( \u02dcdL\ni ) are extracted. Multi-head attention\nselects dictionary features for each temporal position.\n\nput encoder\u2019s self-attention, dictionary encoder\u2019s slice\nattention, and selection attention. Attention MLP inter-\nmediate dimension was 2048; peak prediction MLP was\n\u03d5 = 512. Both encoders had L = 3 layers. Dictionary\nD comprised \u03b1 = 32 sequences from 4 positive classes\n(B. globigii, E. coli, insulin, ubiquitin; 8 per class), ex-\ncluding dust. Limited by single 4070 GPU memory, it\nwas denoised via sub-dictionary SVD with rank r = 2,\nproviding efficient side information for robust pathogen\ndetection in portable systems.\n\nOur model and competing models were trained for\n300 epochs with batch size of 8, a learning rate of\n10\u22124 with 10% warm-up, and a cosine annealing de-\ncay. There were two types of regularization imple-\nmented: neuron dropouts placed similarly to [23] with\n0.1 dropout probability, and binary cross-entropy label\nsmoothing. This type of label smoothing treats each 1\nas 0.9 and each 0 as 0.1, so the model does not become\n\n7\n\n\fFigure 7: Each class\u2019s sub-dictionary attention maps averaged across\nheads are shown. The larger attention scores are located at each class\u2019s\ntrue peak locations showing the dictionary\u2019s efficacy for feature ex-\ntraction.\n\nFigure 8: Top: The overlapping patches of m\nz values are flattened to\nRN\u03c1 (left). M is embedded via a learnable linear layer producing Mpe\n(right). Middle: The first 10 patches of M and Mpe. Bottom: The\npatches are normalized and plotted on the same linspace.\n\noriginal m\nz values, we normalize both to a common am-\nplitude range and plot them on a uniform linear space.\nFig. 9 illustrates this alignment for each patch, high-\nlighting the model\u2019s ability to encode positional infor-\nmation effectively.\n\nFinally, we visualize the attention maps within the\nselection attention head to understand how the model\nselects features from the sub-dictionaries based on the\ninput spectrum class. To achieve this, we input a repre-\nsentative bacteria, protein, and noise spectrum into the\nmodel and extract the attention map from the selection\nattention head for visualization. Fig. 9 shows the at-\ntention scores across the sub-dictionaries for each input\nspectrum. From this figure, it is evident that for each\ninput spectrum, the attention scores are significantly\nhigher for the sub-dictionary corresponding to the same\nclass. However, for the Arizona Road Dust class, the\nattention scores are more dispersed and lower in mag-\nnitude. This behavior is expected because the Arizona\nRoad Dust spectra primarily contain noise peaks, which\ndo not align well with the denoised spectra represented\nin the sub-dictionaries.\n\ntest set, with performance measured using micro and\nmacro accuracy, precision, recall, and F1-score. Table\nIII presents the macro metrics, where MS-DGFormer\nachieves the highest scores across all metrics, despite\nhaving fewer parameters than most other models, with\nthe exception of MS-Former-3. Notably, the biLSTM-\nbased model outperforms MS-Former-7 but still falls\nshort of MS-DGFormer\u2019s performance.\n\nFor brevity, we focus on the micro F1-score in our\nanalysis, as it effectively balances the trade-off between\nfalse positives and false negatives, which is critical for\nclassification tasks. Table IV displays the micro F1-\nscores for each class, with MS-DGFormer consistently\nachieving the highest F1-score across all classes.\nIn\ncontrast, the other models exhibit significant perfor-\nmance degradation on the Arizona Road Dust class.\nThis is particularly concerning because dust-like parti-\ncles are commonly encountered in environmental con-\nditions, and misclassifying them as biological agents\ncould result in a high rate of false alarms.\n\n3.5. Computational Efficiency\n\n3.4. Competing Model Comparisons\n\nHaving demonstrated the spectral features captured\nby MS-DGFormer, we now proceed to evaluate its\nperformance against competing models using standard\nclassification metrics. Each model is assessed on the\n\nReal-time field analysis demands rapid processing\nof continuous spectral streams, where both parameter\ncount and hardware requirements are critical. Our de-\nsign improves efficiency by separating the dictionary\nembedding/encoder from the input embedding/encoder.\n\n8\n\n\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL1\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL2\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL3\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL4\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0013\u0000\u0013\u0000/\u0000H\u0000D\u0000U\u0000Q\u0000H\u0000G\u0000\u0003\u0000'\u0000L\u0000F\u0000W\u0000L\u0000R\u0000Q\u0000D\u0000U\u0000\\\u0000\u0003\u00006\u0000H\u0000T\u0000X\u0000H\u0000Q\u0000F\u0000H\u0000V\u0000\n\u0000\u0003\u0000$\u0000W\u0000W\u0000H\u0000Q\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u00000\u0000D\u0000S\u0000V\u00006\u0000X\u0000E\u0000\u0010\u0000'\u0000L\u0000F\u0000W\u0000L\u0000R\u0000Q\u0000D\u0000U\u0000\\\u0000\u0003\u00006\u0000H\u0000T\u0000X\u0000H\u0000Q\u0000F\u0000H\u0000\u0003\u00001\u0000X\u0000P\u0000E\u0000H\u0000U\u00003\u0000D\u0000W\u0000F\u0000K\u0000\u0003\u00001\u0000X\u0000P\u0000E\u0000H\u0000UM1M500M1000M1500\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0014\u0000\u0011\u0000\u0013\u00001\u0000R\u0000U\u0000P\u0000D\u0000O\u0000L\u0000]\u0000H\u0000G\u0000\u0003mz\u0000)\u0000O\u0000D\u0000W\u0000W\u0000H\u0000Q\u0000H\u0000G\u0000\u0003M\u0000\u0003\u0000\u000b\u0000R\u0000Y\u0000H\u0000U\u0000O\u0000D\u0000S\u0000S\u0000L\u0000Q\u0000J\u0000\u0003mz\u0000\fMpe1Mpe500Mpe1000Mpe1500\u0000\u0018\u0000\u0013\u0000\u0018\u0000(\u0000P\u0000E\u0000H\u0000G\u0000G\u0000H\u0000G\u0000\u0003mz\u0000)\u0000O\u0000D\u0000W\u0000W\u0000H\u0000Q\u0000H\u0000G\u0000\u0003Mpe\u0000\u0003\u0000\u000b\u00003\u0000R\u0000V\u0000\u0011\u0000\u0003\u0000(\u0000P\u0000E\u0000H\u0000G\u0000\u0011\u0000\fM1M3M5M7M9\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0015\u0000)\u0000L\u0000U\u0000V\u0000W\u0000\u0003\u0000\u0014\u0000\u0013\u0000\u0003\u0000S\u0000D\u0000W\u0000F\u0000K\u0000H\u0000VMpe1Mpe3Mpe5Mpe7Mpe9\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0014\u0000)\u0000L\u0000U\u0000V\u0000W\u0000\u0003\u0000\u0014\u0000\u0013\u0000\u0003\u0000O\u0000H\u0000D\u0000U\u0000Q\u0000H\u0000G\u0000\u0003\u0000S\u0000D\u0000W\u0000F\u0000K\u0000H\u0000V\u0000\u0014\u0000\u0016\u0000\u0018\u0000\u001a\u0000\u001cmz\u0000\u0003\u00003\u0000D\u0000W\u0000F\u0000K\u0000H\u0000V\u0000\u0003\u0000I\u0000R\u0000U\u0000\u0003M\u0000\u0003\u0000D\u0000Q\u0000G\u0000\u0003Mpe\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0015\u00001\u0000R\u0000U\u0000P\u0000D\u0000O\u0000L\u0000]\u0000H\u0000G\u0000\u0003\u0000D\u0000Q\u0000G\u0000\u0003\u0000$\u0000O\u0000L\u0000J\u0000Q\u0000H\u0000G\u0000\u0003\u0000R\u0000Q\u0000\u0003\u00008\u0000Q\u0000L\u0000I\u0000R\u0000U\u0000P\u0000\u0003\u0000/\u0000L\u0000Q\u0000V\u0000S\u0000D\u0000F\u0000HMpe1:10M1:10\fTable 2: Macro Metrics Across All Classes.\n\nModel\n\nRNN-6\nLSTM-4\nBiLSTM-6\nMS-Former-3\nMS-Former-7\nMS-DGFormer\n\nAccuracy\n0.560\n0.679\n0.939\n0.709\n0.862\n0.983\n\nMacro Metrics\n\nPrecision Recall\n0.560\n0.679\n0.939\n0.709\n0.862\n0.983\n\n0.832\n0.821\n0.916\n0.845\n0.876\n0.982\n\nF1\n0.491\n0.641\n0.915\n0.664\n0.824\n0.982\n\nParams\n9.52M\n9.50M\n9.48M\n4.13M\n9.39M\n8.36M\n\nModel\n\nRNN-6\n\nLSTM-4\n\nBiLSTM-6\n\nMS-Former-3\n\nMS-Former-7\n\nMS-DGFormer\n\nTable 3: Micro F1 Scores For Each Class\nA.R.Dust B.globigii E.coli\n\nInsulin Ubiq.\n\n0.278\n\n0.331\n\n0.736\n\n0.369\n\n0.553\n\n0.949\n\n0.045\n\n0.623\n\n0.926\n\n0.328\n\n0.666\n\n0.991\n\n0.408\n\n0.374\n\n0.954\n\n0.880\n\n0.984\n\n0.979\n\n0.795\n\n0.906\n\n0.976\n\n0.787\n\n0.921\n\n0.987\n\n0.926\n\n0.972\n\n0.983\n\n0.952\n\n0.994\n\n0.994\n\nFigure 9: Top: E.coli input spectrum and the attention map from the\nselection attention mechanism. The attention scores are larger for\nE.coli, capturing the learning of the selection mechanism. Middle:\nUbiquitin input spectrum and its corresponding selection attention\nmap. Bottom: Arizona Road Dust and its corresponding selection\nattention map showing no dominant features selected from a single\nclass.\n\nthe\n\ndictionary\n\nThe dictionary sequences remain constant during train-\ning and are encoded independently of the input spec-\ntrum. Only the learned sequence per sub-dictionary is\nused: \u03b1 sequences enter the dictionary encoder during\ntraining, but only c sequences are passed to the selection\nattention head.\nBecause\n\nspectrum-\nindependent, we precompute and store the c learned\nsub-dictionary sequences,\nremoving the dictionary\nembedding layer, dictionary encoder, and dictionary\nitself from the inference model. This yields an efficient\nvariant, MS-DGFormer-E, where only the pre-trained\nweights of the remaining components are loaded. The\nc sequences are stored in memory and fed directly into\nthe selection attention head, cutting parameters from\n8.36 \u00d7 106 to 4.39 \u00d7 106 without performance loss.\n\ninput\n\nis\n\nThis architecture also scales efficiently. Adding more\nsequences to sub-dictionaries during training does not\n\naffect inference time, as the number of sequences used\nat inference, c, remains fixed.\nIf \u03b8 new classes are\nc \u03b8 + \u03b1 se-\nadded, the training dictionary expands to \u03b1\nquences, yet inference still requires only c+\u03b8 sequences,\nwith c \u226a \u03b1\nc .\n\nWe evaluated inference speed and spectral throughput\non batches of size 1, 4, and 8, averaging 100 runs after\n10 warm-up runs. As shown in Table V, MS-DGFormer-\nE achieves nearly a 2\u00d7 increase in mean inference speed\nand more than a 2\u00d7 increase in throughput over the full\nMS-DGFormer. The only model with comparable infer-\nence time is MS-Former-3, due to its similar parameter\ncount, but its classification performance is significantly\nlower. While results are hardware-specific, the relative\nefficiency gains should generalize across platforms.\n\n4. Discussion\n\nWe have introduced an approach capable of classify-\ning individual noisy mass spectra without the need to\ncollect several spectra of the same class to create an\naveraged spectrum. The mass spectra are turned into\nsequences of small overlapping patches enabling atten-\ntion mechanisms to capture peak locations. Although,\nthis alone is not enough to accurately classify the spec-\ntral input due to intense noise. We notice that if we\ndo have a batch of spectra all from the same class, the\nSVD is able to significantly reduce noise and reveal true\nspectral peaks. However, it is unlikely this will nat-\nurally occur in the environment. We construct a dic-\ntionary composed of training samples from each class,\n\n9\n\n\fModel\n\nMean (ms) \u2193\n\nRNN-6\nLSTM-4\nBiLSTM-6\nMS-Former-3\nMS-Former-7\nMS-DGFormer\nMS-DGFormer-E\n\n125.32\n108.62\n156.38\n11.88\n23.46\n72.27\n12.31\n\nTable 4: Inference Performance Metrics for Different Models and Batch Sizes\nBatch Size 1\nStd (ms) \u2193\n\nSpectra/s \u2191 Mean (ms) \u2193\n\nBatch Size 4\nStd (ms) \u2193\n\nSpectra/s \u2191 Mean (ms) \u2193\n\nBatch Size 8\nStd (ms) \u2193\n\nSpectra/s \u2191\n\n3.36\n3.31\n2.56\n2.55\n2.32\n3.76\n1.67\n\n7.98\n9.21\n6.39\n84.15\n42.63\n13.84\n81.23\n\n156.30\n116.619\n263.82\n34.70\n74.31\n95.66\n35.98\n\n4.03\n4.79\n1.46\n3.31\n4.18\n2.85\n3.70\n\n25.59\n34.30\n15.16\n115.27\n53.83\n41.81\n111.16\n\n221.37\n194.51\n223.67\n62.87\n141.19\n127.17\n66.33\n\n5.94\n6.79\n4.18\n4.91\n3.08\n2.58\n3.92\n\n36.14\n41.13\n35.77\n127.25\n56.66\n62.91\n120.60\n\nclustered together to resemble sub-dictionaries, and per-\nform the SVD on the sub-dictionaries to be used as side-\ninformation. One learnable (randomly initialized) se-\nquence is concatenated to each sub-dictionary to capture\nfeatures for their respective sub-dictionary for both ef-\nfectiveness and efficiency. Further, our model processes\nthe input spectrum and dictionary through separate en-\ncoders allowing the features to be learned indepen-\ndently, with another attention head capable of selecting\nspecific information from the dictionary. In this man-\nner, only the learned dictionary sequences are required\nfor inference, and the dictionary components can be re-\nmoved from the model. This significantly improves in-\nference speed and spectral throughput since nearly half\nof the model parameters reside in the dictionary compo-\nnents. Finally, our MS-DGFormer is compared against\ncompeting sequential models and achieves the highest\nmicro and macro classification metrics with drastically\nfaster inference metrics.\n\nOverall, our proposed architecture, MS-DGFormer,\naddresses several challenges that arise when transition-\ning MALDI-MS systems from laboratory to environ-\nmental settings. Traditional MALDI-MS workflows re-\nquire extensive sample preparation and pre-processing;\nour approach eliminates much of this burden through\nautonomous aerosol sampling, shifting the workload\nto computational post-processing and machine learning\ninference. Rather than relying on multi-shot spectral\naveraging to obtain clean spectra, we leverage SVD-\ndenoised sub-dictionaries to provide rich side informa-\ntion for accurate classification of raw, minimally pro-\ncessed spectra.\n\nFalse positive classification is a particular challenge\nin environmental sampling, where negative classes such\nas dust are abundant. MS-DGFormer demonstrates ro-\nbust performance in these scenarios, effectively mitigat-\ning misclassification.\n\nOur\n\nresults highlight\n\nreal-time\npathogen detection using portable aerosol MALDI-MS\n\nthe potential of\n\nsystems, paving the way for field-deployable solutions\nthat can transform environmental monitoring, biological\nthreat detection, and efforts to reduce disease spread.\n\nDeclaration of competing interest\n\nW.B., and M.M. have competing interests. W.B. is\nthe President and CEO of Zeteo Tech, Inc. M.M. is the\nVice President of Research and CTO at Zeteo Tech, Inc.\n\nAcknowledgments\n\nThis work was supported in part by the National In-\nstitute of Health under award number T32GM142603\nand in part by Zeteo Tech Inc. Due to privacy or ethi-\ncal restrictions, the data from the study is only available\nupon request from the corresponding author. Code is\navailable upon request from the corresponding author.\n\nAuthor Contributions\n\nConceptualization, K.R., G.A., M.M. and W.B;\nmethodology, K.R., G.A., M.M. and W.B; software,\nK.R; validation, G.A., M.M., W.B.; formal analysis,\nK.R., G.A.; investigation, K.R., G.A., M.M. and W.B;\nresources, K.R., G.A., M.M. and W.B; data curation,\nK.R, M.M, W.B; writing\u2014original draft preparation,\nK.R., G.A; writing\u2014review and editing, G.A., M.M.\nand W.B.; visualization, K.R.; supervision, G.A., M.M.\nand W.B.; project administration, G.A., M.M. and W.B.;\nfunding acquisition, G.A., M.M. and W.B. All authors\nhave read and agreed to the published version of the\nmanuscript.\n\nDeclaration of generative AI and AI-assisted tech-\nnologies in the manuscript preparation process\n\nDuring the preparation of this work the author(s) used\nGrok xAI in order for grammar correction and readabil-\nity. After using this tool/service, the author(s) reviewed\n\n10\n\n\fand edited the content as needed and take(s) full respon-\nsibility for the content of the published article.\n\nReferences\n\n[1] A. El-Aneed, A. Cohen, J. Banoub, Mass spec-\ntrometry,\nreview of the basics: Electrospray,\nmaldi, and commonly used mass analyzers, Ap-\nplied Spectroscopy Reviews 44 (3) (2009) 210\u2013\n230. doi:10.1080/05704920902717872.\n\n[2] W. C. Wiley, I. H. McLaren, Time-of-flight mass\nspectrometer with improved resolution, Review of\nScientific Instruments 26 (12) (2004) 1150\u20131157.\ndoi:10.1063/1.1715212.\n\n[3] W. A. Bryden, R. C. Benson, H. W. Ko, C. Fense-\nlau, R. J. Cotter, Tiny-tof mass spectrome-\nin: P. J. Stopa, M. A.\nter for biodetection,\nBartoszcze (Eds.), Rapid Methods for Analy-\nsis of Biological Materials in the Environment,\nSpringer Netherlands, Dordrecht, 2000, pp. 101\u2013\n110. doi:10.1007/978-94-015-9534-6_10.\n\n[4] R. J. Cotter, Time-of-flight mass spectrometry,\nin: Electrospray and MALDI Mass Spectrome-\ntry, John Wiley & Sons, Ltd, 2010, pp. 345\u2013364.\ndoi:10.1002/9780470588901.ch10.\n\ndesorption/ionisation\n\n[5] A. L. van Wuijckhuijse, M. A. Stowers,\nW. A. Kleefsman, B. L. M. van Baar, C. E.\nKientz, J. C. M. Marijnissen, Matrix-assisted\nlaser\ntime-of-\nflight mass spectrometry for\nthe analysis of\nbioaerosols:\ndevelopment of a fast detector\nfor airborne biological pathogens, Journal of\nAerosol Science 36 (5-6)\n(2005) 677\u2013687.\ndoi:10.1016/j.jaerosci.2004.11.003.\n\naerosol\n\npathogen\n\n[6] C. Papagiannopoulou, R. Parchen, P. Rubbens,\nidentifica-\nFast\nW. Waegeman,\ntion\nlaser\nusing\ndesorption/ionization-aerosol time-of-flight mass\nspectrometry data and deep learning methods,\nAnalytical Chemistry 92 (11) (2020) 7523\u20137531.\ndoi:10.1021/acs.analchem.9b05806.\n\nsingle-cell matrix-assisted\n\n[7] D. Chen, W. A. Bryden, M. McLoughlin, S. A.\nEcelberger, T. J. Cornish, L. P. Moore, K. M. Re-\ngan, digitalmaldi: A single-particle-based mass\nspectrometric detection system for biomolecules,\nJournal of Mass Spectrometry 60 (2) (2025)\ne5110. doi:10.1002/jms.5110.\n\n11\n\n[8] M. McLoughlin, G. Arce, Deterministic prop-\nthe recursive separable median fil-\nerties of\nter,\nIEEE Transactions on Acoustics, Speech,\nand Signal Processing 35 (1) (1987) 98\u2013106.\ndoi:10.1109/TASSP.1987.1165026.\n\n[9] G. R. Arce, N. C. Gallagher, T. A. Nodes, Median\nfilters: Theory for one- and two-dimensional fil-\nters, in: T. S. Huang (Ed.), Advances in Computer\nVision and Image Processing, Vol. 2, JAI Press,\nGreenwich, CT, 1986, pp. 89\u2013166.\n\n[10] J. A. Fessler, R. R. Nadakuditi, Linear Algebra for\nData Science, Machine Learning, and Signal Pro-\ncessing, Cambridge University Press, 2024.\n\n[11] C. Eckart, G. Young, The approximation of one\nmatrix by another of lower rank, Psychometrika\n1 (3) (1936) 211\u2013218.\n\n[12] S. Mallat, Z. H. Zhang, Matching pursuits with\ntime-frequency dictionaries, IEEE Transactions\non Signal Processing 41 (12) (1993) 3391\u20133401.\ndoi:10.1109/78.258082.\n\n[13] D. L. Donoho, Compressed sensing, IEEE Trans-\nactions on Information Theory 52 (4) (2006)\n1289\u20131306.\n\n[14] M. Aharon, M. Elad, A. B. Malach, K-svd: An\nalgorithm for designing overcomplete dictionar-\nies for sparse representation, IEEE Transactions\non Signal Processing 54 (11) (2006) 4311\u20134322.\ndoi:10.1109/TSP.2006.881199.\n\n[15] S. Ravishankar, Y. Bresler, Mr image recon-\nstruction from highly undersampled k-space data\nIEEE Transactions on\nby dictionary learning,\nMedical\n(2011) 1028\u20131041.\ndoi:10.1109/TMI.2010.2090538.\n\nImaging 30 (5)\n\n[16] M. Yamac, M. Ahishali, A. Degerli, S. Ki-\nranyaz, M. E. H. Chowdhury, M. Gabbouj,\nsparse support estimator-based\nConvolutional\ncovid-19\nimages,\nfrom x-ray\nIEEE Transactions on Neural Networks and\nLearning Systems 32 (5)\n(2021) 1810\u20131820.\ndoi:10.1109/TNNLS.2021.3070467.\n\nrecognition\n\n[17] M. Ahishali, M. Yamac, S. Kiranyaz, M. Gab-\nbouj, Representation based regression for object\ndistance estimation, Neural Networks 158 (2023)\n15\u201329. doi:10.1016/j.neunet.2022.11.011.\nURL https://www.sciencedirect.com/science/article/pii/S089360802200452X\n\n\f[18] S. Chen, D. L. Donoho, M. A. Saunders, Atomic\ndecomposition by basis pursuit, SIAM Journal\non Scientific Computing 20 (1) (2001) 33\u201361.\ndoi:10.1137/S1064827500394074.\n\n[28] J. Xu, H. Wu, J. Wang, M. Long, Anomaly trans-\nformer: Time series anomaly detection with asso-\nciation discrepancy (2022). arXiv:2110.02642.\nURL https://arxiv.org/abs/2110.02642\n\n[19] J. Wright, A. Y. Yang, A. Ganesh, S. Sas-\ntry, Y. Ma, Sparse representation for com-\nputer vision and pattern recognition, Proceed-\nings of the IEEE 98 (6) (2009) 1031\u20131044.\ndoi:10.1109/JPROC.2009.2015716.\n\n[20] J. Yang, K. Yu, Y. Gong, T. Huang, Linear spatial\npyramid matching using sparse coding for image\nclassification, in: 2009 IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2009, pp.\n1794\u20131801. doi:10.1109/CVPR.2009.5206757.\n\n[21] J.-L. Starck, E. Candes, D. Donoho, The curvelet\ntransform for image denoising, IEEE Transac-\ntions on Image Processing 11 (6) (2002) 670\u2013684.\ndoi:10.1109/TIP.2002.1014998.\n\n[22] W. Wen, C. Wu, Y. Wang, Y. Chen, H. Li, Learning\nstructured sparsity in deep neural networks (2016).\narXiv:1608.03665.\nURL https://arxiv.org/abs/1608.03665\n\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\nAttention is all you need, in: Advances in Neu-\nral Information Processing Systems (NeurIPS),\nVol. 30, Curran Associates, Inc., 2017, pp. 5998\u2013\n6008. arXiv:1706.03762.\nURL https://arxiv.org/abs/1706.03762\n\n[24] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li,\nH. Xiong, W. Zhang, Informer: Beyond efficient\ntransformer for long sequence time-series fore-\ncasting (2021). arXiv:2012.07436.\nURL https://arxiv.org/abs/2012.07436\n\n[25] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun,\nR. Jin, Fedformer: Frequency enhanced decom-\nposed transformer for long-term series forecasting\n(2022). arXiv:2201.12740.\nURL https://arxiv.org/abs/2201.12740\n\n[26] A. Garza, C. Challu, M. Mergenthaler-Canseco,\n\nTimegpt-1 (2024). arXiv:2310.03589.\nURL https://arxiv.org/abs/2310.03589\n\n[27] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidi-\npaty, C. Eickhoff, A transformer-based framework\nfor multivariate time series representation learning\n(2020). arXiv:2010.02803.\nURL https://arxiv.org/abs/2010.02803\n\n[29] M. Ekvall, P. Truong, W. Gabriel, M. Wil-\ntransformer: A trans-\nfor prediction of ms2 spectrum in-\nProteome Research\n(2022) 1359\u20131364, pMID: 35413196.\n\nhelm, L. K\"all, Prosit\nformer\ntensities,\n21 (5)\ndoi:10.1021/acs.jproteome.1c00870.\nURL https://doi.org/10.1021/acs.jproteome.1c00870\n\nJournal\n\nof\n\n[30] M. Yilmaz, W. E. Fondrie, W. Bittremieux, W. S.\nNoble, Sequence-to-sequence translation from\nmass spectra to peptides with a transformer\nmodel, Nature Communications 15 (2024) 6427.\ndoi:10.1038/s41467-024-49731-x.\nURL https://www.nature.com/articles/s41467-024-49731-x\n\n[31] D. V. Petrovskiy, et al., Powernovo: de novo pep-\ntide sequencing via tandem mass spectrometry us-\ning an ensemble of transformer and bert models,\nScientific Reports 14 (2024) 15000.\n\n[32] Y. Zhao, S. Wang, J. Huang, B. Meng, D. An,\nX. Fang, Y. Wei, X. Dai, A transformer-based\nsemi-autoregressive framework for high-speed\nsequencing,\nand accurate de novo peptide\nCommunications Biology 8 (1)\n(2025) 234.\ndoi:10.1038/s42003-025-07584-0.\nURL https://doi.org/10.1038/s42003-025-07584-0\n\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nN. Houlsby, An image is worth 16x16 words:\nTransformers for image recognition at scale, arXiv\npreprint arXiv:2010.11929 (2021).\nURL https://doi.org/10.48550/arXiv.2010.11929\n\n[34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu,\nRoformer: Enhanced transformer with rotary po-\nsition embedding (2023). arXiv:2104.09864.\nURL https://arxiv.org/abs/2104.09864\n\n[35] J. L. Elman,\n\nFinding\n\nCognitive Science 14 (2)\ndoi:10.1207/s15516709cog1402_1.\n\nstructure\n\nin\ntime,\n(1990) 179\u2013211.\n\n[36] S. Hochreiter, J. Schmidhuber, Long short-term\nmemory, Neural Computation 9 (8) (1997) 1735\u2013\n1780. doi:10.1162/neco.1997.9.8.1735.\n\n12\n\n\f[37] M. Schuster, K. Paliwal, Bidirectional recur-\nIEEE Transactions on\nrent neural networks,\nSignal Processing 45 (11) (1997) 2673\u20132681.\ndoi:10.1109/78.650093.\n\n13\n\n\f"
}