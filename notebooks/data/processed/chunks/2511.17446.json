{
  "id": "2511.17446",
  "chunks": [
    "5 2 0 2 v o N 1 2 ] G L . s c [ 1 v 6 4 4 7 1 . 1 1 5 2 : v i X r a Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry\u22c6 Kyle M. Regana,\u2217, Michael McLoughlinc, Wayne A. Brydenc, Gonzalo R. Arceb aCenter for Bioinformatics and Computational Biology, University of Delaware, Newark, 19713, Delaware, United States bDepartment of Electrical and Computer Engineering, University of Delaware, Newark, 19716, Delaware, United States cZeteo Tech Inc., Sykesville, 21784, Maryland, United States Abstract Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is essential for biomolecular analysis in human health protection, offering precise identification of pathogens through unique mass spectral signatures to support environmental monitoring and disease prevention. However, traditional MALDI-MS relies on labor-intensive sample preparation and multi-shot spectral averaging, confining it to laboratory settings and hindering its application in dynamic, real-world settings critical for public health surveillance. These limitations are amplified in emerging portable aerosol MALDI-MS systems, where autonomous sampling produces noisy, single-shot spectra from a mix- ture of aerosol analytes, demanding novel computational detection methods to safeguard against infectious diseases. To address this, we introduce the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a computational framework that processes raw, minimally prepared mass spectral data for accurate multi-label classification, directly enhancing real-time pathogen monitoring. Utilizing a transformer architecture to model long-range dependencies in spectral time-series, MS-DGFormer incorporates a novel dictionary encoder with Singular Value Decomposition (SVD)-derived denoised side information, empowering the model to extract vital biomolecular patterns from noisy single-shot spectra with high reliability. This approach achieves robust spectral identification in aerosol samples, supporting autonomous, real-time analysis in field-deployable systems. By reducing preprocessing requirements, MS-DGFormer facilitates portable MALDI-MS deployment in high-risk areas like public spaces, transforming human health strategies through proactive environmental monitoring, early detection of biological threats, and rapid response to mitigate disease spread. Preprint Notice. This manuscript is a preprint and has been submitted to *Computer in Biology and Medicine*. Keywords: Pathogen Detection, Mass Spectrometry, Machine Learning, Bioaerosols, Public Health, 1. Introduction Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) has evolved to be a power- ful tool to characterize and identify large biomolecules. MALDI-MS is a \u201csoft ionization\u201d method and typically utilizes a light absorbing matrix that, when mixed with an analytical sample and illuminated with a pulsed laser \u22c6This preprint has been submitted to Computer in Biology and Medicine. \u2217Corresponding author Email addresses: regank@udel.edu (Kyle M. Regan ), mike.mcloughlin@zeteotech.com (Michael McLoughlin), wayne.bryden@zeteotech.com (Wayne A. Bryden), arce@udel.edu (Gonzalo R. Arce) light, will create ions from biomolecules to include pro- teins, peptides, and lipids [1]. For biodefense applica- tions, a major advantage of MALDI is that it can be combined with Time-of-Flight analyzers [2], which do not require a complex fluidic system and can be minia- turized for field applications [3]. To achieve high mass resolution and accuracy, multiple methods such as de- layed extraction and ion reflectors have been developed to compensate for energy spread during the ionization process [4]. Because MALDI-MS uses",
    "ions from biomolecules to include pro- teins, peptides, and lipids [1]. For biodefense applica- tions, a major advantage of MALDI is that it can be combined with Time-of-Flight analyzers [2], which do not require a complex fluidic system and can be minia- turized for field applications [3]. To achieve high mass resolution and accuracy, multiple methods such as de- layed extraction and ion reflectors have been developed to compensate for energy spread during the ionization process [4]. Because MALDI-MS uses a sample de- posited onto a substrate, deconvolution of a mixed sam- ple can be quite complex and has motivated the devel- opment of single particle methods [5], [6]. A portable prototype MALDI-MS, introduced in \u201cdigitalMALDI: A Single-Particle\u2013Based Mass Spectrometric Detection System for Biomolecules\u201d [7] demonstrated the ability to produce spectra from environmental aerosol parti- cles. The spectrometer autonomously samples aerosol particles, irradiates each by an ultraviolet laser creating ions from individual particles, then analyzes the ions by time-of-flight. The portability of this spectrometer enables in-field measurements for rapid identification of possible biological threats such as airborne bacteria, fungi, viruses, toxins, or nonvolatile chemicals. This research is driven by the urgent need to pro- tect communities from natural and engineered biolog- ical threats, such as infectious outbreaks or bioterror- ism. By enabling rapid detection of airborne pathogens, our work could curb disease spread and enhance pub- lic safety. Envision deploying compact devices in high- traffic areas like airports, transit systems, or stadiums for continuous, real-time scanning, to transform how we safeguard against environmental biological risks. At the heart of this approach lies the near autonomous sampling of atmospheric *aerosols, a technique that en- ables continuous monitoring without relying on exten- sive human intervention. Yet, creating a threat detection system that is robust, accurate, and precise is not easy. False positives risk sparking unwarranted alarm, while false negatives could allow a silent spread of infec- tion. Unlike controlled laboratory conditions with care- fully prepared samples, environmental sampling cap- tures a complex mixture of aerosol particles\u2014primarily harmless background particles\u2014making it challenging to identify the unique mass spectra of pathogens. This work tackles these challenges, laying the groundwork for a reliable system capable of safeguarding society from biological threats. MALDI-MS is already FDA-cleared for identifying bacterial and fungal isolates in clinical labs, capable of distinguishing over 10,000 strains. While extensive research applies MALDI-MS to biological specimens, most focus on meticulously prepared lab samples re- quiring days of culturing or separation. Even then, single-laser-shot spectra are noisy, so multi-shot aver- aging is standard to boost signal-to-noise ratio before database matching. This works well in clinics where shots target the same analyte, but environmental sampling involves diverse particles per shot. Averaging can blur features, as shown in Fig. 1, which depicts a batch of spectra from 80 dust aerosols mixed with five spectra each from Bacillus glo- bigii, E. coli, Insulin, and Ubiquitin. Averaging as- sumes uniform analytes [8][9], but mixtures yield mud- dled spectra (Fig. 1A). A single-shot method is essential to isolate individual",
    "boost signal-to-noise ratio before database matching. This works well in clinics where shots target the same analyte, but environmental sampling involves diverse particles per shot. Averaging can blur features, as shown in Fig. 1, which depicts a batch of spectra from 80 dust aerosols mixed with five spectra each from Bacillus glo- bigii, E. coli, Insulin, and Ubiquitin. Averaging as- sumes uniform analytes [8][9], but mixtures yield mud- dled spectra (Fig. 1A). A single-shot method is essential to isolate individual analyte spectra (Fig. 1B-E). To enhance single-shot analysis, we leverage the low- rank structure of spectra using Singular Value Decom- position (SVD), a mathematical technique for denois- ing data by identifying dominant patterns. For a noise- less spectrum z \u2208 Rl and noisy version s = z + \u03f5, we form a matrix S \u2208 Rn\u00d7l from n spectra of the same class. SVD decomposes S = U\u03a3VT, where U \u2208 Rn\u00d7n and VT \u2208 Rl\u00d7l are orthogonal matrices and \u03a3 \u2208 Rn\u00d7l is a diagonal matrix containing the singular values in descending order. The matrix S has a low-rank struc- ture, with r \u226a min (n, l), indicating that z resides within an r-dimensional subspace of Rl spanned by the first r columns of V [10]. By retaining the top r singular val- ues (with r \u226a min(n, l)) approximates the signal sub- space, filtering out noise [11]. This assumes uniform analytes, so for mixtures, we build a dictionary of denoised sub-dictionaries per an- alyte class via SVD, creating a union of subspaces as side information for feature extraction. We then employ a transformer encoder, a machine learning model adept at processing sequences like spectral peaks with posi- tional context, to generate embeddings for input spectra and dictionaries separately. This separation allows spec- tral embeddings to extract biologically relevant features from the dictionary during training, and dictionary re- moval during inference, halving parameters for faster, deployable predictions crucial for field use in biological monitoring. 2. Methods 2.1. Aerosol MALDI-MS Data Acquisition In this study, aerosol particles were ionized us- ing ultraviolet (349 nm) laser pulses in a portable MALDI-Time-of-Flight (ToF) mass spectrometer, as detailed in our prototype system [7]. Each mass spectrum is represented as an intensity vector s = [s1, s2, . . . , sl]T \u2208 Rl paired with a corresponding m z vec- tor m = [m1, m2, . . . , ml]T \u2208 Rl. For batches of n parti- cles, the spectra form a matrix: S = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 s11 s21 ... sn1 s12 s22 ... sn2 . . . . . . . . . . . . \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb s1l s2l ... snl \u2208 Rn\u00d7l . Since m is identical across measurements (unless ma- chine parameters change), a two-dimensional m matrix is unnecessary. Due to the prototype phase, our data collection is lim- ited to a handful of targets. To simulate field-expected 2 Figure 1: (A) Top: An example batch of particles containing 80% dust particulate, with the remaining 20% evenly divided among",
    "... sn2 . . . . . . . . . . . . \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb s1l s2l ... snl \u2208 Rn\u00d7l . Since m is identical across measurements (unless ma- chine parameters change), a two-dimensional m matrix is unnecessary. Due to the prototype phase, our data collection is lim- ited to a handful of targets. To simulate field-expected 2 Figure 1: (A) Top: An example batch of particles containing 80% dust particulate, with the remaining 20% evenly divided among the four biological markers. Each row represents a mass spectrum, and each column corresponds to a mass-to-charge ratio value. Bottom: The column-wise average. (B) Top: The heatmap of rows from (A) corresponding to B. globigii spectra. Bottom: the average spectrum. (C), (D), (E) same format as (A) but with E. coli, Insulin, and Ubiquitin, respectively. spectral profiles for environmental pathogen monitor- ing, we selected bacterial (multi-peak, noisy), protein (few-peak), and non-biological (peakless) types. Data included two bacteria (Bacillus globigii, Escherichia coli), two proteins (insulin, ubiquitin) as positives, and Arizona Road Dust as negative background. For safety, samples were aerosolized and collected in a lab set- ting, mimicking real-world conditions. Data acquisi- tions were performed in a class-specific manner, with m z restricted to the range [500, 10, 000] Da. Each class produces a matrix of raw spectra, S, which is split into 80% for training and 20% for testing. Consequently, our model is trained on individual raw spectra, simulat- ing single-shot detection, to ensure robust performance in field scenarios where a matrix of spectra may con- tain multiple, co-occurring classes. Table 1 summarizes spectra counts and the training/testing split. Class A.R.Dust B.globigii E.coli Insulin Ubiquitin Total Table 1: Number of Spectra per Class # of Samples Training Samples Test Samples 504 1200 1200 1120 1200 5224 630 1500 1500 1400 1500 6530 126 300 300 280 300 1306 2.2. Sparse Signal Processing A mass spectrum s can be modeled by a linear com- bination of columns, referred to as atoms, from a dictio- nary matrix D \u2208 Rl\u00d7\u03b1, such that s = Dx, where x \u2208 R\u03b1 is a sparse coefficient vector (||x||0 \u226a \u03b1). This synthe- sis model leverages sparsity to denoise and extract rel- evant features from noisy spectra, crucial for real-time 3 pathogen detection in complex aerosol signatures. Common dictionaries include orthogonal bases like Cosines, Fourier, or Wavelets (where l = \u03b1), but over-complete dictionaries ((l \u226a \u03b1)) [12] allow more flexible representations, advancing applications in compressive sensing[13], dictionary learning[14], medical imaging[15], classification[16], and object detection[17]. These approaches often solve the relaxed convex op- timization problem known as Basis Pursuit (BP)[18]: ||x||1 s.t. s = Dx min x (1) where the sparse vector, x, encodes rich task-specific information. This motivated data-driven dictionaries, as demonstrated in the Face Recognition imaging problem [19], where training samples form columns based on the principle that high-dimensional data from the same class lie in a low-dimensional subspace. A test sam- ple is thus represented as a sparse linear combination of training samples",
    "approaches often solve the relaxed convex op- timization problem known as Basis Pursuit (BP)[18]: ||x||1 s.t. s = Dx min x (1) where the sparse vector, x, encodes rich task-specific information. This motivated data-driven dictionaries, as demonstrated in the Face Recognition imaging problem [19], where training samples form columns based on the principle that high-dimensional data from the same class lie in a low-dimensional subspace. A test sam- ple is thus represented as a sparse linear combination of training samples via Sparse Representation Classifica- tion (SRC)[19]. SRC has been applied to many applica- tions such as image classification [20], denoising [21], and deep learning [22]. 2.3. Proposed Dictionary Construction Inspired by SRC, we constructed the dictionary D \u2208 Rl\u00d7\u03b1 using \u03b1 training spectra evenly distributed across c classes (\u03b1/c per class), arranged column-wise with same-class spectra grouped into sub-dictionaries Di (i = 1, . . . , c). Thus, Di = [di,1, . . . , di,\u03b1/c], and D = [D1, . . . , Dc]. This class-specific clustering exploits the low-rank structure inherent in spectra from the same biomolecu- lar class (rank r \u226a \u03b1/c). To denoise and capture essen- tial features, we applied Singular Value Decomposition (SVD) to each sub-dictionary: \u02dcDi = Ur\u03a3rVT r , r \u226a \u03b1 c , where Ur \u2208 Rl\u00d7r and Vr \u2208 R\u03b1/c\u00d7r are orthogonal matri- ces of left and right singular vectors, and \u03a3r \u2208 Rr\u00d7r con- tains the top r singular values. The denoised dictionary was then formed by stacking these low-rank approxima- tions: \u02dcD = [ \u02dcD1, \u02dcD2, . . . , \u02dcDc] \u2208 R\u03b1\u00d7l. This creates a union of rank-r subspaces that efficiently represent key biomolecular patterns in noisy spectra, en- hancing multi-label classification for airborne pathogen Figure 2: Top: Heatmap of 200 mass spectra from Bacillus globigii. Middle: A low-rank approximation via the Singular Value Decompo- sition (SVD) with rank r = 2. Bottom: The first 50 singular values from the SVD plotted on a y-axis log-scale. detection in public health scenarios. The approach\u2019s ef- ficacy is shown in Fig. 2, where SVD on Bacillus glo- bigii data reveals the first two singular values captur- ing primary features, with subsequent values as noise; a rank-2 approximation sharpens peaks and reduces noise for clearer identification. 2.4. Transformers for Time-Series and Mass Spectrom- etry Transformers, first introduced for natural language processing [23], have been extended to time-series anal- ysis, including forecasting [24, 25, 26] and classifica- tion [27, 28]. In mass spectrometry (MS), they support peptide/protein identification and protein structure pre- diction [29, 30], capitalizing on long-range dependen- cies among spectral peaks. Typically, these models use denoised, high-resolution spectra from lab instruments, with m z values as positional inputs. Recent MS advance- ments include PowerNovo [31], an ensemble of trans- former and BERT models for tandem MS peptide se- quencing, and a semi-autoregressive transformer frame- work for rapid sequencing [32]. These methods acceler- ate proteomics but often rely on preprocessed data, con- trasting our focus on raw, noisy single-shot spectra",
    "pre- diction [29, 30], capitalizing on long-range dependen- cies among spectral peaks. Typically, these models use denoised, high-resolution spectra from lab instruments, with m z values as positional inputs. Recent MS advance- ments include PowerNovo [31], an ensemble of trans- former and BERT models for tandem MS peptide se- quencing, and a semi-autoregressive transformer frame- work for rapid sequencing [32]. These methods acceler- ate proteomics but often rely on preprocessed data, con- trasting our focus on raw, noisy single-shot spectra from 4 \u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0014\u0000\u0011\u0000\u0019\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0018\u0000\u0011\u0000\u0018\u0000\u001b\u0000\u0011\u0000\u0016\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0014\u0000\u0018\u0000\u0013\u0000,\u0000R\u0000Q\u0000\u0003\u0000,\u0000Q\u0000W\u0000H\u0000Q\u0000V\u0000L\u0000W\u0000\\\u0000\u0003\u0000 \u0000P\u00009\u0000 \u0000%\u0000\u0011\u0000J\u0000O\u0000R\u0000E\u0000L\u0000J\u0000L\u0000L\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0016\u0000\u0011\u0000\u0019\u0000\u0017\u0000\u0011\u0000\u0013\u0000\u0017\u0000\u0011\u0000\u0017\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0017\u0000\u0013\u0000=\u0000R\u0000R\u0000P\u0000H\u0000G\u0000\u0003\u0000,\u0000Q\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0016\u0000\u0013\u0000\u0017\u0000\u0013\u0000\u0018\u0000\u0013\u0000,\u0000Q\u0000G\u0000H\u0000[\u0000\u0015\u0000\u00ee\u0000\u0014\u0000\u0013\u0000\u0016\u0000\u0016\u0000\u00ee\u0000\u0014\u0000\u0013\u0000\u0016\u0000\u0017\u0000\u00ee\u0000\u0014\u0000\u0013\u0000\u0016\u00006\u0000L\u0000Q\u0000J\u0000X\u0000O\u0000D\u0000U\u0000\u0003\u00009\u0000D\u0000O\u0000X\u0000H\u0000\u0003\u0000 \u0000O\u0000R\u0000J\u0000 \u00006\u0000L\u0000Q\u0000J\u0000X\u0000O\u0000D\u0000U\u0000\u0003\u00009\u0000D\u0000O\u0000X\u0000H\u0000V\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013\u0000\u0011\u0000\u0019\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0014\u0000\u0011\u0000\u0019\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0018\u0000\u0011\u0000\u0018\u0000\u001b\u0000\u0011\u0000\u0016\u0000P\u0000\u0012\u0000]\u0000\u0003\u0000 \u0000N\u0000'\u0000D\u0000 \u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0014\u0000\u0018\u0000\u0013\u0000,\u0000R\u0000Q\u0000\u0003\u0000,\u0000Q\u0000W\u0000H\u0000Q\u0000V\u0000L\u0000W\u0000\\\u0000\u0003\u0000 \u0000P\u00009\u0000 \u00005\u0000D\u0000Q\u0000N\u0000\u0003\u0000\u0015\u0000\u0003\u0000$\u0000S\u0000S\u0000U\u0000R\u0000[\u0000\u0011\u0000\u0016\u0000\u0011\u0000\u0015\u0000\u0016\u0000\u0011\u0000\u0019\u0000\u0017\u0000\u0011\u0000\u0013\u0000\u0017\u0000\u0011\u0000\u0017\u0000P\u0000\u0012\u0000]\u0000\u0003\u0000 \u0000N\u0000'\u0000D\u0000 \u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0017\u0000\u0013\u0000=\u0000R\u0000R\u0000P\u0000H\u0000G\u0000\u0003\u0000,\u0000Q\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013\u0000\u0011\u0000\u0019\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0013 forming M \u2208 RN\u00d7\u03c1. A linear projection maps these to h: Mpe = MWT + b \u2208 RN\u00d7h, with W \u2208 Rh\u00d7\u03c1 and b \u2208 Rh. Adding Mpe to spectral embeddings P integrates intensity and m z positions, al- lowing learned extrapolation to h for better biomolecu- lar pattern recognition in noisy spectra. Dictionary Embeddings. Similarly, each spectrum in the denoised dictionary \u02dcD \u2208 R\u03b1\u00d7l (Section 2.3) un- dergoes convolutional embedding, producing patch se- quences \u02dcdp i \u2208 RN\u00d7h for i = 1, . . . , \u03b1, stacked into \u02dcDp \u2208 R\u03b1\u00d7N\u00d7h. The same positional embeddings Mpe are added to consistently encode m z positions. Sepa- rate learnable weights for input and dictionary spectra distinguish noisy from denoised features, allowing tar- geted extraction of clean biomolecular signatures for improved detection accuracy. Encoder Blocks. Both input and dictionary pathways employ transformer encoder blocks based on Vaswani et al. [23], featuring multi-head self-attention, an MLP, layer normalization, and residuals for gradient stability. For the input, embeddings P (with added positional em- beddings) are projected to queries (Q), keys (K), and values (V): Q = PWQ, K = PWK, V = PWV , c with scaled dot-product attention per head and concate- nation via WO. This captures long-range spectral de- pendencies essential for noisy data. For the dictionary, embeddings \u02dcDp (with added positional embeddings) are processed per sub-dictionary \u02dcDp i \u2208 R \u03b1 c \u00d7N\u00d7h. For each sub-dictionary, a learnable sequence \u02dcdL i \u2208 R1\u00d7N\u00d7h is concatenated, the resulting tensor is permuted to +1)\u00d7h, and attention is applied slice-wise across se- RN\u00d7( \u03b1 quences at each patch position (Fig. 5). The c sequence tokens gather information globally throughout the sub- dictionary, providing aggregated side information for the input encoder\u2019s output. Keeping the input and dic- tionary encoders separate ensures denoised priors guide classification without contaminating raw signals, facili- tating precise pathogen identification for biodefense ap- plications. Selection Attention. Following encoding, the input se- quence (shape N \u00d7 1 \u00d7 h) selectively extracts features from the c aggregated sub-dictionary sequences (shape N \u00d7 c \u00d7 h) via a multi-head cross-attention layer (Fig. 6). The input acts as queries, with sub-dictionaries as keys and values, enabling class-specific feature integration Figure 3: The input spectral embedding layer creates a sequence of small overlapping patches from the mass spectrum s through one- dimensional convolution filters, transforming the 1D spectrum to 2D sequence. portable aerosol systems for real-time pathogen detec- tion in environmental health",
    "h) selectively extracts features from the c aggregated sub-dictionary sequences (shape N \u00d7 c \u00d7 h) via a multi-head cross-attention layer (Fig. 6). The input acts as queries, with sub-dictionaries as keys and values, enabling class-specific feature integration Figure 3: The input spectral embedding layer creates a sequence of small overlapping patches from the mass spectrum s through one- dimensional convolution filters, transforming the 1D spectrum to 2D sequence. portable aerosol systems for real-time pathogen detec- tion in environmental health monitoring. 2.5. Proposed MS-DGFormer Architecture The Mass Spectral Dictionary-Guided Transformer (MS-DGFormer) processes raw input spectra and de- noised dictionary spectra through separate embedding and encoding pathways, enabling robust multi-label classification of biomolecular patterns in noisy aerosol data for real-time public health monitoring (see Fig. 4 for model overview). Input Embedding. To handle the intensity vector s \u2208 Rl z values m \u2208 Rl, we adapt the and corresponding m \"patchification\" from Vision Transformers [33] for 1D spectra. Overlapping patches are extracted via 1D con- volution with kernel size \u03c1, stride \u03b3, and h output chan- nels (hidden dimension), yielding N = l\u2212\u03c1 + 1 embed- \u03b3 dings: pi, j = \u03c1\u22121(cid:88) k=0 w j,k s\u03b3i+k + b j, forming matrix P \u2208 RN\u00d7h. This convolutional method captures local peaks amid noise, outperforming linear projections by reducing edge artifacts and enhancing ro- bustness (Fig. 3). Transformer attention is permutation- invariant, focusing on pairwise token relationships with- out inherent order. Positional embeddings address this by encoding sequence positions, using methods like fixed sinusoids [23], rotary embeddings (RoPE) [34], or learnable parameters. In mass spectrometry, the m z vec- tor m provides intrinsic positional data from time-of- flight. For overlapping patches projected to dimension h, we patch m similarly: Mi = m[\u03b3(i\u22121)+1:\u03b3(i\u22121)+\u03c1] \u2208 R\u03c1, 5 Figure 4: The Mass Spectral Dictionary-Guided Transformer (MS-DGFormer) architecture. (A) Input embedding module. (B) Dictionary Embed- ding Module. (C) Selection Attention Mechanism. (D) Final peak prediction layer. (e.g., prioritizing the relevant low-rank subspace for a given pathogen). A residual connection preserves orig- inal input information, enhancing model stability and accuracy in noisy environmental samples. Peak Prediction. Known peak locations in training classes form ground-truth yi \u2208 RN, where yi, j = 1 if class i has a peak at patch j. An MLP processes the model output Pout \u2208 RN\u00d7h for binary predictions: \u02c6y = sigmoid (cid:16) ReLU(PoutW(1) + b(1))W(2) + b(2)(cid:17) , with W(1) \u2208 Rh\u00d7\u03d5, b(1) \u2208 R\u03d5, W(2) \u2208 R\u03d5\u00d71, and b(2) \u2208 R. This outputs probabilities per patch, supporting multi- label classification for rapid biomolecular threat detec- tion in aerosols. The final predicted class \u02c6c is the one with maximum cosine similarity to ground truth vectors yc for c \u2208 {1, . . . , 5}: \u02c6c = arg max c \u02c6y \u00b7 yc \u2225\u02c6y\u2225\u2225yc\u2225 . Training optimizes binary cross-entropy between \u02c6y and y, while class predictions inform evaluation metrics: ac- curacy, precision, recall, and F1 score. 3. Results 3.1. Competing Models To evaluate our model\u2019s performance, we bench- marked it against recurrent baselines",
    "biomolecular threat detec- tion in aerosols. The final predicted class \u02c6c is the one with maximum cosine similarity to ground truth vectors yc for c \u2208 {1, . . . , 5}: \u02c6c = arg max c \u02c6y \u00b7 yc \u2225\u02c6y\u2225\u2225yc\u2225 . Training optimizes binary cross-entropy between \u02c6y and y, while class predictions inform evaluation metrics: ac- curacy, precision, recall, and F1 score. 3. Results 3.1. Competing Models To evaluate our model\u2019s performance, we bench- marked it against recurrent baselines and a dictionary- ablated variant, focusing on sequence modeling for noisy mass spectra in pathogen detection. To ensure a fair comparison, we strive to maintain consistency in model parameters where possible; however, due to ar- chitectural differences, exact parameter matching is not always feasible. Recurrent Neural Network (RNN)[35], Long Short- Term Memory (LSTM)[36], and Bidirectional LSTM (biLSTM)[37] models retained our input embedding and peak prediction layers for consistent processing and output. Positional embeddings were omitted, as RNNs and LSTMs inherently capture sequential order. The dictionary, input encoder, dictionary encoder, and se- lection attention were replaced with RNN, LSTM, or biLSTM blocks. This evaluation directly compares re- transformer-based sequence handling in current vs. biomolecular classification. To assess the dictionary\u2019s impact in our model, we trained a model without the dictionary embedding, encoder, and selection attention (MS-Former), which halved the total parameter count. Essentially this model is a standard transformer. For fair comparison, we trained variants with 3 input encoder layers (MS- Former-3; 4.13M parameters) to match our core archi- tecture and 7 layers (MS-Former-7) to approximate to- tal parameters (8-9M across models), isolating the dic- tionary\u2019s role in enhancing accuracy for environmental health applications. 3.2. Model and Dictionary Hyperparameters Experiments used convolutional embedding window size \u03c1 = 100 and overlap \u03b3 = 50 (50%), yielding N = 1765 patches for spectra of length l = 88300. Cor- responding m z values were patched identically. Patches mapped to hidden dimension h = 256. Multi-head at- tention (nheads = 8, dk = 32) was applied in the in- 6 c c \u00d7N\u00d7h, where each kernel encodes temporal peak information. A learnable token sequence \u02dcdL 1 ]T are transformed into token sequences via convolution with overlapping kernels and h output channels, yielding \u02dcDp 1 Figure 5: The processing of a sub-dictionary is illustrated by exemplifying the first low-rank approximated sub-dictionary \u02dcD1 \u2208 R \u03b1 spectra [ \u02dcd1,1, . . . , \u02dcd1, \u03b1 R \u03b1 \u02dcDp 1 attention mechanism aggregates information across the \u03b1 c with contextual information, are extracted to represent the aggregated temporal information. c \u00d7l. The \u2208 1 , forming +1)\u00d7h, for attention to be computed independently across the N temporal positions. The 1 , now enriched + 1 sequences at each temporal location. Finally, the learnable tokens \u02dcdL +1)\u00d7N\u00d7h. This tensor is permuted to RN\u00d7( \u03b1 \u2208 R1\u00d7N\u00d7h is concatenated with \u02dcDp \u2208 R( \u03b1 c c overconfident in its peak predictions. 3.3. MS-DGFormer Evaluation We begin by examining the results obtained from MS-DGFormer before proceeding to a comparison with",
    "temporal information. c \u00d7l. The \u2208 1 , forming +1)\u00d7h, for attention to be computed independently across the N temporal positions. The 1 , now enriched + 1 sequences at each temporal location. Finally, the learnable tokens \u02dcdL +1)\u00d7N\u00d7h. This tensor is permuted to RN\u00d7( \u03b1 \u2208 R1\u00d7N\u00d7h is concatenated with \u02dcDp \u2208 R( \u03b1 c c overconfident in its peak predictions. 3.3. MS-DGFormer Evaluation We begin by examining the results obtained from MS-DGFormer before proceeding to a comparison with competing models. To understand the features captured in each sub-dictionary, we visualize the attention maps derived from each learnable sequence, \u02dcdL i . Figure 7 dis- plays the average attention scores, revealing that patches containing spectral peaks receive higher attention scores compared to those without. Thus, \u02dcdL i effectively focuses on the peaks found within the ith sub-dictionary. Next, to gain insight into how the model processes the raw overlapping m z values and maps them to its em- bedding space, we extract and visualize the positional embeddings. For visualization purposes, we flatten both the raw m z values and the corresponding positional em- z matrix M \u2208 R1765\u00d7100 beddings. Specifically, the raw m is flattened to a vector in R176500, and the positional embeddings P \u2208 R1765\u00d7256 are flattened to a vector in R451840. We then plot these flattened vectors, focusing on the segments corresponding to the first 10 patches, as illustrated in Fig. 8. From this figure, it is evident that the model preserves the structural characteristics of the raw m z values, such as their overlapping nature (with 50% overlap between adjacent patches), while mapping them to a higher-dimensional embedding space (from 100 to 256 dimensions per patch). Additionally, the amplitude of the positional embeddings is adjusted to a range of approximately \u00b15, aligning with the intensity scales observed in the mass spectra. To demonstrate the alignment between the positional embeddings and the Figure 6: An input spectral sequence P \u2208 RN\u00d7h is first encoded by the input encoder. Each sub-dictionary\u2019s sequences are permuted to RN\u00d7( \u03b1 +1)\u00d7h, processed by the dictionary encoder, and the respective c learnable token sequences ( \u02dcdL i ) are extracted. Multi-head attention selects dictionary features for each temporal position. put encoder\u2019s self-attention, dictionary encoder\u2019s slice attention, and selection attention. Attention MLP inter- mediate dimension was 2048; peak prediction MLP was \u03d5 = 512. Both encoders had L = 3 layers. Dictionary D comprised \u03b1 = 32 sequences from 4 positive classes (B. globigii, E. coli, insulin, ubiquitin; 8 per class), ex- cluding dust. Limited by single 4070 GPU memory, it was denoised via sub-dictionary SVD with rank r = 2, providing efficient side information for robust pathogen detection in portable systems. Our model and competing models were trained for 300 epochs with batch size of 8, a learning rate of 10\u22124 with 10% warm-up, and a cosine annealing de- cay. There were two types of regularization imple- mented: neuron dropouts placed similarly to [23] with 0.1 dropout probability, and binary cross-entropy",
    "cluding dust. Limited by single 4070 GPU memory, it was denoised via sub-dictionary SVD with rank r = 2, providing efficient side information for robust pathogen detection in portable systems. Our model and competing models were trained for 300 epochs with batch size of 8, a learning rate of 10\u22124 with 10% warm-up, and a cosine annealing de- cay. There were two types of regularization imple- mented: neuron dropouts placed similarly to [23] with 0.1 dropout probability, and binary cross-entropy label smoothing. This type of label smoothing treats each 1 as 0.9 and each 0 as 0.1, so the model does not become 7 Figure 7: Each class\u2019s sub-dictionary attention maps averaged across heads are shown. The larger attention scores are located at each class\u2019s true peak locations showing the dictionary\u2019s efficacy for feature ex- traction. Figure 8: Top: The overlapping patches of m z values are flattened to RN\u03c1 (left). M is embedded via a learnable linear layer producing Mpe (right). Middle: The first 10 patches of M and Mpe. Bottom: The patches are normalized and plotted on the same linspace. original m z values, we normalize both to a common am- plitude range and plot them on a uniform linear space. Fig. 9 illustrates this alignment for each patch, high- lighting the model\u2019s ability to encode positional infor- mation effectively. Finally, we visualize the attention maps within the selection attention head to understand how the model selects features from the sub-dictionaries based on the input spectrum class. To achieve this, we input a repre- sentative bacteria, protein, and noise spectrum into the model and extract the attention map from the selection attention head for visualization. Fig. 9 shows the at- tention scores across the sub-dictionaries for each input spectrum. From this figure, it is evident that for each input spectrum, the attention scores are significantly higher for the sub-dictionary corresponding to the same class. However, for the Arizona Road Dust class, the attention scores are more dispersed and lower in mag- nitude. This behavior is expected because the Arizona Road Dust spectra primarily contain noise peaks, which do not align well with the denoised spectra represented in the sub-dictionaries. test set, with performance measured using micro and macro accuracy, precision, recall, and F1-score. Table III presents the macro metrics, where MS-DGFormer achieves the highest scores across all metrics, despite having fewer parameters than most other models, with the exception of MS-Former-3. Notably, the biLSTM- based model outperforms MS-Former-7 but still falls short of MS-DGFormer\u2019s performance. For brevity, we focus on the micro F1-score in our analysis, as it effectively balances the trade-off between false positives and false negatives, which is critical for classification tasks. Table IV displays the micro F1- scores for each class, with MS-DGFormer consistently achieving the highest F1-score across all classes. In contrast, the other models exhibit significant perfor- mance degradation on the Arizona Road Dust class. This is particularly concerning because dust-like parti- cles are commonly encountered in environmental con- ditions, and misclassifying them",
    "focus on the micro F1-score in our analysis, as it effectively balances the trade-off between false positives and false negatives, which is critical for classification tasks. Table IV displays the micro F1- scores for each class, with MS-DGFormer consistently achieving the highest F1-score across all classes. In contrast, the other models exhibit significant perfor- mance degradation on the Arizona Road Dust class. This is particularly concerning because dust-like parti- cles are commonly encountered in environmental con- ditions, and misclassifying them as biological agents could result in a high rate of false alarms. 3.5. Computational Efficiency 3.4. Competing Model Comparisons Having demonstrated the spectral features captured by MS-DGFormer, we now proceed to evaluate its performance against competing models using standard classification metrics. Each model is assessed on the Real-time field analysis demands rapid processing of continuous spectral streams, where both parameter count and hardware requirements are critical. Our de- sign improves efficiency by separating the dictionary embedding/encoder from the input embedding/encoder. 8 \u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL1\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL2\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL3\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0017\u0000\u0019dL4\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0013\u0000\u0013\u0000/\u0000H\u0000D\u0000U\u0000Q\u0000H\u0000G\u0000\u0003\u0000'\u0000L\u0000F\u0000W\u0000L\u0000R\u0000Q\u0000D\u0000U\u0000\\\u0000\u0003\u00006\u0000H\u0000T\u0000X\u0000H\u0000Q\u0000F\u0000H\u0000V\u0000 \u0000\u0003\u0000$\u0000W\u0000W\u0000H\u0000Q\u0000W\u0000L\u0000R\u0000Q\u0000\u0003\u00000\u0000D\u0000S\u0000V\u00006\u0000X\u0000E\u0000\u0010\u0000'\u0000L\u0000F\u0000W\u0000L\u0000R\u0000Q\u0000D\u0000U\u0000\\\u0000\u0003\u00006\u0000H\u0000T\u0000X\u0000H\u0000Q\u0000F\u0000H\u0000\u0003\u00001\u0000X\u0000P\u0000E\u0000H\u0000U\u00003\u0000D\u0000W\u0000F\u0000K\u0000\u0003\u00001\u0000X\u0000P\u0000E\u0000H\u0000UM1M500M1000M1500\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0018\u0000\u0014\u0000\u0011\u0000\u0013\u00001\u0000R\u0000U\u0000P\u0000D\u0000O\u0000L\u0000]\u0000H\u0000G\u0000\u0003mz\u0000)\u0000O\u0000D\u0000W\u0000W\u0000H\u0000Q\u0000H\u0000G\u0000\u0003M\u0000\u0003\u0000 \u0000R\u0000Y\u0000H\u0000U\u0000O\u0000D\u0000S\u0000S\u0000L\u0000Q\u0000J\u0000\u0003mz\u0000 Mpe1Mpe500Mpe1000Mpe1500\u0000\u0018\u0000\u0013\u0000\u0018\u0000(\u0000P\u0000E\u0000H\u0000G\u0000G\u0000H\u0000G\u0000\u0003mz\u0000)\u0000O\u0000D\u0000W\u0000W\u0000H\u0000Q\u0000H\u0000G\u0000\u0003Mpe\u0000\u0003\u0000 \u00003\u0000R\u0000V\u0000\u0011\u0000\u0003\u0000(\u0000P\u0000E\u0000H\u0000G\u0000\u0011\u0000 M1M3M5M7M9\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0015\u0000)\u0000L\u0000U\u0000V\u0000W\u0000\u0003\u0000\u0014\u0000\u0013\u0000\u0003\u0000S\u0000D\u0000W\u0000F\u0000K\u0000H\u0000VMpe1Mpe3Mpe5Mpe7Mpe9\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0014\u0000)\u0000L\u0000U\u0000V\u0000W\u0000\u0003\u0000\u0014\u0000\u0013\u0000\u0003\u0000O\u0000H\u0000D\u0000U\u0000Q\u0000H\u0000G\u0000\u0003\u0000S\u0000D\u0000W\u0000F\u0000K\u0000H\u0000V\u0000\u0014\u0000\u0016\u0000\u0018\u0000\u001a\u0000 mz\u0000\u0003\u00003\u0000D\u0000W\u0000F\u0000K\u0000H\u0000V\u0000\u0003\u0000I\u0000R\u0000U\u0000\u0003M\u0000\u0003\u0000D\u0000Q\u0000G\u0000\u0003Mpe\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0015\u00001\u0000R\u0000U\u0000P\u0000D\u0000O\u0000L\u0000]\u0000H\u0000G\u0000\u0003\u0000D\u0000Q\u0000G\u0000\u0003\u0000$\u0000O\u0000L\u0000J\u0000Q\u0000H\u0000G\u0000\u0003\u0000R\u0000Q\u0000\u0003\u00008\u0000Q\u0000L\u0000I\u0000R\u0000U\u0000P\u0000\u0003\u0000/\u0000L\u0000Q\u0000V\u0000S\u0000D\u0000F\u0000HMpe1:10M1:10 Table 2: Macro Metrics Across All Classes. Model RNN-6 LSTM-4 BiLSTM-6 MS-Former-3 MS-Former-7 MS-DGFormer Accuracy 0.560 0.679 0.939 0.709 0.862 0.983 Macro Metrics Precision Recall 0.560 0.679 0.939 0.709 0.862 0.983 0.832 0.821 0.916 0.845 0.876 0.982 F1 0.491 0.641 0.915 0.664 0.824 0.982 Params 9.52M 9.50M 9.48M 4.13M 9.39M 8.36M Model RNN-6 LSTM-4 BiLSTM-6 MS-Former-3 MS-Former-7 MS-DGFormer Table 3: Micro F1 Scores For Each Class A.R.Dust B.globigii E.coli Insulin Ubiq. 0.278 0.331 0.736 0.369 0.553 0.949 0.045 0.623 0.926 0.328 0.666 0.991 0.408 0.374 0.954 0.880 0.984 0.979 0.795 0.906 0.976 0.787 0.921 0.987 0.926 0.972 0.983 0.952 0.994 0.994 Figure 9: Top: E.coli input spectrum and the attention map from the selection attention mechanism. The attention scores are larger for E.coli, capturing the learning of the selection mechanism. Middle: Ubiquitin input spectrum and its corresponding selection attention map. Bottom: Arizona Road Dust and its corresponding selection attention map showing no dominant features selected from a single class. the dictionary The dictionary sequences remain constant during train- ing and are encoded independently of the input spec- trum. Only the learned sequence per sub-dictionary is used: \u03b1 sequences enter the dictionary encoder during training, but only c sequences are passed to the selection attention head. Because spectrum- independent, we precompute and store the c learned sub-dictionary sequences, removing the dictionary embedding layer, dictionary encoder, and dictionary itself from the inference model. This yields an efficient variant, MS-DGFormer-E, where only the pre-trained weights of the remaining components are loaded. The c sequences are stored in memory and fed directly into the selection attention head, cutting parameters from 8.36 \u00d7 106 to 4.39 \u00d7 106 without performance loss. input is This architecture also scales efficiently. Adding more sequences to sub-dictionaries during training does not affect inference time, as the number of sequences used at inference, c, remains fixed. If \u03b8 new classes are c \u03b8 + \u03b1 se- added, the training dictionary expands to \u03b1 quences, yet inference still requires only c+\u03b8 sequences, with",
    "stored in memory and fed directly into the selection attention head, cutting parameters from 8.36 \u00d7 106 to 4.39 \u00d7 106 without performance loss. input is This architecture also scales efficiently. Adding more sequences to sub-dictionaries during training does not affect inference time, as the number of sequences used at inference, c, remains fixed. If \u03b8 new classes are c \u03b8 + \u03b1 se- added, the training dictionary expands to \u03b1 quences, yet inference still requires only c+\u03b8 sequences, with c \u226a \u03b1 c . We evaluated inference speed and spectral throughput on batches of size 1, 4, and 8, averaging 100 runs after 10 warm-up runs. As shown in Table V, MS-DGFormer- E achieves nearly a 2\u00d7 increase in mean inference speed and more than a 2\u00d7 increase in throughput over the full MS-DGFormer. The only model with comparable infer- ence time is MS-Former-3, due to its similar parameter count, but its classification performance is significantly lower. While results are hardware-specific, the relative efficiency gains should generalize across platforms. 4. Discussion We have introduced an approach capable of classify- ing individual noisy mass spectra without the need to collect several spectra of the same class to create an averaged spectrum. The mass spectra are turned into sequences of small overlapping patches enabling atten- tion mechanisms to capture peak locations. Although, this alone is not enough to accurately classify the spec- tral input due to intense noise. We notice that if we do have a batch of spectra all from the same class, the SVD is able to significantly reduce noise and reveal true spectral peaks. However, it is unlikely this will nat- urally occur in the environment. We construct a dic- tionary composed of training samples from each class, 9 Model Mean (ms) \u2193 RNN-6 LSTM-4 BiLSTM-6 MS-Former-3 MS-Former-7 MS-DGFormer MS-DGFormer-E 125.32 108.62 156.38 11.88 23.46 72.27 12.31 Table 4: Inference Performance Metrics for Different Models and Batch Sizes Batch Size 1 Std (ms) \u2193 Spectra/s \u2191 Mean (ms) \u2193 Batch Size 4 Std (ms) \u2193 Spectra/s \u2191 Mean (ms) \u2193 Batch Size 8 Std (ms) \u2193 Spectra/s \u2191 3.36 3.31 2.56 2.55 2.32 3.76 1.67 7.98 9.21 6.39 84.15 42.63 13.84 81.23 156.30 116.619 263.82 34.70 74.31 95.66 35.98 4.03 4.79 1.46 3.31 4.18 2.85 3.70 25.59 34.30 15.16 115.27 53.83 41.81 111.16 221.37 194.51 223.67 62.87 141.19 127.17 66.33 5.94 6.79 4.18 4.91 3.08 2.58 3.92 36.14 41.13 35.77 127.25 56.66 62.91 120.60 clustered together to resemble sub-dictionaries, and per- form the SVD on the sub-dictionaries to be used as side- information. One learnable (randomly initialized) se- quence is concatenated to each sub-dictionary to capture features for their respective sub-dictionary for both ef- fectiveness and efficiency. Further, our model processes the input spectrum and dictionary through separate en- coders allowing the features to be learned indepen- dently, with another attention head capable of selecting specific information from the dictionary. In this man- ner, only the learned dictionary sequences are required for inference, and the dictionary components can",
    "be used as side- information. One learnable (randomly initialized) se- quence is concatenated to each sub-dictionary to capture features for their respective sub-dictionary for both ef- fectiveness and efficiency. Further, our model processes the input spectrum and dictionary through separate en- coders allowing the features to be learned indepen- dently, with another attention head capable of selecting specific information from the dictionary. In this man- ner, only the learned dictionary sequences are required for inference, and the dictionary components can be re- moved from the model. This significantly improves in- ference speed and spectral throughput since nearly half of the model parameters reside in the dictionary compo- nents. Finally, our MS-DGFormer is compared against competing sequential models and achieves the highest micro and macro classification metrics with drastically faster inference metrics. Overall, our proposed architecture, MS-DGFormer, addresses several challenges that arise when transition- ing MALDI-MS systems from laboratory to environ- mental settings. Traditional MALDI-MS workflows re- quire extensive sample preparation and pre-processing; our approach eliminates much of this burden through autonomous aerosol sampling, shifting the workload to computational post-processing and machine learning inference. Rather than relying on multi-shot spectral averaging to obtain clean spectra, we leverage SVD- denoised sub-dictionaries to provide rich side informa- tion for accurate classification of raw, minimally pro- cessed spectra. False positive classification is a particular challenge in environmental sampling, where negative classes such as dust are abundant. MS-DGFormer demonstrates ro- bust performance in these scenarios, effectively mitigat- ing misclassification. Our results highlight real-time pathogen detection using portable aerosol MALDI-MS the potential of systems, paving the way for field-deployable solutions that can transform environmental monitoring, biological threat detection, and efforts to reduce disease spread. Declaration of competing interest W.B., and M.M. have competing interests. W.B. is the President and CEO of Zeteo Tech, Inc. M.M. is the Vice President of Research and CTO at Zeteo Tech, Inc. Acknowledgments This work was supported in part by the National In- stitute of Health under award number T32GM142603 and in part by Zeteo Tech Inc. Due to privacy or ethi- cal restrictions, the data from the study is only available upon request from the corresponding author. Code is available upon request from the corresponding author. Author Contributions Conceptualization, K.R., G.A., M.M. and W.B; methodology, K.R., G.A., M.M. and W.B; software, K.R; validation, G.A., M.M., W.B.; formal analysis, K.R., G.A.; investigation, K.R., G.A., M.M. and W.B; resources, K.R., G.A., M.M. and W.B; data curation, K.R, M.M, W.B; writing\u2014original draft preparation, K.R., G.A; writing\u2014review and editing, G.A., M.M. and W.B.; visualization, K.R.; supervision, G.A., M.M. and W.B.; project administration, G.A., M.M. and W.B.; funding acquisition, G.A., M.M. and W.B. All authors have read and agreed to the published version of the manuscript. Declaration of generative AI and AI-assisted tech- nologies in the manuscript preparation process During the preparation of this work the author(s) used Grok xAI in order for grammar correction and readabil- ity. After using this tool/service, the author(s) reviewed 10 and edited the content as needed and take(s) full",
    "K.R.; supervision, G.A., M.M. and W.B.; project administration, G.A., M.M. and W.B.; funding acquisition, G.A., M.M. and W.B. All authors have read and agreed to the published version of the manuscript. Declaration of generative AI and AI-assisted tech- nologies in the manuscript preparation process During the preparation of this work the author(s) used Grok xAI in order for grammar correction and readabil- ity. After using this tool/service, the author(s) reviewed 10 and edited the content as needed and take(s) full respon- sibility for the content of the published article. References [1] A. El-Aneed, A. Cohen, J. Banoub, Mass spec- trometry, review of the basics: Electrospray, maldi, and commonly used mass analyzers, Ap- plied Spectroscopy Reviews 44 (3) (2009) 210\u2013 230. doi:10.1080/05704920902717872. [2] W. C. Wiley, I. H. McLaren, Time-of-flight mass spectrometer with improved resolution, Review of Scientific Instruments 26 (12) (2004) 1150\u20131157. doi:10.1063/1.1715212. [3] W. A. Bryden, R. C. Benson, H. W. Ko, C. Fense- lau, R. J. Cotter, Tiny-tof mass spectrome- in: P. J. Stopa, M. A. ter for biodetection, Bartoszcze (Eds.), Rapid Methods for Analy- sis of Biological Materials in the Environment, Springer Netherlands, Dordrecht, 2000, pp. 101\u2013 110. doi:10.1007/978-94-015-9534-6_10. [4] R. J. Cotter, Time-of-flight mass spectrometry, in: Electrospray and MALDI Mass Spectrome- try, John Wiley & Sons, Ltd, 2010, pp. 345\u2013364. doi:10.1002/9780470588901.ch10. desorption/ionisation [5] A. L. van Wuijckhuijse, M. A. Stowers, W. A. Kleefsman, B. L. M. van Baar, C. E. Kientz, J. C. M. Marijnissen, Matrix-assisted laser time-of- flight mass spectrometry for the analysis of bioaerosols: development of a fast detector for airborne biological pathogens, Journal of Aerosol Science 36 (5-6) (2005) 677\u2013687. doi:10.1016/j.jaerosci.2004.11.003. aerosol pathogen [6] C. Papagiannopoulou, R. Parchen, P. Rubbens, identifica- Fast W. Waegeman, tion laser using desorption/ionization-aerosol time-of-flight mass spectrometry data and deep learning methods, Analytical Chemistry 92 (11) (2020) 7523\u20137531. doi:10.1021/acs.analchem.9b05806. single-cell matrix-assisted [7] D. Chen, W. A. Bryden, M. McLoughlin, S. A. Ecelberger, T. J. Cornish, L. P. Moore, K. M. Re- gan, digitalmaldi: A single-particle-based mass spectrometric detection system for biomolecules, Journal of Mass Spectrometry 60 (2) (2025) e5110. doi:10.1002/jms.5110. 11 [8] M. McLoughlin, G. Arce, Deterministic prop- the recursive separable median fil- erties of ter, IEEE Transactions on Acoustics, Speech, and Signal Processing 35 (1) (1987) 98\u2013106. doi:10.1109/TASSP.1987.1165026. [9] G. R. Arce, N. C. Gallagher, T. A. Nodes, Median filters: Theory for one- and two-dimensional fil- ters, in: T. S. Huang (Ed.), Advances in Computer Vision and Image Processing, Vol. 2, JAI Press, Greenwich, CT, 1986, pp. 89\u2013166. [10] J. A. Fessler, R. R. Nadakuditi, Linear Algebra for Data Science, Machine Learning, and Signal Pro- cessing, Cambridge University Press, 2024. [11] C. Eckart, G. Young, The approximation of one matrix by another of lower rank, Psychometrika 1 (3) (1936) 211\u2013218. [12] S. Mallat, Z. H. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing 41 (12) (1993) 3391\u20133401. doi:10.1109/78.258082. [13] D. L. Donoho, Compressed sensing, IEEE Trans- actions on Information Theory 52 (4) (2006) 1289\u20131306. [14] M. Aharon, M. Elad, A. B. Malach, K-svd: An algorithm",
    "Data Science, Machine Learning, and Signal Pro- cessing, Cambridge University Press, 2024. [11] C. Eckart, G. Young, The approximation of one matrix by another of lower rank, Psychometrika 1 (3) (1936) 211\u2013218. [12] S. Mallat, Z. H. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing 41 (12) (1993) 3391\u20133401. doi:10.1109/78.258082. [13] D. L. Donoho, Compressed sensing, IEEE Trans- actions on Information Theory 52 (4) (2006) 1289\u20131306. [14] M. Aharon, M. Elad, A. B. Malach, K-svd: An algorithm for designing overcomplete dictionar- ies for sparse representation, IEEE Transactions on Signal Processing 54 (11) (2006) 4311\u20134322. doi:10.1109/TSP.2006.881199. [15] S. Ravishankar, Y. Bresler, Mr image recon- struction from highly undersampled k-space data IEEE Transactions on by dictionary learning, Medical (2011) 1028\u20131041. doi:10.1109/TMI.2010.2090538. Imaging 30 (5) [16] M. Yamac, M. Ahishali, A. Degerli, S. Ki- ranyaz, M. E. H. Chowdhury, M. Gabbouj, sparse support estimator-based Convolutional covid-19 images, from x-ray IEEE Transactions on Neural Networks and Learning Systems 32 (5) (2021) 1810\u20131820. doi:10.1109/TNNLS.2021.3070467. recognition [17] M. Ahishali, M. Yamac, S. Kiranyaz, M. Gab- bouj, Representation based regression for object distance estimation, Neural Networks 158 (2023) 15\u201329. doi:10.1016/j.neunet.2022.11.011. URL https://www.sciencedirect.com/science/article/pii/S089360802200452X [18] S. Chen, D. L. Donoho, M. A. Saunders, Atomic decomposition by basis pursuit, SIAM Journal on Scientific Computing 20 (1) (2001) 33\u201361. doi:10.1137/S1064827500394074. [28] J. Xu, H. Wu, J. Wang, M. Long, Anomaly trans- former: Time series anomaly detection with asso- ciation discrepancy (2022). URL https://arxiv.org/abs/2110.02642 [19] J. Wright, A. Y. Yang, A. Ganesh, S. Sas- try, Y. Ma, Sparse representation for com- puter vision and pattern recognition, Proceed- ings of the IEEE 98 (6) (2009) 1031\u20131044. doi:10.1109/JPROC.2009.2015716. [20] J. Yang, K. Yu, Y. Gong, T. Huang, Linear spatial pyramid matching using sparse coding for image classification, in: 2009 IEEE Conference on Com- puter Vision and Pattern Recognition, 2009, pp. 1794\u20131801. doi:10.1109/CVPR.2009.5206757. [21] J.-L. Starck, E. Candes, D. Donoho, The curvelet transform for image denoising, IEEE Transac- tions on Image Processing 11 (6) (2002) 670\u2013684. doi:10.1109/TIP.2002.1014998. [22] W. Wen, C. Wu, Y. Wang, Y. Chen, H. Li, Learning structured sparsity in deep neural networks (2016). URL https://arxiv.org/abs/1608.03665 [23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, in: Advances in Neu- ral Information Processing Systems (NeurIPS), Vol. 30, Curran Associates, Inc., 2017, pp. 5998\u2013 6008. URL https://arxiv.org/abs/1706.03762 [24] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, W. Zhang, Informer: Beyond efficient transformer for long sequence time-series fore- casting (2021). URL https://arxiv.org/abs/2012.07436 [25] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, R. Jin, Fedformer: Frequency enhanced decom- posed transformer for long-term series forecasting (2022). URL https://arxiv.org/abs/2201.12740 [26] A. Garza, C. Challu, M. Mergenthaler-Canseco, Timegpt-1 (2024). URL https://arxiv.org/abs/2310.03589 [27] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidi- paty, C. Eickhoff, A transformer-based framework for multivariate time series representation learning (2020). URL https://arxiv.org/abs/2010.02803 [29] M. Ekvall, P. Truong, W. Gabriel, M. Wil- transformer: A trans- for prediction of ms2 spectrum in- Proteome Research (2022) 1359\u20131364,",
    "Z. Ma, Q. Wen, X. Wang, L. Sun, R. Jin, Fedformer: Frequency enhanced decom- posed transformer for long-term series forecasting (2022). URL https://arxiv.org/abs/2201.12740 [26] A. Garza, C. Challu, M. Mergenthaler-Canseco, Timegpt-1 (2024). URL https://arxiv.org/abs/2310.03589 [27] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidi- paty, C. Eickhoff, A transformer-based framework for multivariate time series representation learning (2020). URL https://arxiv.org/abs/2010.02803 [29] M. Ekvall, P. Truong, W. Gabriel, M. Wil- transformer: A trans- for prediction of ms2 spectrum in- Proteome Research (2022) 1359\u20131364, pMID: 35413196. helm, L. K\"all, Prosit former tensities, 21 (5) doi:10.1021/acs.jproteome.1c00870. URL https://doi.org/10.1021/acs.jproteome.1c00870 Journal of [30] M. Yilmaz, W. E. Fondrie, W. Bittremieux, W. S. Noble, Sequence-to-sequence translation from mass spectra to peptides with a transformer model, Nature Communications 15 (2024) 6427. doi:10.1038/s41467-024-49731-x. URL https://www.nature.com/articles/s41467-024-49731-x [31] D. V. Petrovskiy, et al., Powernovo: de novo pep- tide sequencing via tandem mass spectrometry us- ing an ensemble of transformer and bert models, Scientific Reports 14 (2024) 15000. [32] Y. Zhao, S. Wang, J. Huang, B. Meng, D. An, X. Fang, Y. Wei, X. Dai, A transformer-based semi-autoregressive framework for high-speed sequencing, and accurate de novo peptide Communications Biology 8 (1) (2025) 234. doi:10.1038/s42003-025-07584-0. URL https://doi.org/10.1038/s42003-025-07584-0 [33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis- senborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint (2021). URL https://doi.org/10.48550/arXiv.2010.11929 [34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, Roformer: Enhanced transformer with rotary po- sition embedding (2023). URL https://arxiv.org/abs/2104.09864 [35] J. L. Elman, Finding Cognitive Science 14 (2) doi:10.1207/s15516709cog1402_1. structure in time, (1990) 179\u2013211. [36] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Computation 9 (8) (1997) 1735\u2013 1780. doi:10.1162/neco.1997.9.8.1735. 12 [37] M. Schuster, K. Paliwal, Bidirectional recur- IEEE Transactions on rent neural networks, Signal Processing 45 (11) (1997) 2673\u20132681. doi:10.1109/78.650093. 13"
  ]
}