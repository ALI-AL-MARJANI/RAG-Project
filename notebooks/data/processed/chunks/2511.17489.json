{
  "id": "2511.17489",
  "chunks": [
    "5 2 0 2 v o N 1 2 ] G L . s c [ 1 v 9 8 4 7 1 . 1 1 5 2 : v i X r a 1\u201330 Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization Vinay Kanakeri Department of Electrical and Computer Engineering, North Carolina State University VKANAKE@NCSU.EDU Shivam Bajaj The Elmore Family School of Electrical and Computer Engineering, Purdue University BAJAJ41@PURDUE.EDU Ashwin Verma The Elmore Family School of Electrical and Computer Engineering, Purdue University VERMA240@PURDUE.EDU Vijay Gupta The Elmore Family School of Electrical and Computer Engineering, Purdue University GUPTA869@PURDUE.EDU Aritra Mitra Department of Electrical and Computer Engineering, North Carolina State University AMITRA2@NCSU.EDU Abstract It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it has been proposed that the learning algorithm utilize data from \u2018approximately similar\u2019 processes. However, since the process models are unknown, identifying which other processes are similar poses a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding to a copy of a linear process to be controlled. The agents\u2019 local processes can be partitioned into clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable notion of cluster separation that captures differences in closed-loop performance across systems, we prove that our approach guarantees correct clustering with high probability. Furthermore, we show that the sub-optimality gap of the policy learned for each cluster scales inversely with the size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based control. Our work is the first to reveal how clustering can be used in data-driven control to learn personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality due to inclusion of data from dissimilar processes. From a distributed implementation perspective, our method is attractive as it incurs only a mild logarithmic communication overhead. Keywords: Policy gradients for LQR; Collaborative Learning; Transfer/Multi-Task Learning. 1. Introduction The last decade or so has seen a surge of interest in model-free data-driven control (Hu et al., 2023), where control laws (policies) are learned directly from data, bypassing the need to estimate the system model as an intermediate step. Although such a framework is promising, it relies on the availability of adequate data to learn high-precision policies. Unfortunately, however, data from physical processes (such as real-world robotic environments) could be scarce and/or difficult to collect. Drawing inspiration from popular paradigms such as federated and meta-learning, some recent papers (Zhang et al., 2023; Wang et al., 2023a; Toso et al., 2024) have attempted to mitigate this challenge by exploring the idea of combining information generated by multiple environments, where each environment represents a dynamical system with an associated cost performance metric",
    "of adequate data to learn high-precision policies. Unfortunately, however, data from physical processes (such as real-world robotic environments) could be scarce and/or difficult to collect. Drawing inspiration from popular paradigms such as federated and meta-learning, some recent papers (Zhang et al., 2023; Wang et al., 2023a; Toso et al., 2024) have attempted to mitigate this challenge by exploring the idea of combining information generated by multiple environments, where each environment represents a dynamical system with an associated cost performance metric \u00a9 V. Kanakeri, S. Bajaj, A. Verma, V. Gupta & A. Mitra. KANAKERI BAJAJ VERMA GUPTA MITRA that captures a task or a goal. The unifying theme in such papers is to learn a single common policy that performs well across all environments by minimizing an average-cost performance metric. When environments differ considerably in their tasks, such a single common policy might incur highly sub-optimal performance on any given environment. More fundamentally, when environments differ in dynamics, even the existence of a common stabilizing policy is unclear and difficult to verify in the absence of models. Departing from the approach of learning a single common policy, in this paper, we ask: (When) is it possible to learn personalized policies in a sample-efficient way by leveraging data generated by potentially non-identical dynamical processes? To formalize our study, we consider a scenario involving multiple agents that can be partitioned into distinct clusters. We assume that all agents within a given cluster interact with the same physical environment modeled as a linear time-variant (LTI) system with unknown dynamics; furthermore, all agents within a cluster share the same quadratic cost function. However, the dynamics and cost functions across clusters can be arbitrarily different. Thus, our setting captures both similarities and differences in dynamics and tasks. As is common in collaborative and federated learning, we allow agents to exchange information via a central aggregator. Concretely, our problem of interest is to learn a personalized policy for each cluster that enjoys the benefits of collaboration, i.e., we wish to show that such a policy can be learned faster (relative to a single-agent setting) by using the collective samples available within the cluster. However, this is challenging, as we explain below. Challenges. To make our setting realistic, we assume that the cluster structure is unknown a priori. Since the system models associated with the clusters are also unknown, it becomes difficult to decide how information should be exchanged between agents. In particular, care needs to be taken to avoid misclustering, since transfer of information across clusters with arbitrarily different LTI systems can lead to the learning of destabilizing policies; thus, in our setting, more data can potentially hurt if not used judiciously. Additional subtleties arise as the agents in our setting access only noisy zeroth-order information for both clustering and learning policies; we discuss them in Sections 2 and 3. In light of these challenges, the main contributions of this paper are as follows. \u2022 Problem Formulation. While clustering has been explored in federated learning (FL) for",
    "arbitrarily different LTI systems can lead to the learning of destabilizing policies; thus, in our setting, more data can potentially hurt if not used judiciously. Additional subtleties arise as the agents in our setting access only noisy zeroth-order information for both clustering and learning policies; we discuss them in Sections 2 and 3. In light of these challenges, the main contributions of this paper are as follows. \u2022 Problem Formulation. While clustering has been explored in federated learning (FL) for static supervised learning tasks (Ghosh et al., 2020), our work provides the first principled study of clustering in the context of model-free data-driven control, and shows how such a formalism can enable learning personalized policies in a sample-efficient manner. As part of our formulation, we identify a dissimilarity metric \u2206 (see (3)) that captures differences in optimal costs between clusters. Our results reveal that a larger value of \u2206 leads to a faster separation of clusters. \u2022 Novel Algorithm. The primary contribution of this paper is the development of a novel model-free Personalized and Collaborative Policy Optimization (PO) algorithm (Algorithm 1) called PCPO that combines ideas from sequential elimination in multi-armed bandits (Even-Dar et al., 2006) and policy gradient algorithms in reinforcement learning (RL) (Agarwal et al., 2021). The lack of prior knowledge of the cluster-separation gap \u2206 motivates the need for a sequential elimination strategy to identify the clusters. Moreover, since it is non-trivial to decide when to stop clustering and start collaborating, we propose an epoch-based approach that involves clustering and collaboration in every epoch by requiring agents to maintain two separate sequences of policies: local policies used purely for sequential clustering and global policies for collaboration. Another key feature of our algorithm is that it only incurs a mild communication cost that scales logarithmically with the number of agents and samples, making it particularly appealing for distributed implementation. \u2022 Collaborative Gains. In Theorem 1, we prove that with high probability, our approach leads to correct clustering, despite the absence of prior knowledge of models, cluster structure, and separation 2 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS gap \u2206. This result also reveals that more heterogeneity can actually aid the learning process in that a larger \u2206 incurs fewer noisy function evaluations for correct clustering. Building on Theorem 1, our main result in Theorem 2 proves that by using PCPO, each agent can learn a near-optimal personalized policy for its own system with a sub-optimality gap that scales inversely with the number of agents in its cluster. In other words, PCPO prevents negative transfer of information across clusters, while ensuring sample-complexity reductions via collaboration within each cluster. To our knowledge, this is the first result to show how data from heterogeneous dynamical systems admitting a cluster structure can be harnessed to expedite the learning of personalized policies. Since this is a preliminary investigation of clustering in data-driven control, we restrict our attention to the canonical Linear Quadratic Regulator (LQR) formalism. That said, we anticipate that our general algorithmic template",
    "words, PCPO prevents negative transfer of information across clusters, while ensuring sample-complexity reductions via collaboration within each cluster. To our knowledge, this is the first result to show how data from heterogeneous dynamical systems admitting a cluster structure can be harnessed to expedite the learning of personalized policies. Since this is a preliminary investigation of clustering in data-driven control, we restrict our attention to the canonical Linear Quadratic Regulator (LQR) formalism. That said, we anticipate that our general algorithmic template can be used in other supervised or RL problems. Related Work. To put our contributions into perspective, we discuss relevant literature below. \u2022 Policy Gradient for LQR. We build on the rich set of results on policy gradient methods for the LQR problem (Fazel et al., 2018; Malik et al., 2020; Gravell et al., 2020; Zhang et al., 2021; Mohammadi et al., 2019, 2020; Hu et al., 2023; Moghaddam et al., 2025). Generalizing these results from the single system setting to our clustered multi-system formulation introduces various nuances and challenges (outlined in Sections 2 and 3) that we address in this paper. \u2022 Personalized Federated Learning. We draw inspiration from the work on clustering in FL (Ghosh et al., 2020, 2022; Sattler et al., 2020) that aims to learn personalized models for groups of agents that are similar in terms of their data distributions. Despite cosmetic similarities, the specifics of our setting differ significantly in that we focus on control of dynamical systems where stability plays a crucial role; no such stability concerns arise in the FL papers above on supervised learning. \u2022 Collaborative System Identification. Our paper is related to a growing body of work that seeks to leverage data from multiple dynamical systems to achieve statistical gains in estimation accuracy. In this context, several papers (Wang et al., 2023b; Toso et al., 2023; Chen et al., 2023; Modi et al., 2024; Rui and Dahleh, 2025; Tupe et al., 2025; Xin et al., 2025) have explored collaborative system identification by combining trajectory data from multiple systems that share structural similarities. In particular, Toso et al. (2023) and Rui and Dahleh (2025) assume a cluster structure like us. While the above papers focus on using collective data for an open-loop estimation problem, namely system- identification, our work focuses instead on data-efficient closed-loop control by directly learning policies. As such, our notion of heterogeneity captures differences in closed-loop performance across clusters as opposed to similarity metrics imposed on open-loop system matrices in the papers above. \u2022 Meta, Multi-Task, and Transfer Learning in Control. Under the umbrella framework of meta and transfer learning, various recent papers (Wang et al., 2023a; Toso et al., 2024; Aravind et al., 2024; Stamouli et al., 2025) have used PO methods to study how information from multiple LTI systems can be aggregated to learn policies that adapt across similar systems. Our formulation, which seeks to find a personalized policy for every system, departs fundamentally from this line of work which instead aims to learn a common policy",
    "in Control. Under the umbrella framework of meta and transfer learning, various recent papers (Wang et al., 2023a; Toso et al., 2024; Aravind et al., 2024; Stamouli et al., 2025) have used PO methods to study how information from multiple LTI systems can be aggregated to learn policies that adapt across similar systems. Our formulation, which seeks to find a personalized policy for every system, departs fundamentally from this line of work which instead aims to learn a common policy for all systems. In this regard, we note that the closely related papers of Wang et al. (2023a) and Toso et al. (2024) need to assume that all the systems are sufficiently similar to admit a common stabilizing set. Even under this restrictive assumption, the results in these papers indicate that the sub-optimality gap exhibits an additive heterogeneity-induced bias term that might negate the speedups from collaboration. In contrast, our work does not require a common stabilizing policy to exist for systems across clusters. Furthermore, our personalization approach completely eliminates heterogeneity-induced biases. We also note that our approach incurs a logarithmic (in agents and samples) communication cost as opposed to the linear cost in Wang 3 KANAKERI BAJAJ VERMA GUPTA MITRA et al. (2023a). Finally, complementary to our clustering-based approach, ideas from representation learning (Zhang et al., 2023; Guo et al., 2023; Lee et al., 2025) and domain randomization (Fujinami et al., 2025) have also been recently used to improve data-efficiency in dynamic control tasks. 2. Problem Formulation We consider a setting with N agents partitioned into H disjoint clusters {Mj}j\u2208[H]. With each cluster j \u2208 [H], we associate a tuple Sj = (Aj, Bj, Qj, Rj), comprising a system matrix Aj \u2208 Rn\u00d7n, a control input matrix Bj \u2208 Rn\u00d7m, and two positive definite matrices Qj \u2208 Rn\u00d7n, Rj \u2208 Rm\u00d7m that define the LQR cost function for cluster j. Each agent in cluster j interacts with the same instance of the LQR problem specified by Sj, and aims to find a linear policy of the form ut = \u2212Kxt that minimizes the following infinite-horizon discounted cost: Cj(K) = E (cid:34) \u221e (cid:88) t=0 \u03b3t (cid:16) t Qjxt + u\u22a4 x\u22a4 t Rjut (cid:35) (cid:17) subject to xt+1 = Ajxt + Bjut + zt, (1) where x0 = 0 and xt, ut, and zt are the state, control input (action), and exogenous process noise, respectively, at time t, \u03b3 \u2208 (0, 1) is a discount factor, and K is a control gain matrix. We make the standard assumption that the pair (Aj, Bj) is controllable for every j \u2208 [H]. Following Malik et al. (2020), we assume that zt is sampled independently from a distribution D, such that: E[zt] = 0, E[ztz\u22a4 t ] = I, and \u2225zt\u22252 2 \u2264 B, \u2200t, (2) where B > 0 is some positive constant. For the LQR problem described in (1), it is well known (Bert- sekas, 2015) that the optimal control law is a linear feedback policy of the form ut",
    "the pair (Aj, Bj) is controllable for every j \u2208 [H]. Following Malik et al. (2020), we assume that zt is sampled independently from a distribution D, such that: E[zt] = 0, E[ztz\u22a4 t ] = I, and \u2225zt\u22252 2 \u2264 B, \u2200t, (2) where B > 0 is some positive constant. For the LQR problem described in (1), it is well known (Bert- sekas, 2015) that the optimal control law is a linear feedback policy of the form ut = \u2212K\u2217 j xt, where K\u2217 j is the optimal control gain matrix for cluster j. When Sj is known, each agent in Mj can obtain K\u2217 j by solving the discrete-time algebraic Riccati equation (DARE) (Anderson and Moore, 2007). However, our interest is in the learning scenario where the system matrices {(Aj, Bj)}j\u2208[H] are unknown to the agents. Even in this setting, it is known that policy optimization (PO) algorithms that treat the control gain as the optimization variable converge to the optimal policy (Fazel et al., 2018; Malik et al., 2020). The implementation of such algorithms relies on noisy trajectory rollouts to compute estimates of policy gradients.1 Specifically, given T independent rollouts from the tuple Sj, each agent within Mj can generate a gain \u02c6K such that with high probability, Cj( \u02c6K) \u2212 Cj(K\u2217 j ) \u2264 \u02dcO(1/ T ) (Malik et al., 2020). Our goal is to investigate whether this sample-complexity bound can be improved by leveraging the cluster structure in our problem. \u221a To achieve potential gains in sample-complexity via collaboration, we allow the agents to communicate via a central server, and make the following assumption that is common in the literature on collaborative/federated learning (Kone\u02c7cn`y et al., 2016; McMahan et al., 2017). Assumption 1 The noise processes across agents are statistically independent, i.e., for all i1, i2 \u2208 [N ] such that i1 \u0338= i2, the noise stochastic processes {z(i1) } are mutually independent. Here, with a slight overload of notation, we use {z(i) t } to denote the noise process for agent i \u2208 [N ]. } and {z(i2) t t Although the above assumption suggests that exchange of information between agents can accelerate the learning of an optimal policy, collaboration is complicated by the heterogeneity among clusters, due to the difference in system dynamics (Aj, Bj) and in task objectives (Qj, Rj). To capture such 1. The notion of a rollout will be made precise later in this section. 4 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS heterogeneity across clusters, we take inspiration from the notion of \u201ccluster separation gaps\" in supervised learning (Ghosh et al., 2022; Su et al., 2022; Sattler et al., 2020), and introduce \u2206 := min j1,j2\u2208[H]:j1\u0338=j2 |Cj1(K\u2217 j1) \u2212 Cj2(K\u2217 j2)|. (3) We assume a non-zero separation across clusters, i.e., \u2206 > 0. While one can certainly formu- late alternative notions of heterogeneity, we will show (in Theorem 2) that our metric of cluster- dissimilarity, as captured by \u2206 in (3), can be suitably exploited to separate clusters and",
    "the notion of \u201ccluster separation gaps\" in supervised learning (Ghosh et al., 2022; Su et al., 2022; Sattler et al., 2020), and introduce \u2206 := min j1,j2\u2208[H]:j1\u0338=j2 |Cj1(K\u2217 j1) \u2212 Cj2(K\u2217 j2)|. (3) We assume a non-zero separation across clusters, i.e., \u2206 > 0. While one can certainly formu- late alternative notions of heterogeneity, we will show (in Theorem 2) that our metric of cluster- dissimilarity, as captured by \u2206 in (3), can be suitably exploited to separate clusters and learn personalized policies for each cluster. In particular, our results reveal that heterogeneity can be helpful: a larger value of \u2206 leads to faster cluster separation using fewer rollouts. With the above ideas in place, we are now ready to formally state our problem of interest. For each agent i \u2208 [N ], let us use \u03c3(i) to represent the index of the cluster to which it belongs. Problem 1 (Clustered LQR Problem) Let \u03b4 \u2208 (0, 1) be a given failure probability. Suppose every agent i \u2208 [N ] has access to T independent rollouts from its corresponding system S\u03c3(i). Develop an algorithm that returns { \u02c6Ki}i\u2208[N ] such that with probability at least 1 \u2212 \u03b4, the following is true \u2200i \u2208 [N ]: C\u03c3(i)( \u02c6Ki) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 \u02dcO \uf8eb \uf8ed 1 (cid:113) |M\u03c3(i)|T \uf8f6 \uf8f8 . In simple words, our goal is to come up with an algorithm that generates a personalized control policy for every agent that benefits from the collective information available within that agent\u2019s cluster. This is quite non-trivial due to the following technical challenges. \u2022 In our setting, the system dynamics and the cluster identities are both unknown a priori. Thus, our problem requires learning the cluster identities and optimal policies simultaneously. \u2022 The clustering process is complicated by two main issues. First, the information used for clustering is based on noisy function evaluations that are insufficient for estimating the system models, ruling out system-identification-based approaches in Toso et al. (2023) and Rui and Dahleh (2025). Thus, we need to develop a model-free clustering algorithm. Second, the minimum separation gap \u2206 in (3) is assumed to be unknown, ruling out the possibility of simple one-shot clustering approaches. \u2022 Unlike supervised learning problems in Ghosh et al. (2020) and Su et al. (2022) where misclustering only introduces a bias due to heterogeneity, the price of misclustering can be more severe in our control setting. In particular, since the system tuples across clusters are allowed to be arbitrarily different, transfer of information across clusters can lead to destabilizing policies. In the next section, we will develop the PCPO algorithm that addresses the above challenges and solves Problem 1, while incurring only a logarithmic (with respect to the number of agents and rollouts) communication cost. In preparation for the next section, we now define the notion of a rollout and a zeroth-order gradient estimator. Given a policy K, a rollout for an agent i \u2208 Mj yields a noisy sample of the infinite-horizon trajectory",
    "clusters can lead to destabilizing policies. In the next section, we will develop the PCPO algorithm that addresses the above challenges and solves Problem 1, while incurring only a logarithmic (with respect to the number of agents and rollouts) communication cost. In preparation for the next section, we now define the notion of a rollout and a zeroth-order gradient estimator. Given a policy K, a rollout for an agent i \u2208 Mj yields a noisy sample of the infinite-horizon trajectory cost, defined as: Cj(K; Z (i)) = \u03b3t (cid:16) \u221e (cid:88) t=0 t Qjxt + u\u22a4 x\u22a4 t Rjut (cid:17) , where xt+1 = Ajxt + Bjut + zt, ut = \u2212Kxt, (4) 5 KANAKERI BAJAJ VERMA GUPTA MITRA x0 = 0 and Z (i) = {z(i) t }. We will interpret each rollout as a sample. Using such noisy function evaluations for a policy K run by an agent i \u2208 Mj, we define the M -minibatched zeroth-order gradient estimator with a smoothing radius r, as follows (Fazel et al., 2018; Malik et al., 2020): gi(K) := 1 M M (cid:88) k=1 Cj(K + rUk; Z (i) k ) (cid:19) (cid:18) D r Uk, (5) where D = mn, Uk is drawn independently from a uniform distribution over matrices with unit Frobenius norm, and Z (i) k are independent copies of Z (i) for all k \u2208 [M ]. In the sequel, for an agent i \u2208 Mj, we use the shorthand ZOi(K, M, r) to refer to the M -minibatched zeroth-order gradient estimator at policy K with smoothing radius r, as defined in (5). We use the notation c(p,_) to denote problem-parameter-dependent constants and provide their expressions in Appendix A of Kanakeri et al. (2025). We make the standard assumption (Fazel et al., 2018; Malik et al., 2020) that each agent i has access to an initial controller K(0) that lies within its respective stabilizing set: {K \u2208 Rm\u00d7n : \u03c1(A\u03c3(i) \u2212 B\u03c3(i)K) < 1}, where \u03c1(X) is the spectral radius of a matrix X \u2208 Rn\u00d7n. i 3. Description of the Algorithm In this section, we present our proposed algorithm, Personalized and Collaborative Policy Opti- mization (PCPO) (Algorithm 1), which effectively addresses Problem 1 by carefully accounting for its inherent challenges. Since the cluster separation gap \u2206 is unknown, we propose a sequential elimination strategy to identify the correct clusters. While the idea of sequential elimination has been explored in the context of multi-armed bandits (Even-Dar et al., 2006), we show that a similar approach can be used to effectively cluster LTI systems that satisfy the heterogeneity metric defined in (3). The algorithm proceeds in epochs (indexed by l) of increasing duration, where in each epoch, each agent i updates two sequences: a local sequence, {X (l) i }l\u22650. The local sequence is updated using the zeroth-order policy optimization algorithm in Fazel et al. (2018); Malik et al. (2020), and is used exclusively for clustering the agents. The global sequence is updated by aggregating the gradient estimates",
    "used to effectively cluster LTI systems that satisfy the heterogeneity metric defined in (3). The algorithm proceeds in epochs (indexed by l) of increasing duration, where in each epoch, each agent i updates two sequences: a local sequence, {X (l) i }l\u22650. The local sequence is updated using the zeroth-order policy optimization algorithm in Fazel et al. (2018); Malik et al. (2020), and is used exclusively for clustering the agents. The global sequence is updated by aggregating the gradient estimates from all the agents within an appropriately defined neighborhood set. After each epoch, the neighborhood sets are updated based on the concentration of the estimated cost around the optimal cost. As the number of rollouts increases across epochs, this concentration becomes tighter, hence pruning out misclustered agents over successive epochs. The various components of the algorithm are succinctly captured in Figure 1. In Section 4, we show that the neighborhood sets eventually converge to the correct clusters with high probability, after which the global sequence enjoys the collaborative gains without any heterogeneity bias. In what follows, we elaborate on the rationale behind the design of the various components of PCPO. i }l\u22650, and a global sequence, { \u02c6K(l) Building intuition. Correctly identifying the agents\u2019 clusters is crucial to reap any potential ben- efits from collaboration, as collaborating with agents from a different cluster can lead to destabilizing policies. In this regard, the major difficulty arises from the fact that the cluster separation gap \u2206 in (3) is unknown a priori. To appreciate the associated challenges, as a thought experiment, let us consider a simpler case where \u2206 is known. Under this scenario, each agent can locally run policy optimization to obtain a policy in a sufficiently close neighborhood of the optimal policy. Then, each agent can evaluate the cost at this policy and ensure that it is concentrated around the optimal cost. If the neighborhood radius, which depends on \u2206, is carefully chosen, it is easy to see that such a one-shot approach leads to correct clustering. However, when \u2206 is unknown, one-shot clustering may no longer work, motivating the sequential clustering idea in our proposed PCPO Algorithm. 6 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS Figure 1: Illustration of the epoch-based structure of PCPO, where each epoch involves three key steps: local policy optimization (PO), cost estimation, and global PO. i Sequential elimination. In each epoch l of PCPO, for every agent i \u2208 [N ], the server maintains a neighborhood set N (l) as an estimate of the true cluster M\u03c3(i). All such neighborhood sets are initialized from the set of all agents and sequentially pruned over epochs. For pruning, we start with an initial estimate of \u2206, denoted by \u22060, and update it by halving its value at the beginning of each epoch l to obtain \u2206l (Line 3 of Algo. 1). Our goal is to ensure that for all agents, the estimated cost in epoch l is in the \u2206l/4 neighborhood of its optimal cost. We achieve",
    "cluster M\u03c3(i). All such neighborhood sets are initialized from the set of all agents and sequentially pruned over epochs. For pruning, we start with an initial estimate of \u2206, denoted by \u22060, and update it by halving its value at the beginning of each epoch l to obtain \u2206l (Line 3 of Algo. 1). Our goal is to ensure that for all agents, the estimated cost in epoch l is in the \u2206l/4 neighborhood of its optimal cost. We achieve this in a two-step process, where we first perform local policy optimization to obtain a policy that is \u2206l/8-suboptimal (Line 4). The localPO subroutine performs Ml-minibatched policy optimization for Rl iterations starting with a controller X (l\u22121) . In every iteration t \u2208 [Rl], X(i,t), the t-th sub-iterate of localPO, is updated as X(i,t+1) \u2190 X(i,t) \u2212 \u03b7gi(X(i,t)), where gi(X(i,t)) = ZOi(X(i,t), Ml, r(loc) ). Then, we estimate the cost at the policy X (l) i obtained from localPO with an error tolerance of \u2206l/8 using Ml rollouts (Line 5). Having achieved the desired cost-estimation accuracy of \u2206l/4 for all agents, we prune the neighborhood sets according to (7) in Line 13. Eventually, as \u2206l \u2264 \u2206, which happens in O(log(\u22060/\u2206)) epochs, correct clustering takes place, as elaborated in the next section. i l Local and Global Sequences. To motivate the need for maintaining two sequences in PCPO, let us again consider the case where \u2206 is known, where it would suffice for the agents to only maintain a single sequence to run local PO until clustering, as discussed earlier in the one-shot clustering scheme. After the clusters are identified, the same sequence can be used for collaboration. In our setting, however, although the neighborhood sets eventually converge to the correct clusters in logarithmic number of epochs with respect to 1/\u2206, the number of such epochs cannot be determined a priori with- out knowledge of \u2206, making it difficult to decide when to initiate collaboration. Furthermore, with a single sequence, collaborating with misclustered agents can lead to an undesirable scenario where the sequence used to cluster is itself contaminated due to misclustering. PCPO carefully navigates this difficulty by maintaining two sequences of policies at each agent. The local sequence, {X (l) i }l\u22650, is used purely for clustering, and the global sequence, { \u02c6K(l) i }l\u22650, is updated by aggregating gradients from agents within the neighborhood set N (l\u22121) from the previous epoch l \u2212 1; see (6) in Line 9. i Logarithmic communication. Since both local and global PO are performed for Rl iterations with Ml rollouts per iteration, the overall sample complexity per epoch is Tl = 2RlMl + Ml, where the additional Ml rollouts are due to the cost estimation step. Each iteration of global PO proceeds as follows. First, for every agent, the server combines the minibatched zeroth-order gradient estimates as per (6). Then, the agents update the iterates using the averaged gradient with an appropriately chosen but fixed step size \u03b7 (Line 10). Since the",
    "are performed for Rl iterations with Ml rollouts per iteration, the overall sample complexity per epoch is Tl = 2RlMl + Ml, where the additional Ml rollouts are due to the cost estimation step. Each iteration of global PO proceeds as follows. First, for every agent, the server combines the minibatched zeroth-order gradient estimates as per (6). Then, the agents update the iterates using the averaged gradient with an appropriately chosen but fixed step size \u03b7 (Line 10). Since the above essentially incurs O(Rl) communication steps 7 KANAKERI BAJAJ VERMA GUPTA MITRA Algorithm 1 Personalized and Collaborative Policy Optimization (PCPO) 1: Initialization: \u22060; \u2200i \u2208 [N ], \u02c6K(0) 2: For l = 1, 2, . . . , i \u2190 K(0) i \u2190 K(0) , N (0) , X (0) i \u2190 [N ]. i i At Each Agent i: \u2206l \u2190 \u2206l\u22121 2 2l2 , \u03b7 \u2190 c(p,1), Rl \u2190 c(p,2) log (cid:17) (cid:16) c(p,3)N \u22062 l Ml \u2190 c(p,4) \u22062 l log (cid:16) 8DN Rl \u03b4l (cid:17) , \u02dcrl \u2190 (cid:114) log (cid:16) 8DN Rl \u03b4l (cid:17)(cid:19)1/2 , r(loc) l \u2190 min{c(p,6), \u02dcrl}. , \u03b4l \u2190 \u03b4 (cid:18) c(p,5)\u221a Ml Local Policy Optimization: X (l) Cost estimation: \u02c6C\u03c3(i)(X (l) Initialize Y (0) i \u2190 \u02c6K(l\u22121) For k = 0, 1, . . . , Rl \u2212 1 i \u2190 localPO(X (l\u22121) j=1 C\u03c3(i)(X (l) i ) \u2190 1 Ml and set r(global) (i,l) \u2190 min (cid:80)Ml (cid:26) i i i , Ml, Rl, r(loc) , Z (i) j ). l ). (cid:27) c(p,6), \u02dcrl |N (l\u22121) i |1/4 At Each Agent i: Transmit gi(Y (k) ) to the Server. At Server: Compute and transmit the averaged gradient estimate as follows: ) \u2190 ZOi(Y (k) , Ml, r(global) (i,l) i i \u25b7 For collaborative PO Gi \u2190 1 |N (l\u22121) i | (cid:88) gj(Y (k) j ). j\u2208N (l\u22121) i (6) At Each Agent i: Y (k+1) i \u2190 Y (k) i \u2212 \u03b7Gi. \u25b7 Global policy update via collaboration End For . Transmit X (l) At Each Agent i: Update \u02c6K(l) i At Server: Update the neighborhood set as follows: i \u2190 Y (Rl) i , \u02c6C\u03c3(i)(X (l) i ) to the server. \u25b7 Sequential elimination N (l) i \u2190 {j \u2208 N (l\u22121) i (cid:12) \u02c6C\u03c3(j)(X (l) (cid:12) (cid:12) (cid:12) j ) \u2212 \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) \u2264 \u2206l/2} i ) : (7) If N (l) i \u0338= N (l\u22121) i for some i \u2208 [N ]: For all agents i \u2208 [N ], update and transmit \u02c6K(l) i \u25b7 Reinitialization to ensure stability as follows: \u02c6K(l) i \u2190 argmin :j\u2208N (l) {X (l) i } j { \u02c6C\u03c3(j)(X (l) j ) : j \u2208 N (l) i }. (8) 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: End For per epoch, the total communication complexity of PCPO is O(Rl \u00d7 Number of Epochs). Based on our choice of parameters, both objects in the above product are logarithmic in the number",
    "\u02c6K(l) i \u25b7 Reinitialization to ensure stability as follows: \u02c6K(l) i \u2190 argmin :j\u2208N (l) {X (l) i } j { \u02c6C\u03c3(j)(X (l) j ) : j \u2208 N (l) i }. (8) 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: End For per epoch, the total communication complexity of PCPO is O(Rl \u00d7 Number of Epochs). Based on our choice of parameters, both objects in the above product are logarithmic in the number of agents N , the gap 1/\u2206, and the number of total rollouts per agent, namely T . Note on reinitialization. Since the server averages gradients in every epoch based on the neighborhood set, agents inevitably collaborate across clusters until correct clustering is achieved. This can lead to destabilizing policies in the global sequence. To mitigate this, at the end of each epoch, we reinitialize the global policy sequences for all agents whenever any neighborhood set is updated, as specified in (8). In the next section, we establish in Theorem 1 that the neighborhood sets eventually converge to the correct clusters and cease to update. Consequently, reinitialization 8 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS ensures that the global sequences of agents within the same cluster evolve identically and achieve collaborative gains once the correct clusters are identified. 4. Main Results Our main results concern the two key components of the PCPO algorithm: (i) identifying the correct clusters via sequential elimination, and (ii) performing collaborative policy optimization with logarithmic communication. The following theorem captures the clustering component. Theorem 1 (Clustering with sequential elimination) Define L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2}. Given a failure probability \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4/2, the following statements concerning the neighborhood sets from the PCPO algorithm hold for every agent i \u2208 [N ]: 1. In every epoch l, we have M\u03c3(i) \u2286 N (l) i . 2. For any epoch l such that l \u2265 L, we have M\u03c3(i) = N (l) i . Discussion. The key technical contribution of Theorem 1 lies in showing that the sequential elimination strategy successfully lets the neighborhood sets converge to the correct clusters with high probability. In particular, we show that the true clusters are included in the neighborhood sets for all agents in each epoch, and there exists an epoch L after which the neighborhood sets have converged to the true clusters and remain fixed for all subsequent epochs (l \u2265 L). Later in this section, we provide a proof sketch and discuss how these claims follow from the local policy optimization and the cost estimation step, relying on our notion of the cluster separation gap as defined in (3). The following theorem captures the key result concerning the collaborative optimization part in PCPO. Theorem 2 (Collaborative Policy Optimization) Let the failure probability be \u03b4 \u2208 (0, 1). Define L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2} and let \u00afL",
    "section, we provide a proof sketch and discuss how these claims follow from the local policy optimization and the cost estimation step, relying on our notion of the cluster separation gap as defined in (3). The following theorem captures the key result concerning the collaborative optimization part in PCPO. Theorem 2 (Collaborative Policy Optimization) Let the failure probability be \u03b4 \u2208 (0, 1). Define L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2} and let \u00afL denote the last epoch. If the number of rollouts per agent satisfies T \u2265 \u02dcO(1/\u22062), and Assumption 1 holds, then \u00afL > L and \u02c6K( \u00afL) satisfies the following with probability at least 1 \u2212 \u03b4 for every agent i \u2208 [N ]: i C\u03c3(i)( \u02c6K( \u00afL) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 O \uf8ed \uf8eb (cid:113) c(p,7) (cid:113) log (cid:0) 8DN T \u03b4 \uf8f6 (cid:1) \uf8f8 . (9) T |M\u03c3(i)| \u221a Discussion. It was shown in Malik et al. (2020) that zeroth-order policy optimization provides a \u02dcO(1/ T ) suboptimal policy using T rollouts for a single system LQR problem. In contrast, Wang et al. (2023a) showed collaborative gains for a federated LQR setting while incurring additive bias terms that depend on the heterogeneity gap. Moreover, the results in Wang et al. (2023a) apply only to systems with bounded heterogeneity. Theorem 2 bridges this gap by showing that collaborative |M\u03c3(i)|-factor speedup for each agent i in (9)) can be achieved without gains (as evidenced by the any additive bias through careful cluster identification and collaboration exclusively within clusters. Furthermore, these gains hold for systems that can be arbitrarily different, as long as their optimal costs are separated according to (3). (cid:113) Corollary 3 (Logarithmic communication complexity) The PCPO algorithm guarantees a loga- rithmic communication complexity with respect to the total number of rollouts T , the number of agents N , and the inverse of the separation gap 1/\u2206. 9 KANAKERI BAJAJ VERMA GUPTA MITRA In the following, we provide proof sketches for both Theorem 1 and Theorem 2, while deferring the detailed proofs to Kanakeri et al. (2025). The statements made in the proof sketches are proba- bilistic in nature. However, to keep the exposition simpler, we omit specifying the success/failure probability of the statements and refer the readers to Kanakeri et al. (2025) for such details. Proof sketch for Theorem 1. We start by showing that the estimated cost for each agent is in the \u2206l/4 neighborhood of its optimal cost, i.e., in each epoch l, for each agent i, we prove that (10) | \u02c6C\u03c3(i)(X (l) \u03c3(i))| \u2264 \u2206l/4. \u03c3(i)) = C\u03c3(j)(K\u2217 i ) \u2212 C\u03c3(i)(K\u2217 Note that for any agent j \u2208 M\u03c3(i), since C\u03c3(i)(K\u2217 \u03c3(j)), in light of (10), it is apparent that such an agent will pass the requirement in (7), and hence, never be eliminated from the neighborhood sets of agent i. This explains the first claim of Theorem 1. As for the second claim, note that for any j /\u2208 M\u03c3(i),",
    "each agent i, we prove that (10) | \u02c6C\u03c3(i)(X (l) \u03c3(i))| \u2264 \u2206l/4. \u03c3(i)) = C\u03c3(j)(K\u2217 i ) \u2212 C\u03c3(i)(K\u2217 Note that for any agent j \u2208 M\u03c3(i), since C\u03c3(i)(K\u2217 \u03c3(j)), in light of (10), it is apparent that such an agent will pass the requirement in (7), and hence, never be eliminated from the neighborhood sets of agent i. This explains the first claim of Theorem 1. As for the second claim, note that for any j /\u2208 M\u03c3(i), in light of our dissimilarity metric \u2206 in (3), |C\u03c3(i)(K\u2217 \u03c3(j))| \u2265 \u2206. Now for an epoch l such that \u2206l \u2264 \u2206/2, the above inequality can be combined with that in (10) to see that | \u02c6C\u03c3(i)(X (l) j )| \u2265 3\u2206/4, violating the requirement for inclusion in (7). It remains to establish (10), which follows from two guarantees: (i) | \u02c6C\u03c3(i)(X (l) i )| \u2264 \u2206l/8, and (ii) |C\u03c3(i)(X (l) \u03c3(i))| \u2264 \u2206l/8. The second guarantee follows from an analysis of the local PO sub-routine in Line 4 of Algo. 1, drawing on Malik et al. (2020); the first follows from analyzing the cost estimation step in Line 5 based on a simple Hoeffding bound. i ) \u2212 \u02c6C\u03c3(j)(X (l) i ) \u2212 C\u03c3(i)(X (l) \u03c3(i)) \u2212 C\u03c3(j)(K\u2217 i ) \u2212 C\u03c3(i)(K\u2217 Proof sketch for Theorem 2. We prove Theorem 2 by conditioning on the event where the claims made in Theorem 1 hold. As agents collaborate exclusively within their respective clusters and the reinitialization step synchronizes their global sequences after correct clustering, the gradient estimate obtained by averaging (see (6)) estimates of agents within a cluster enjoys a variance reduction effect under Assumption 1. Using this, and the fact that the LQR cost satisfies a \u03d5-smoothness and \u00b5-PL condition locally (Fazel et al., 2018; Malik et al., 2020), we establish that in each iteration k of the final epoch \u00afL, the following recursion holds with probability 1 \u2212 \u03b4\u2032: \uf8eb \uf8f6 Sk+1 \u2264 (cid:16) 1 \u2212 (cid:17) \u03b7\u00b5 4 Sk + 3\u03b7 \uf8ec (p,8)D2 c2 \uf8ec \uf8ec \uf8ec (r(global) )2|M\u03c3(i)|M \u00afL \uf8ec (i, \u00afL) \uf8ed (cid:124) (cid:123)(cid:122) s1 log (cid:19) (cid:18) 2D \u03b4\u2032 (cid:125) + \u03d52(r(global) (cid:124) (i, \u00afL) (cid:123)(cid:122) s2 \uf8f7 \uf8f7 )2 \uf8f7 \uf8f7 \uf8f7 (cid:125) \uf8f8 , (11) i ) \u2212 C\u03c3(i)(K\u2217 where Sk := C\u03c3(i)(Y (k) \u03c3(i)). The term s1 is due to the concentration of the minibatched gradient estimate around the gradient of a smoothed cost defined in Appendix A of Kanakeri et al. (2025); this is the term that benefits from collaboration. The term s2 captures the bias that arises when estimating gradients from noisy function evaluations. To ensure that this bias term does not negate the collaborative speedup in s1, we choose the smoothing radius r(global) to minimize the sum s1 + s2. Unrolling the recursion for Rl iterations (with the choice of Rl in PCPO) provides the per epoch convergence with rate \u02dcO . Finally, using the fact that Ml increases |M\u03c3(i)|M \u00afL exponentially with epochs, we establish that",
    "collaboration. The term s2 captures the bias that arises when estimating gradients from noisy function evaluations. To ensure that this bias term does not negate the collaborative speedup in s1, we choose the smoothing radius r(global) to minimize the sum s1 + s2. Unrolling the recursion for Rl iterations (with the choice of Rl in PCPO) provides the per epoch convergence with rate \u02dcO . Finally, using the fact that Ml increases |M\u03c3(i)|M \u00afL exponentially with epochs, we establish that M \u00afL = \u02dc\u2126(T ). (i, \u00afL) (cid:113) 1/ (cid:17) (cid:16) 5. Conclusion We developed a novel clustering-based approach for learning personalized control policies using data from heterogeneous dynamical processes. As future work, we will explore (i) alternative measures of dissimilarity across systems, (ii) more general dynamical processes, and (iii) online settings. 10 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS References Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1\u201376, 2021. Brian DO Anderson and John B Moore. Optimal control: linear quadratic methods. Courier Corporation, 2007. Ashwin Aravind, Mohammad Taha Toghani, and C\u00e9sar A Uribe. A moreau envelope approach for LQR meta-policy estimation. In 2024 IEEE 63rd Conference on Decision and Control (CDC), pages 415\u2013420. IEEE, 2024. Dimitri P Bertsekas. Dynamic programming and optimal control 4th edition, volume ii. Athena Scientific, 2015. Yiting Chen, Ana M Ospina, Fabio Pasqualetti, and Emiliano Dall\u2019Anese. Multi-task system identification of similar linear time-invariant dynamical systems. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 7342\u20137349. IEEE, 2023. Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006. Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In Int. Conf. on Machine Learning, pages 1467\u20131476. PMLR, 2018. Tesshu Fujinami, Bruce D Lee, Nikolai Matni, and George J Pappas. Domain randomization is sample efficient for linear quadratic control. arXiv preprint 2025. Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. Advances in neural information processing systems, 33:19586\u201319597, 2020. Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. IEEE Transactions on Information Theory, 68(12):8076\u20138091, 2022. Benjamin Gravell, Peyman Mohajerin Esfahani, and Tyler Summers. Learning optimal controllers for linear systems with multiplicative noise via policy gradient. IEEE Transactions on Automatic Control, 66(11):5283\u20135298, 2020. Taosha Guo, Abed AlRahman Al Makdah, Vishaal Krishnan, and Fabio Pasqualetti. Imitation and transfer learning for lqg control. IEEE Control Systems Letters, 7:2149\u20132154, 2023. Bin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer Ba\u00b8sar. Toward a theoretical foundation of policy optimization for learning control policies. Annual Review of Control, Robotics, and Autonomous Systems, 6(1):123\u2013158, 2023. 11 KANAKERI BAJAJ VERMA GUPTA MITRA Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note",
    "Taosha Guo, Abed AlRahman Al Makdah, Vishaal Krishnan, and Fabio Pasqualetti. Imitation and transfer learning for lqg control. IEEE Control Systems Letters, 7:2149\u20132154, 2023. Bin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer Ba\u00b8sar. Toward a theoretical foundation of policy optimization for learning control policies. Annual Review of Control, Robotics, and Autonomous Systems, 6(1):123\u2013158, 2023. 11 KANAKERI BAJAJ VERMA GUPTA MITRA Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note arXiv preprint on concentration inequalities for random vectors with subgaussian norm. 2019. Vinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, and Aritra Mitra. Harnessing data from clustered LQR systems: Personalized and collaborative policy optimization. arXiv preprint, 2025. Jakub Kone\u02c7cn`y, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint 2016. Bruce D Lee, Leonardo F Toso, Thomas T Zhang, James Anderson, and Nikolai Matni. Regret analysis of multi-task representation learning for linear-quadratic adaptive control. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 18062\u201318070, 2025. Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter L Bartlett, and Martin J Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic systems. Journal of Machine Learning Research, 21(21):1\u201351, 2020. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli- gence and Statistics, pages 1273\u20131282. PMLR, 2017. Aditya Modi, Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Joint learning of linear time-invariant dynamical systems. Automatica, 164:111635, 2024. Amirreza Neshaei Moghaddam, Alex Olshevsky, and Bahman Gharesifard. Sample complexity of the linear quadratic regulator: A reinforcement learning lens. Journal of Machine Learning Research, 26(151):1\u201350, 2025. Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanovi\u00b4c. Global exponential convergence of gradient methods over the nonconvex landscape of the linear quadratic regulator. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 7474\u20137479. IEEE, 2019. Hesameddin Mohammadi, Mahdi Soltanolkotabi, and Mihailo R Jovanovi\u00b4c. On the linear conver- gence of random search for discrete-time LQR. IEEE Control Systems Letters, 5(3):989\u2013994, 2020. Maryann Rui and Munther A Dahleh. Learning clusters of partially observed linear dynamical systems. In 2025 American Control Conference (ACC), pages 3545\u20133550. IEEE, 2025. Felix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered federated learning: Model- agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and learning systems, 32(8):3710\u20133722, 2020. Charis Stamouli, Leonardo F Toso, Anastasios Tsiamis, George J Pappas, and James Anderson. Policy gradient bounds in multitask LQR. arXiv preprint 2025. 12 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS Lili Su, Jiaming Xu, and Pengkun Yang. Global convergence of federated learning for mixed regression. Advances in Neural Information Processing Systems, 35:29889\u201329902, 2022. Leonardo F Toso, Han Wang, and James Anderson. Learning personalized models with clustered system identification. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 7162\u20137169. IEEE, 2023. Leonardo Felipe Toso, Donglin Zhan, James Anderson, and Han Wang. Meta-learning linear",
    "Policy gradient bounds in multitask LQR. arXiv preprint 2025. 12 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS Lili Su, Jiaming Xu, and Pengkun Yang. Global convergence of federated learning for mixed regression. Advances in Neural Information Processing Systems, 35:29889\u201329902, 2022. Leonardo F Toso, Han Wang, and James Anderson. Learning personalized models with clustered system identification. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 7162\u20137169. IEEE, 2023. Leonardo Felipe Toso, Donglin Zhan, James Anderson, and Han Wang. Meta-learning linear In 6th Annual quadratic regulators: a policy gradient maml approach for model-free LQR. Learning for Dynamics & Control Conference, pages 902\u2013915. PMLR, 2024. Omkar Tupe, Max Hartman, Lav R Varshney, and Saurav Prakash. Federated nonlinear system identification. arXiv preprint 2025. Han Wang, Leonardo F Toso, Aritra Mitra, and James Anderson. Model-free learning with het- erogeneous dynamical systems: A federated LQR approach. arXiv preprint 2023a. Han Wang, Leonardo Felipe Toso, and James Anderson. Fedsysid: A federated approach to sample-efficient system identification. In Learning for Dynamics and Control Conference, pages 1308\u20131320. PMLR, 2023b. Lei Xin, Lintao Ye, George Chiu, and Shreyas Sundaram. Learning dynamical systems by leveraging data from similar systems. IEEE Transactions on Automatic Control, 2025. Kaiqing Zhang, Bin Hu, and Tamer Basar. Policy optimization for h2 linear control with h\u221e robustness guarantee: Implicit regularization and global convergence. SIAM Journal on Control and Optimization, 59(6):4081\u20134109, 2021. Thomas T Zhang, Katie Kang, Bruce D Lee, Claire Tomlin, Sergey Levine, Stephen Tu, and Nikolai Matni. Multi-task imitation learning for linear dynamical systems. In Learning for Dynamics and Control Conference, pages 586\u2013599. PMLR, 2023. 13 KANAKERI BAJAJ VERMA GUPTA MITRA Appendix A. Properties of the LQR problem Notation. For matrices A \u2208 Rm\u00d7n and B \u2208 Rm\u00d7n, we use \u2225A\u2225 to denote the Frobenius norm of A which is defined as \u2225A\u2225 = (cid:112)trace(A\u22a4A). We use \u27e8A, B\u27e9 to denote the Frobenius inner-product defined as \u27e8A, B\u27e9 = trace(A\u22a4B). In this section, we discuss some of the properties of the LQR cost that were established in Fazel et al. (2018); Malik et al. (2020). In particular, the LQR cost in (1) is locally Lipschitz, locally smooth, and enjoys a gradient-domination property over the set of stabilizing controllers. These key properties aid in the convergence analysis of the model-free policy gradient algorithm, and we use them in the proofs of Theorem 1 in Appendix B and Theorem 2 in Appendix C. However, to show the convergence, it is crucial to ensure that the policy gradient iterates always lie within a restricted subset of the stabilizing set with high probability. In Malik et al. (2020), such a restricted set is chosen based on the initial suboptimality gap. In our setting, given access to a set of initial stabilizing controllers for all agents, {K(0) i }i\u2208[N ], and our initial guess for the cluster separation gap, \u22060, we define \u02dc\u22060 := max{maxi\u2208[N ](C\u03c3(i)(K(0) \u03c3(i))), \u22060}, and the restricted sets as follows for all j \u2208 [H]: ) \u2212 C\u03c3(i)(K\u2217 i (12) j ) \u2264 10 \u02dc\u22060}. j",
    "set with high probability. In Malik et al. (2020), such a restricted set is chosen based on the initial suboptimality gap. In our setting, given access to a set of initial stabilizing controllers for all agents, {K(0) i }i\u2208[N ], and our initial guess for the cluster separation gap, \u22060, we define \u02dc\u22060 := max{maxi\u2208[N ](C\u03c3(i)(K(0) \u03c3(i))), \u22060}, and the restricted sets as follows for all j \u2208 [H]: ) \u2212 C\u03c3(i)(K\u2217 i (12) j ) \u2264 10 \u02dc\u22060}. j := {K \u2208 Rm\u00d7n : Cj(K) \u2212 Cj(K\u2217 G0 Properties of the LQR cost. For each system j \u2208 [H], on the restricted domain G0 j , Malik et al. (2020) showed that the local properties hold uniformly, i.e., \u2203\u03d5j > 0, \u03bbj > 0, \u03c1j > 0, such that the LQR cost in (1) is (\u03bbj, \u03c1j)-locally Lipschitz and (\u03d5j, \u03c1j)-locally smooth for all policies K \u2208 G0 j . Furthermore, it is known that the LQR cost satisfies the PL (gradient-domination) condition for all policies in the stabilizing set (see Lemma 3 of Malik et al. (2020)). Denoting the parameter for the PL condition for system j by \u00b5j > 0, we define \u00b5 := min{\u00b51, \u00b52, . . . , \u00b5H }. Similarly, defining \u03d5 := max{\u03d51, \u03d52, . . . , \u03d5H }, \u03bb := max{\u03bb1, \u03bb2, . . . , \u03bbH }, and \u03c1 := min{\u03c11, \u03c12, . . . , \u03c1H }, we can ensure that for every system j \u2208 [H], the cost in (1) is (\u03bb, \u03c1)-locally Lipschitz and (\u03d5, \u03c1)-locally smooth for all policies in their respective restricted sets G0 j , and \u00b5-PL in their respective stabilizing sets. The following lemmas from Malik et al. (2020) capture these properties. Lemma 4 (LQR cost is locally Lipschitz). For any system j \u2208 [H], given a pair of policies (K, K\u2032) \u2208 (G0 j \u00d7 G0 j ), if \u2225K \u2212 K\u2032\u2225 \u2264 \u03c1, we have (cid:12)Cj(K) \u2212 Cj(K\u2032)(cid:12) (cid:12) (cid:12) \u2264 \u03bb\u2225K \u2212 K\u2032\u2225. Lemma 5 (LQR cost has locally Lipschitz gradients.) For any system j \u2208 [H], given a pair of policies (K, K\u2032) \u2208 (G0 j \u00d7 G0 j ), if \u2225K \u2212 K\u2032\u2225 \u2264 \u03c1, we have (cid:13)\u2207Cj(K) \u2212 \u2207Cj(K\u2032)(cid:13) (cid:13) (cid:13) \u2264 \u03d5\u2225K \u2212 K\u2032\u2225. Lemma 6 (LQR cost satisfies PL.) For any system j \u2208 [H], given a stable policy K, we have \u2225\u2207Cj(K)\u22252 \u2265 \u00b5(Cj(K) \u2212 Cj(K\u2217 j )). Smoothed cost and the properties of the gradient estimate. The smoothed cost with a radius r for a system j \u2208 [H] is defined as Cj,r(K) := E[Cj(K + rv)], where v is uniformly distributed over all matrices in Rm\u00d7n with the Frobenius norm of at most 1. It is shown in Fazel et al. (2018); Malik et al. (2020) that the zeroth-order gradient estimate gi(\u00b7) as defined in (5) for an agent i is an 14 (13) (14) (15) HARNESSING DATA FROM CLUSTERED LQR SYSTEMS unbiased estimator of the gradient of the smoothed cost. In",
    "for a system j \u2208 [H] is defined as Cj,r(K) := E[Cj(K + rv)], where v is uniformly distributed over all matrices in Rm\u00d7n with the Frobenius norm of at most 1. It is shown in Fazel et al. (2018); Malik et al. (2020) that the zeroth-order gradient estimate gi(\u00b7) as defined in (5) for an agent i is an 14 (13) (14) (15) HARNESSING DATA FROM CLUSTERED LQR SYSTEMS unbiased estimator of the gradient of the smoothed cost. In particular, for all systems j \u2208 [H] and all agents i \u2208 Mj, we have the following properties for all K \u2208 G0 j and r \u2208 (0, \u03c1): E[gi(K)] = \u2207C\u03c3(i),r(K) \u2225\u2207Cj,r(K) \u2212 \u2207Cj(K)\u2225 \u2264 \u03d5r. (16) (17) j , \u2200r \u2208 (0, \u03c1) and all U \u2208 Rm\u00d7n with \u2225U \u2225 = 1. In other words, there exists G(j) Furthermore, it is known that the noisy rollout cost Cj(K + rU ; Z (i)) is uniformly bounded \u221e \u2265 0 such \u221e .) Let \u221e }. Hence, for any agent i \u2208 [N ], we have the following \u221e (see Lemma 11 of Malik et al. (2020) for the expression of G(j) \u2200K \u2208 G0 that Cj(K + rU ; Z (i)) \u2264 G(j) us define G\u221e := max{G(1) \u2200K \u2208 G0 j , \u2200r \u2208 (0, \u03c1) and all U \u2208 Rm\u00d7n with \u2225U \u2225 = 1: \u221e , . . . , G(H) \u221e , G(2) C\u03c3(i)(K + rU ; Z (i)) \u2264 G\u221e. (18) We then have the following concentration result that will prove useful in establishing \u201cvariance- reduction\" effects. Lemma 7 (Concentration of the zeroth-order gradient estimates). For any system j \u2208 [H], given a policy K \u2208 G0 j , a smoothing radius r \u2208 (0, \u03c1) and a failure probability \u03b4\u2032 \u2208 (0, 1), the following holds for the M -minibatched zeroth-order gradient estimate of an agent i \u2208 Mj with probability at least 1 \u2212 \u03b4\u2032: \u2225gi(K) \u2212 \u2207Cj,r(K)\u2225 \u2264 (cid:16) D + \u03d5 \u03c12 G\u221e + \u03bb \u03c1 \u221a M D r (cid:17) (cid:115) D log (cid:18) 2D \u03b4\u2032 (cid:19) . (19) (cid:13) (cid:13) k=1(g(i,k)(K) \u2212 \u2207Cj,r(K)) Proof We have \u2225gi(K) \u2212 \u2207Cj,r(K)\u2225 = (cid:13), where we denoted the k-th component of the minibatch as g(i,k)(K). Recall from (5) that this k-th component takes the form (cid:80)M (cid:13) (cid:13) (cid:13) 1 M g(i,k)(K) = Cj(K + rUk; Z (i) k ) (cid:19) (cid:18) D r Uk. Hence, we have \u2225g(i,k)(K)\u2225 \u2264 D We then have r G\u221e due to (18) as \u2225Uk\u2225 = 1. Let us define c(p,8) := (cid:16) G\u221e + \u03bb \u03c1 D + \u03d5 \u03c12 D (cid:17) . \u2225g(i,k)(K) \u2212 \u2207Cj,r(K)\u2225 = \u2225g(i,k)(K) \u2212 \u2207Cj,r(K) + \u2207Cj(K) \u2212 \u2207Cj(K)\u2225 (a) \u2264 \u2225g(i,k)(K)\u2225 + \u2225\u2207Cj,r(K) \u2212 \u2207Cj(K)\u2225 + \u2225\u2207Cj(K)\u2225 (b) \u2264 D r D r D r D r = (c) \u2264 = G\u221e + \u03d5r + \u03bb (cid:18) G\u221e + (cid:18) G\u221e + r2 D \u03c12 D \u03d5 + \u03d5 + (cid:19) \u03bb r D (cid:19) \u03bb",
    "as \u2225Uk\u2225 = 1. Let us define c(p,8) := (cid:16) G\u221e + \u03bb \u03c1 D + \u03d5 \u03c12 D (cid:17) . \u2225g(i,k)(K) \u2212 \u2207Cj,r(K)\u2225 = \u2225g(i,k)(K) \u2212 \u2207Cj,r(K) + \u2207Cj(K) \u2212 \u2207Cj(K)\u2225 (a) \u2264 \u2225g(i,k)(K)\u2225 + \u2225\u2207Cj,r(K) \u2212 \u2207Cj(K)\u2225 + \u2225\u2207Cj(K)\u2225 (b) \u2264 D r D r D r D r = (c) \u2264 = G\u221e + \u03d5r + \u03bb (cid:18) G\u221e + (cid:18) G\u221e + r2 D \u03c12 D \u03d5 + \u03d5 + (cid:19) \u03bb r D (cid:19) \u03bb \u03c1 D c(p,8). 15 KANAKERI BAJAJ VERMA GUPTA MITRA Table 1: Relevant notation and definitions Notation Ml Rl \u03b7 rl \u2206l N (l) i X (l) i \u02c6K(l) i Definition Minibatch size used to estimate zeroth-order gradients in the l-th epoch. Number of steps/iterations of the local and global policy optimization in the l-th epoch. Step size for both local and global policy optimization. Smoothing radius used in the zeroth-order gradient estimates in the l-th epoch. Estimate of \u2206 used to cluster the agents in the lth epoch. Neighborhood set corresponding to the i-th agent in the l-th epoch. Local policy for the i-th agent in the l-th epoch. Global policy for the i-th agent in the l-th epoch. In the above, (a) follows from the triangle inequality, and (b) follows from the uniform-boundedness of the noisy rollout together with (5), the bounded bias property as shown in (17), and the local- Lipschitz property in Lemma 4. Finally, (c) follows from using r < \u03c1. Hence, g(i,k)(K) \u2212 \u2207Cj,r(K) has a bounded norm and therefore belongs to a class of norm sub-Gaussian random matrices (Jin et al., 2019). Furthermore, it has zero mean due to (16). Therefore, the concentration result follows from a direct application of Corollary 7 from Jin et al. (2019) which provides a Hoeffding-type inequality for norm sub-Gaussian random matrices. Corollary 8 (Concentration of the collaborative zeroth-order gradient estimates.) Suppose As- sumption 1 holds. For any system j \u2208 [H], given a policy K \u2208 G0 j , a smoothing radius r \u2208 (0, \u03c1), define the collaborative zeroth-order gradient estimate as Gj(K) = 1 gi(K), where gi(K) is the M -minibatched gradient estimate from agent i \u2208 Mj. Let \u03b4\u2032 \u2208 (0, 1). The following holds with probability at least 1 \u2212 \u03b4\u2032: i\u2208Mj |Mj | (cid:80) (cid:16) \u2225Gj(K) \u2212 \u2207Cj,r(K)\u2225 \u2264 G\u221e + \u03bb \u03c1 D + \u03d5 \u03c12 r(cid:112)|Mj|M D (cid:17) (cid:115) D log (cid:18) 2D \u03b4\u2032 (cid:19) . (20) Proof Under Assumption 1, we note that the noise processes for all agents in Mj are independent. The proof then follows from Lemma 7 as the collaborative zeroth-order gradient estimate can be interpreted as a gradient estimate for system j with a |Mj|-fold increased minibatch size. Note on the problem dependent constants. In Malik et al. (2020), the values of the constants \u03bbj, \u03d5j, \u03c1j, G(j) \u221e are first derived locally in terms of the local cost Cj(K), and then, the global parameters are obtained by noting that the local cost is uniformly bounded",
    "Mj are independent. The proof then follows from Lemma 7 as the collaborative zeroth-order gradient estimate can be interpreted as a gradient estimate for system j with a |Mj|-fold increased minibatch size. Note on the problem dependent constants. In Malik et al. (2020), the values of the constants \u03bbj, \u03d5j, \u03c1j, G(j) \u221e are first derived locally in terms of the local cost Cj(K), and then, the global parameters are obtained by noting that the local cost is uniformly bounded over the restricted domain as shown in Lemma 9 of Malik et al. (2020). Since we have a different definition of the restricted domain, the values of our parameters vary from the ones provided in Malik et al. (2020). That said, the global parameters in our setting can be derived exactly in the same way as in Malik et al. (2020) by bounding the local cost as Cj(K) \u2264 10 \u02dc\u22060 + Cj(K\u2217 j ). For convenience, we compile all the relevant notation in Table 1. In the main text, we used the notation c(p,_) to denote the problem-parameter-dependent constants which are defined in the following. 16 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS (cid:18) c(p,8) = G\u221e + \u03bb \u03c1 D + \u03d5 (cid:19) \u03c12 D c(p,9) = 12c(p,8) \u00b5 (cid:18) max (cid:26) (cid:112)\u03d5, (cid:27)(cid:19)2 1 \u03c1 (p,9)D2, c2 (cid:33) (p,8)D2\u22062 0, 36G2 \u221e c(p,10) = max \u22062 0, 256c2 (cid:110) c(p,11) = c(p,12) = (cid:32) (cid:1) \u22062 0 c(p,10) log (cid:0) 8DN (cid:32) (cid:32) 4 \u03b7\u00b5 log \u03b4 c(p,10)N \u02dc\u22062 0 \u22062 0 (cid:113) c(p,13) = 4 max{1, c(p,9)} \uf8f1 \uf8f2 \uf8f3 8 \u00b5 , 1 4\u03d5 , c(p,1) = min c(p,2) = 4 \u03b7\u00b5 (cid:33) (cid:33) + log(4) c(p,12) log(c(p,11)T ) \uf8fc \uf8fd \u03c1 \u03bb + 2 max (cid:110)\u221a \u03d5, 1 \u03c1 (cid:111) \uf8fe (cid:111) (21) c(p,3) = \u02dc\u22060 max{16, 10c(p,10)} c(p,4) = c(p,10) c(p,5) = c(p,8)D \u03d5 c(p,6) = \u03c1 c(p,7) = Dc(p,13) Based on the above definitions of the problem-parameter-dependent constants, we provide the values used for the hyperparameters in the l-th epoch of the PCPO algorithm in Table 2. 17 KANAKERI BAJAJ VERMA GUPTA MITRA Table 2: Hyperparameters with their values in the lth epoch Hyperparameters Values \u2206l \u03b4l \u03b7 Rl Ml \u02dcrl r(loc) l r(global) l \u22060 2l \u03b4 2l2 c(p,1) c(p,2) log c(p,4) \u22062 l (cid:114) log log (cid:18) c(p,5)\u221a Ml (cid:17) (cid:17) (cid:16) c(p,3)N \u22062 l (cid:16) 8DN Rl \u03b4l (cid:16) 8DN Rl \u03b4l (cid:17)(cid:19)1/2 min{c(p,6), \u02dcrl} (cid:26) (cid:27) min c(p,6), \u02dcrl |N (l\u22121) i |1/4 Appendix B. Proof of Theorem 1 In this section, we provide the proof of Theorem 1 which concerns the clustering aspect of the PCPO algorithm. In particular, we show that, with high probability, the true clusters are included in the neighborhood sets for all agents in each epoch, and if epoch l \u2265 L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2}, the neighborhood sets are identical to the clusters. More specifically, we show that for all agents i \u2208 [N ],",
    "In this section, we provide the proof of Theorem 1 which concerns the clustering aspect of the PCPO algorithm. In particular, we show that, with high probability, the true clusters are included in the neighborhood sets for all agents in each epoch, and if epoch l \u2265 L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2}, the neighborhood sets are identical to the clusters. More specifically, we show that for all agents i \u2208 [N ], with probability at least 1 \u2212 \u03b4/2, M\u03c3(i) \u2286 N (l) in every epoch l, and if l \u2265 L, then i M\u03c3(i) = N (l) i . To prove both claims, it suffices to show that with high probability, the estimated cost at a locally optimized policy is concentrated in the \u2206l/4-neighborhood of the optimal cost in every epoch l for all agents i \u2208 [N ]. To see this, consider a \u201cgood\u201d event that occurs with probability 1 \u2212 \u03b4/2 where the following holds for all agents i \u2208 [N ] in every epoch l (we will prove that such an event exists later in this section): | \u02c6C\u03c3(i)(X (l) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i))| \u2264 \u2206l/4. (22) On this \u201cgood\u201d event, in what follows, we show that the first claim of Theorem 1 holds. Accordingly, fix an agent i and consider an agent j \u2208 M\u03c3(i). We now show by induction that j belongs to N (l) i in every epoch l. For the base case of induction, note that since we initialize the neighborhood sets . Next, for an epoch l \u2212 1 \u2265 1, let us assume that j \u2208 N (l\u22121) with all agents, j \u2208 N (0) . Since \u03c3(i)) = C\u03c3(j)(K\u2217 C\u03c3(i)(K\u2217 \u03c3(j)) as a consequence of j \u2208 M\u03c3(i), in the l-th epoch under the \u201cgood\u201d i i 18 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS event where (22) holds, we have | \u02c6C\u03c3(i)(X (l) i ) \u2212 \u02c6C\u03c3(j)(X (l) j )| \u2264 | \u02c6C\u03c3(i)(X (l) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i))| + | \u02c6C\u03c3(j)(X (l) j ) \u2212 C\u03c3(j)(K\u2217 \u03c3(j))| \u2264 \u2206l/4 + \u2206l/4 = \u2206l/2, based on the neighborhood set update rule in (7). Hence, by induction, implying that j \u2208 N (l) j \u2208 N (l) i i in every epoch, therefore establishing the first claim of Theorem 1. Next, we show that on the \u201cgood\u201d event where (22) holds for all agents in every epoch, the second claim of Theorem 1 is also true. We prove this claim via contradiction. To proceed, suppose that there exist an epoch l \u2265 L, an agent i, and an agent j /\u2208 M\u03c3(i) such that j \u2208 N (l) . Then, we i have the following in light of the heterogeneity metric defined in (3): \u2206 \u2264 \u2264 \u2264 (cid:12) (cid:12)C\u03c3(i)(K\u2217 (cid:12) (cid:12) (cid:12)C\u03c3(i)(K\u2217 (cid:12) (cid:12) (cid:12)C\u03c3(i)(K\u2217 (cid:12) (cid:12) (cid:12) \u03c3(j)) (cid:12) i ) + \u02c6C\u03c3(j)(X (l) (cid:12) \u02c6C\u03c3(j)(X (l) (cid:12) (cid:12) \u03c3(i)) \u2212 C\u03c3(j)(K\u2217 \u03c3(i)) \u2212 \u02c6C\u03c3(i)(X (l) (cid:12) \u03c3(i)) \u2212",
    "contradiction. To proceed, suppose that there exist an epoch l \u2265 L, an agent i, and an agent j /\u2208 M\u03c3(i) such that j \u2208 N (l) . Then, we i have the following in light of the heterogeneity metric defined in (3): \u2206 \u2264 \u2264 \u2264 (cid:12) (cid:12)C\u03c3(i)(K\u2217 (cid:12) (cid:12) (cid:12)C\u03c3(i)(K\u2217 (cid:12) (cid:12) (cid:12)C\u03c3(i)(K\u2217 (cid:12) (cid:12) (cid:12) \u03c3(j)) (cid:12) i ) + \u02c6C\u03c3(j)(X (l) (cid:12) \u02c6C\u03c3(j)(X (l) (cid:12) (cid:12) \u03c3(i)) \u2212 C\u03c3(j)(K\u2217 \u03c3(i)) \u2212 \u02c6C\u03c3(i)(X (l) (cid:12) \u03c3(i)) \u2212 \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) + i ) (cid:12) \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) (cid:12) i ) \u2212 \u02c6C\u03c3(j)(X (l) (cid:12) j ) (cid:12) (cid:12) i ) \u2212 \u02c6C\u03c3(j)(X (l) (cid:12) (cid:12) , j ) \u02c6C\u03c3(i)(X (l) (a) \u2264 \u2206l/4 + \u2206l/4 + (b) \u2264 \u2206/4 + (cid:12) (cid:12) (cid:12) j ) \u2212 C\u03c3(j)(K\u2217 j ) \u2212 C\u03c3(j)(K\u2217 \u03c3(j)) + \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + \u03c3(j)) \u02c6C\u03c3(i)(X (l) i ) \u2212 \u02c6C\u03c3(j)(X (l) j ) (cid:12) (cid:12) (cid:12) (cid:12) i ) \u2212 \u02c6C\u03c3(j)(X (l) (cid:12) j ) (cid:12) (cid:12) (cid:12) (cid:12) \u02c6C\u03c3(i)(X (l) where (a) holds due to (22), and (b) follows as \u2206l \u2264 \u2206/2 since l \u2265 L. The above set of inequalities (cid:12) i ) \u2212 \u02c6C\u03c3(j)(X (l) (cid:12) (cid:12) \u2265 (3/4)\u2206 \u2265 (3/2)\u2206l, contradicting our assumption that j ) imply that (cid:12) (cid:12) (cid:12) \u2264 \u2206l/2. Therefore, N (l) i ) \u2212 \u02c6C\u03c3(j)(X (l) \u02c6C\u03c3(i)(X (l) j \u2208 N (l) (cid:12) (cid:12) i = M\u03c3(i) for all j ) (cid:12) l \u2265 L, establishing the second claim of Theorem 1. i which requires that Now, it remains to prove that the \u201cgood\u201d event where (22) holds for all agents in every epoch occurs with probability at least 1 \u2212 \u03b4/2. To do so, for an agent i \u2208 [N ] in epoch l, we have (cid:12) (cid:12) (cid:12) \u02c6C\u03c3(i)(X (l) i ) \u2212 C\u03c3(i)(K\u2217 (cid:12) (cid:12) (cid:12) \u2264 \u03c3(i)) (cid:12) (cid:12)C\u03c3(i)(X (l) (cid:12) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12) \u02c6C\u03c3(i)(X (l) (cid:12) i ) \u2212 C\u03c3(i)(X (l) (cid:12) (cid:12) . i ) Therefore, to show (22), it suffices to show the following two guarantees for all agents in every epoch: (a) (b) (cid:12) (cid:12)C\u03c3(i)(X (l) (cid:12) (cid:12) \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) i ) \u2212 C\u03c3(i)(X (l) i ) (cid:12) (cid:12) (cid:12) \u2264 \u2206l/8 (cid:12) (cid:12) (cid:12) \u2264 \u2206l/8. (23) Now, let us establish the claims in (23) via induction across epochs. Let us assume that for a fixed epoch l \u2212 1 \u2265 1, the claims in (23) hold for all agents i \u2208 [N ] in every epoch k \u2264 {1, 2, . . . , l \u2212 1}, with probability at least (1 \u2212 (cid:80)l\u22121 \u03b4j 2 ). Denoting this event as El\u22121, in the following, we show that j=1 the claims in (23) hold in epoch l, \u2200i \u2208 [N ] with probability at least (1 \u2212 \u03b4l 2 ) conditioned on the event El\u22121. In the following, fixing an",
    "1, the claims in (23) hold for all agents i \u2208 [N ] in every epoch k \u2264 {1, 2, . . . , l \u2212 1}, with probability at least (1 \u2212 (cid:80)l\u22121 \u03b4j 2 ). Denoting this event as El\u22121, in the following, we show that j=1 the claims in (23) hold in epoch l, \u2200i \u2208 [N ] with probability at least (1 \u2212 \u03b4l 2 ) conditioned on the event El\u22121. In the following, fixing an agent i \u2208 [N ], we show: (a) (cid:12) (cid:12)C\u03c3(i)(X (l) (cid:12) with probability at least 1\u2212\u03b4l/(4N ) conditioned on the event El\u22121, and (b) i ) \u2212 C\u03c3(i)(K\u2217 (cid:12) \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) \u2264 \u2206l/8 \u03c3(i)) (cid:12) i ) \u2212 C\u03c3(i)(X (l) (cid:12) (cid:12) \u2264 i ) 19 KANAKERI BAJAJ VERMA GUPTA MITRA \u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N ) conditioned on the intersection of the events El\u22121 and the one where item (a) in (23) holds. The following lemma provides the convergence of the local policy optimization sub-routine in epoch l which aids in establishing claim (a) from (23). Lemma 9 (Local Policy Optimization.) For any agent i \u2208 Mj, given a policy K0 \u2208 G0 j , let KR be the output of the localPO(K0, M, R, r) subroutine with step size \u03b7. Then, for any \u03b4\u2032 \u2208 (0, 1/R), with probability at least 1 \u2212 \u03b4\u2032R, KR \u2208 G0 j and we have the following: Cj(KR) \u2212 Cj(K\u2217 j ) \u2264 (cid:16) 1 \u2212 (cid:17)R \u03b7\u00b5 4 (Cj(K0) \u2212 Cj(K\u2217 j )) + (cid:32) c(p,9)D \u221a M (cid:115) log (cid:18) 2D \u03b4\u2032 (cid:19)(cid:33) , (24) when \u03b7 = c(p,1), M \u2265 c(p,4) \u22062 0 log(2D/\u03b4\u2032), r = min{\u03c1, \u02dcr}, where \u02dcr = (cid:18) c(p,5)\u221a M (cid:113) log (cid:0) 2D \u03b4\u2032 (cid:19)1/2 (cid:1) . The proof of Lemma 9 is provided in Appendix B.1. We use Lemma 9 to analyze the local policy (cid:12) i ) \u2212 C\u03c3(i)(K\u2217 (cid:12) optimization step in line 4 of the PCPO algorithm that helps in establishing (cid:12) \u2264 \u03c3(i)) \u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N ). More precisely, the settings for the hyperparameters (\u03b7, Ml, r(loc) , Rl) from Table 2 meet the requirement for the corresponding hyperparameters in Lemma 9. Furthermore, conditioned on the event El\u22121, claim (a) in (23) implies that X (l\u22121) \u03c3(i). Hence, the following holds due to Lemma 9 with probability at least 1\u2212\u03b4\u2032Rl for some \u03b4\u2032 \u2208 (0, 1/Rl): (cid:12) (cid:12)C\u03c3(i)(X (l) (cid:12) \u2208 G0 i l C\u03c3(i)(X (l) i )\u2212C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 1 \u2212 (cid:17)Rl \u03b7\u00b5 4 (cid:16) (cid:124) (C\u03c3(i)(X (l\u22121) i (cid:123)(cid:122) s1 ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i))) (cid:125) + (cid:32) (cid:124) c(p,9)D \u221a Ml (cid:115) log (cid:18) 2D \u03b4\u2032 (cid:123)(cid:122) s2 (cid:19)(cid:33) . (cid:125) i (cid:17) (cid:17) (cid:16) 16 \u02dc\u22060 \u2206l ) \u2212 C\u03c3(i)(K\u2217 ensures s1 \u2264 \u2206l/16. Similarly, setting Ml = Note that (C\u03c3(i)(X (l\u22121) event El\u22121 where claim (a) of (23) holds. Hence, from Table 2, setting Rl = c(p,2) log \u03c3(i))) \u2264 \u2206l\u22121/8",
    "(l) i )\u2212C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 1 \u2212 (cid:17)Rl \u03b7\u00b5 4 (cid:16) (cid:124) (C\u03c3(i)(X (l\u22121) i (cid:123)(cid:122) s1 ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i))) (cid:125) + (cid:32) (cid:124) c(p,9)D \u221a Ml (cid:115) log (cid:18) 2D \u03b4\u2032 (cid:123)(cid:122) s2 (cid:19)(cid:33) . (cid:125) i (cid:17) (cid:17) (cid:16) 16 \u02dc\u22060 \u2206l ) \u2212 C\u03c3(i)(K\u2217 ensures s1 \u2264 \u2206l/16. Similarly, setting Ml = Note that (C\u03c3(i)(X (l\u22121) event El\u22121 where claim (a) of (23) holds. Hence, from Table 2, setting Rl = c(p,2) log \u03c3(i))) \u2264 \u2206l\u22121/8 \u2264 \u22060 \u2264 \u02dc\u22060 as a result of conditioning on the \u2265 (cid:16) c(p,3)N \u22062 l log (cid:0) 2D \u03b4\u2032 \u03c3(i)) \u2264 \u03c3(i) with probability at least (1 \u2212 \u03b4l/(4N )), based on Lemma 9. Let us 4 \u03b7\u00b5 log ensures s2 \u2264 \u2206l/16. Finally, setting \u03b4\u2032 = \u03b4l/(4N Rl) provides us with C\u03c3(i)(X (l) \u2206l/8, and hence X (l) i \u2208 G0 denote this event by \u02dcE(l,1). (cid:12) (cid:12) i ) \u2212 C\u03c3(i)(X (l) \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) (cid:12) \u2264 \u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N ) i ) Now, we show that (cid:12) j ) and E[C\u03c3(i)(X (l) conditioned on the event \u02dcE(l,1). We have \u02c6C\u03c3(i)(X (l) i ) = 1 Ml , Z (i) C\u03c3(i)(X (l) i \u2208 G0 j ) \u2264 G\u221e due to (18). Using Hoeffding\u2019s inequality, the following holds for all s \u2265 0: (cid:80)Ml j=1 C\u03c3(i)(X (l) , Z (i) j , we have C\u03c3(i)(X (l) i ). Furthermore, since on event \u02dcE(l,1), X (l) (p,9)D2 \u22062 l i )\u2212C\u03c3(i)(K\u2217 log (cid:0) 2D \u03b4\u2032 c(p,4) \u22062 l (cid:1) \u2265 162c2 (cid:1) i i i , Z (i) j )] = P \uf8eb \uf8ed (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 Ml Ml(cid:88) j=1 C\u03c3(i)(X (l) i , Z (i) j ) \u2212 C\u03c3(i)(X (l) i ) \uf8f6 \u2265 s \uf8f8 \u2264 2 exp (cid:18) \u22122s2Ml G2 \u221e (cid:19) . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Setting s = \u2206l/8, and requiring the failure probability on the R.H.S to be lesser than \u03b4l/(4N ) leads to the requirement: Ml \u2265 36G2 which is satisfied by our choice of Ml from Table 2. \u22062 l (cid:16) 8N \u03b4l log (cid:17) \u221e 20 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS (cid:12) \u02c6C\u03c3(i)(X (l) (cid:12) Hence, (cid:12) this event as \u02dcE(l,2). i ) \u2212 C\u03c3(i)(X (l) i ) (cid:12) (cid:12) (cid:12) \u2264 \u2206l/8 with probability at least 1 \u2212 \u03b4l/(4N ). Let us denote Now, to show the two claims in (23), let us define an event \u02dcEl = \u02dcE(l,1) \u2229 \u02dcE(l,2). Then, P( \u02dcEl|El\u22121) = P( \u02dcE(l,2)| \u02dcE(l,1), El\u22121)P( \u02dcE(l,1)|El\u22121) \u2265 (1 \u2212 \u03b4l/(4N ))(1 \u2212 \u03b4l/(4N )) \u2265 1 \u2212 \u03b4l/(2N ). (cid:12) Therefore, on event \u02dcEl, both the guarantees: (a) (cid:12) (cid:12) \u2264 \u2206l/8 and (b) (cid:12) (cid:12) \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) (cid:12) \u2264 \u2206l/8 hold with probability at least 1 \u2212 \u03b4l/(2N ). Union bounding (cid:12) over all the agents, with probability at least 1 \u2212 \u03b4l/2, both claims in (23) hold for all agents i \u2208 [N ] on the event",
    "P( \u02dcE(l,2)| \u02dcE(l,1), El\u22121)P( \u02dcE(l,1)|El\u22121) \u2265 (1 \u2212 \u03b4l/(4N ))(1 \u2212 \u03b4l/(4N )) \u2265 1 \u2212 \u03b4l/(2N ). (cid:12) Therefore, on event \u02dcEl, both the guarantees: (a) (cid:12) (cid:12) \u2264 \u2206l/8 and (b) (cid:12) (cid:12) \u02c6C\u03c3(i)(X (l) (cid:12) (cid:12) (cid:12) \u2264 \u2206l/8 hold with probability at least 1 \u2212 \u03b4l/(2N ). Union bounding (cid:12) over all the agents, with probability at least 1 \u2212 \u03b4l/2, both claims in (23) hold for all agents i \u2208 [N ] on the event \u02dcEl after conditioning on the event El\u22121. i ) \u2212 C\u03c3(i)(X (l) i ) (cid:12) (cid:12)C\u03c3(i)(X (l) (cid:12) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) Finally, defining an event El = \u02dcEl \u2229 El\u22121, we have, P(El) = P( \u02dcEl|El\u22121)P(El\u22121) \u2265 (1 \u2212 \u03b4l/2) \uf8ed1 \u2212 \uf8eb l\u22121 (cid:88) j=1 \u03b4j 2 \uf8f6 \uf8eb \uf8f8 \u2265 \uf8ed1 \u2212 \uf8f6 \uf8f8 . l (cid:88) j=1 \u03b4j 2 Since \u03b4l = \u03b4/(2l2), we have (cid:80)l j=1 B.1. Proof of Lemma 9 \u03b4j 2 = (cid:80)l j=1 \u03b4 4j2 \u2264 \u03b4/2. This completes the proof of Theorem 1. In this section, we prove Lemma 9 which provides the convergence of the localPO subroutine. Fix a system j \u2208 [H] and let Kt denote the controller in the t-th iteration of localPO \u2200t = 0, 1, . . . , R. Note that the localPO sub-routine proceeds as follows: starting with a controller K0 \u2208 G0 j , in every iteration t, Kt is updated as Kt+1 = Kt \u2212 \u03b7g(Kt), where g(Kt) = ZO(Kt, M, r) is the M -minibatched zeroth-order gradient estimate with a smoothing radius r. We prove the statement via induction. Given the base case K0 \u2208 G0 j and \u03b4\u2032 \u2208 (0, 1/R), let us assume that in the t-th iteration the following holds for all \u03c4 \u2208 {1, 2, . . . , t} with probability at least (1 \u2212 \u03b4\u2032t) : K\u03c4 \u2208 G0 j Cj(K\u03c4 ) \u2212 Cj(K\u2217 j ) \u2264 (cid:16) 1 \u2212 (cid:17) \u03b7\u00b5 4 (Cj(K\u03c4 \u22121) \u2212 Cj(K\u2217 j )) + (cid:32) \u03b7\u00b5 4 c(p,9)D \u221a M (cid:115) log (cid:19)(cid:33) . (cid:18) 2D \u03b4\u2032 (25) Let us denote the event where both the claims in (25) hold by Et. Now, conditioned on the event Et, in the following, we will show that with probability at least 1 \u2212 \u03b4\u2032, Kt+1 \u2208 G0 j and Cj(Kt+1) \u2212 Cj(K\u2217 j ) \u2264 (cid:16) 1 \u2212 (cid:17) \u03b7\u00b5 4 (Cj(Kt) \u2212 Cj(K\u2217 j )) + (cid:32) \u03b7\u00b5 4 c(p,9)D \u221a M (cid:115) log (cid:19)(cid:33) . (cid:18) 2D \u03b4\u2032 In what follows, we omit the subscript notation j for convenience. Conditioned on the event Et, we begin by analyzing the one-step progress in the (t + 1)-th iteration of localPO. From Lemma 7, as the event Et ensures that Kt \u2208 G0, we have \u2225g(Kt) \u2212 \u2207Cr(Kt)\u2225 \u2264 (cid:1) with probability at least (1 \u2212 \u03b4\u2032). Let us denote this event by \u02dcEt. Define et := (cid:113) log (cid:0) 2D \u03b4\u2032 c(p,8)D \u221a M r 21 KANAKERI BAJAJ VERMA",
    "In what follows, we omit the subscript notation j for convenience. Conditioned on the event Et, we begin by analyzing the one-step progress in the (t + 1)-th iteration of localPO. From Lemma 7, as the event Et ensures that Kt \u2208 G0, we have \u2225g(Kt) \u2212 \u2207Cr(Kt)\u2225 \u2264 (cid:1) with probability at least (1 \u2212 \u03b4\u2032). Let us denote this event by \u02dcEt. Define et := (cid:113) log (cid:0) 2D \u03b4\u2032 c(p,8)D \u221a M r 21 KANAKERI BAJAJ VERMA GUPTA MITRA g(Kt) \u2212 \u2207C(Kt). Conditioned on the event \u02dcEt \u2229 Et, we have \u2225et\u2225 = \u2225g(Kt) \u2212 \u2207Cr(Kt) + \u2207Cr(Kt) \u2212 \u2207C(Kt)\u2225 (a) \u2264 \u2225g(Kt) \u2212 \u2207Cr(Kt)\u2225 + \u2225\u2207Cr(Kt) \u2212 \u2207C(Kt)\u2225 (b) \u2264 c(p,8)D \u221a M r (cid:115) log (cid:19) (cid:18) 2D \u03b4\u2032 + \u03d5r, (26) where (a) follows from the triangle inequality and the (b) due to the event \u02dcEt \u2229 Et and (17). Let us (cid:18) c(p,8)D (cid:19)1/2 (cid:113) (cid:113) , r = min{\u03c1, \u02dcr}, and define log (cid:0) 2D \u03b4\u2032 (cid:1) log (cid:0) 2D \u03b4\u2032 define cp = c(p,8)D \u221a M r (cid:113) log (cid:0) 2D Z := \u03b4\u2032 following sequence of bounds on cp : c(p,8)D \u221a M \u03d5 (cid:1). Based on the choice M \u2265 (cid:1) + \u03d5r. Set \u02dcr = \u221a M c(p,4) \u22062 0 log(2D/\u03b4\u2032), we have Z \u2264 1, yielding the cp = Z r + \u03d5r (cid:26) Z \u02dcr (cid:40) \u2264 max + \u03d5\u02dcr, (a) \u2264 max 2(cid:112)Z\u03d5, (cid:40) 2(cid:112)Z\u03d5, = max (cid:27) + \u03d5\u03c1 (cid:41) + \u03d5\u02dcr (cid:41) + (cid:112)Z\u03d5 Z \u03c1 \u221a Z \u03c1 \u221a Z \u03c1 \u221a \u2264 2 Z max (cid:26) (cid:112)\u03d5, (b) \u2264 2 max (cid:26) (cid:112)\u03d5, (cid:27) 1 \u03c1 (cid:27) 1 \u03c1 , (27) (28) where (a) and (b) follow from Z \u2264 1. Based on the above, we have \u03b7\u2225g(Kt)\u2225 \u2264 \u03b7(\u2225\u2207C(Kt)\u2225) + \u2225et\u2225 (a) \u2264 \u03b7(\u03bb + cp) (cid:18) (b) \u2264 \u03b7 \u03bb + 2 max (cid:26) (cid:112)\u03d5, (cid:27)(cid:19) , 1 \u03c1 where (a) follows from Lemma 4 and (b) follows from (28). Setting the RHS \u2264 \u03c1 leads to (cid:111) which is satisfied by setting \u03b7 = c(p,1). This ensures that the requirement \u03b7 \u2264 \u03c1 (cid:110)\u221a \u03bb+2 max \u03d5, 1 \u03c1 22 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS \u2225Kt+1 \u2212 Kt\u2225 \u2264 \u03c1. Using the local smoothness property (Lemma 5), we then have C(Kt+1) \u2212 C(Kt) \u2264 \u27e8\u2207C(Kt), Kt+1 \u2212 Kt\u27e9 + \u2225Kt+1 \u2212 Kt\u22252 = \u2212\u03b7\u27e8\u2207C(Kt), g(Kt)\u27e9 + = \u2212\u03b7\u27e8\u2207C(Kt), \u2207C(Kt) + et\u27e9 + \u2225\u2207C(Kt) + et\u22252 \u03d5 2 \u03d5\u03b72 2 \u2225g(Kt)\u22252 \u03d5\u03b72 2 (a) \u2264 \u2212\u03b7\u2225\u2207C(Kt)\u22252 \u2212 \u03b7\u27e8\u2207C(Kt), et\u27e9 + \u03d5\u03b72\u2225\u2207C(Kt)\u22252 + \u03d5\u03b72\u2225et\u22252 (b) \u2264 \u2212\u03b7(1 \u2212 \u03d5\u03b7)\u2225\u2207C(Kt)\u22252 + \u2225et\u22252 + \u03d5\u03b72\u2225et\u22252 \u03b7 2 (1 \u2212 2\u03d5\u03b7)\u2225\u2207C(Kt)\u22252 + \u03b7 2 (1 + 2\u03d5\u03b7)\u2225et\u22252 \u2225\u2207C(Kt)\u22252 + \u03b7 2 = \u2212 (c) \u2264 \u2212 \u03b7 2 \u03b7 4 \u2225\u2207C(Kt)\u22252 + 3\u03b7 4 c2 p. In the above, we used \u2225A + B\u22252 \u2264 2\u2225A\u22252 + 2\u2225B\u22252 in (a), and \u22122\u27e8A, B\u27e9 \u2264 \u2225A\u22252 + \u2225B\u22252 in (b) where A and B are any matrices in Rm\u00d7n. In (c), we used",
    "et\u27e9 + \u03d5\u03b72\u2225\u2207C(Kt)\u22252 + \u03d5\u03b72\u2225et\u22252 (b) \u2264 \u2212\u03b7(1 \u2212 \u03d5\u03b7)\u2225\u2207C(Kt)\u22252 + \u2225et\u22252 + \u03d5\u03b72\u2225et\u22252 \u03b7 2 (1 \u2212 2\u03d5\u03b7)\u2225\u2207C(Kt)\u22252 + \u03b7 2 (1 + 2\u03d5\u03b7)\u2225et\u22252 \u2225\u2207C(Kt)\u22252 + \u03b7 2 = \u2212 (c) \u2264 \u2212 \u03b7 2 \u03b7 4 \u2225\u2207C(Kt)\u22252 + 3\u03b7 4 c2 p. In the above, we used \u2225A + B\u22252 \u2264 2\u2225A\u22252 + 2\u2225B\u22252 in (a), and \u22122\u27e8A, B\u27e9 \u2264 \u2225A\u22252 + \u2225B\u22252 in (b) where A and B are any matrices in Rm\u00d7n. In (c), we used \u03b7 \u2264 1/(4\u03d5) (satisfied by our choice \u03b7 = c(p,1)) and \u2225et\u2225 \u2264 cp. Denoting the suboptimality gap as St = C(Kt) \u2212 C(K\u2217), and using the PL condition (15) in the above, we obtain the following with probability at least 1 \u2212 \u03b4\u2032: St+1 \u2264 \u2264 (cid:16) (cid:16) 1 \u2212 1 \u2212 (cid:17) (cid:17) \u03b7\u00b5 4 \u03b7\u00b5 4 St + St + 3\u03b7 4 \u03b7\u00b5 4 c2 p (cid:18) 3 \u00b5 (cid:19) . c2 p (29) On event Et, since we have Kt \u2208 G0 j , St \u2264 10 \u02dc\u22060. Furthermore, due to (27), we have 3 \u00b5 c2 p \u2264 12 \u00b5 (cid:18) (cid:26) (cid:112)\u03d5, max 1 \u03c1 (cid:27)(cid:19)2 c(p,8)D \u221a M (cid:115) log (cid:18) 2D \u03b4\u2032 (cid:19) . (cid:16) (cid:110)\u221a (cid:111)(cid:17)2 12c(p,8) \u00b5 max Defining c(p,9) := by M \u2265 based on (29), we have St+1 \u2264 (cid:0)1 \u2212 \u03b7\u00b5 event \u02dcEt \u2229 Et, Kt+1 \u2208 G0 c(p,4) \u22062 0 j and 4 \u03d5, 1 \u03c1 and setting M \u2265 (p,9)D2 c2 \u22062 0 log (cid:0) 2D (cid:1), which is satisfied \u03b4\u2032 p \u2264 \u22060 \u2264 10 \u02dc\u22060. Hence, \u00b5 c2 4 10 \u02dc\u22060 \u2264 10 \u02dc\u22060. Therefore, conditioned on the (cid:1) 10 \u02dc\u22060 + \u03b7\u00b5 log(2D/\u03b4\u2032) from the statement of Lemma 9, ensures that 3 St+1 \u2264 (cid:16) 1 \u2212 (cid:17) \u03b7\u00b5 4 St + \u03b7\u00b5 4 (cid:32) c(p,9)D \u221a M (cid:115) log (cid:19)(cid:33) . (cid:18) 2D \u03b4\u2032 Now, let us define Et+1 := \u02dcEt \u2229 Et. We have P( \u02dcEt \u2229 Et) = P( \u02dcEt|Et)P(Et) \u2265 (1 \u2212 \u03b4\u2032)(1 \u2212 \u03b4\u2032t) \u2265 1 \u2212 \u03b4\u2032(t + 1). This completes the induction step. To prove the statement of Lemma 9, since \u03b7 = c(p,1) \u2264 8/\u00b5, for any R \u2265 1, we can unroll the recursion on the event ER which occurs with 23 KANAKERI BAJAJ VERMA GUPTA MITRA probability 1 \u2212 \u03b4\u2032R. Doing so, we obtain the following which completes the proof: SR \u2264 (cid:16) 1 \u2212 (cid:16) \u2264 1 \u2212 \u03b7\u00b5 4 \u03b7\u00b5 4 (cid:17)R S0 + R\u22121 (cid:88) (cid:16) 1 \u2212 (cid:17)R S0 + k=0 (cid:32) c(p,9)D \u221a M \u03b7\u00b5 4 (cid:115) (cid:17)k \u03b7\u00b5 4 (cid:32) c(p,9)D \u221a M (cid:115) log (cid:18) 2D \u03b4\u2032 (cid:19)(cid:33) (cid:19)(cid:33) . (cid:18) 2D \u03b4\u2032 (30) log 24 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS Appendix C. Proof of Theorem 2 We prove Theorem 2 by conditioning on the event where the claims in (23) hold for all agents in every epoch. In Appendix B, we showed that such an event occurs with probability at least 1 \u2212",
    "S0 + k=0 (cid:32) c(p,9)D \u221a M \u03b7\u00b5 4 (cid:115) (cid:17)k \u03b7\u00b5 4 (cid:32) c(p,9)D \u221a M (cid:115) log (cid:18) 2D \u03b4\u2032 (cid:19)(cid:33) (cid:19)(cid:33) . (cid:18) 2D \u03b4\u2032 (30) log 24 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS Appendix C. Proof of Theorem 2 We prove Theorem 2 by conditioning on the event where the claims in (23) hold for all agents in every epoch. In Appendix B, we showed that such an event occurs with probability at least 1 \u2212 \u03b4/2 and let us denote it by EThm1. Furthermore, under this event, the claims of Theorem 1 hold as shown in Appendix B. In particular, the true clusters are always contained in the neighborhood sets and correct clustering takes place at the latest during the L-th epoch, ensuring that the agents collaborate solely within their own clusters from the (L + 1)-th epoch onward. With that in mind, we consider the following approach to prove Theorem 2. First, we take for granted that the last epoch occurs after the correct clustering takes place, i.e, \u00afL > L, and later show that this is indeed true if the total number of rollouts T \u2265 \u02dcO(1/\u22062). Next, we show that for any l > L (note that at least one such epoch exists in light of \u00afL > L,) the policy at the start of the collaborative policy optimization remains in the corresponding restricted domain with high probability, i.e, \u02c6K(l\u22121) \u03c3(i). Then, we focus on the last epoch \u00afL and provide the convergence guarantee, and finally conclude by analyzing the number of rollouts needed to ensure \u00afL > L. \u2208 G0 Conditioned on the event EThm1, we follow an induction based argument to show that \u02c6K(l\u22121) \u2208 \u03c3(i) for all agents i \u2208 [N ] in every epoch l > L. Let us define \u02dcL as the first epoch where correct G0 clustering takes place. Due to Theorem 1, since the correct clustering takes place at the latest during the Lth epoch, \u02dcL \u2264 L, and moreover, since the neighborhood sets are sequentially pruned with no new agents getting added to the neighborhood sets, we have M\u03c3(i) = N (l) for all agents in i every epoch l \u2265 \u02dcL. Furthermore, \u02dcL being the first epoch where correct clustering takes place, i = M\u03c3(i) for some agent i \u2208 [N ], hence causing reinitializtion as shown in (8). After N this reinitialization, the global sequences for all the agents are updated by collaborating within their respective clusters, and hence the global sequences for two agents within a cluster evolve identically in light of (6). In other words, for all agents i, j, if M\u03c3(i) = M\u03c3(j), then for all l \u2265 \u02dcL, we have the following on the event EThm1: \u0338= N \u02dcL \u02dcL\u22121 i i i i = \u02c6K(l) \u02c6K(l) i = N (l) M\u03c3(i) = N (l) j j = M\u03c3(j) (31) i Taking this into account, we show that \u02c6K(l\u22121) \u03c3(i) for all agents i \u2208 [N ] in",
    "for two agents within a cluster evolve identically in light of (6). In other words, for all agents i, j, if M\u03c3(i) = M\u03c3(j), then for all l \u2265 \u02dcL, we have the following on the event EThm1: \u0338= N \u02dcL \u02dcL\u22121 i i i i = \u02c6K(l) \u02c6K(l) i = N (l) M\u03c3(i) = N (l) j j = M\u03c3(j) (31) i Taking this into account, we show that \u02c6K(l\u22121) \u03c3(i) for all agents i \u2208 [N ] in every epoch l > \u02dcL \u2208 G0 via induction across epochs. For the base case l = \u02dcL + 1, as a consequence of reinitialization during the \u02dcLth epoch, and since X ( \u02dcL) i \u2208 G0 \u03c3(i) for all agents i \u2208 [N ] as a result of conditioning on the event EThm1, we have \u02c6K( \u02dcL) \u03c3(i) for all agents i \u2208 [N ]. Let us assume that for an epoch l \u2265 \u02dcL + 1, i with probability at least (1 \u2212 (cid:80)l\u22121 \u03b4j 4 ), we have \u02c6K(t\u22121) \u2208 G0 \u03c3(i) for all agents i \u2208 [N ] and for j=1 all t \u2208 { \u02dcL + 1, \u02dcL + 2, . . . , l}. With a slight abuse of notation, let us denote this event by El\u22121. Next, conditioned on the event EThm1 \u2229 El\u22121, we show that \u02c6K(l) \u03c3(i) for all agents i \u2208 [N ] with probability at least 1 \u2212 \u03b4l/4. i \u2208 G0 \u2208 G0 i In what follows we fix an agent i \u2208 [N ] and omit the notation i and \u03c3(i) for convenience. Given that K(l\u22121) \u2208 G0 on the event EThm1 \u2229 El\u22121, we focus on analyzing the iterates {Y (k)}0\u2264k<Rl in the lth epoch. The iterates are updated as follows: Y (k+1) = Y (k) \u2212 \u03b7G(Y (k)) with Y (0) = \u02c6K(l\u22121), where we used G(Y (k)) to denote the averaged zeroth-order gradient estimate as shown in (6) in the kth iteration. Note that in the light of Assumption 1, the averaged gradient estimate is an unbiased 25 KANAKERI BAJAJ VERMA GUPTA MITRA estimate of \u2207Cr(Y (k)) with an effective minibatch size of Ml|M|. Therefore, we follow an approach similar to the one from the proof of Lemma 9 in Appendix B.1 to analyze the one-step progress and to show that Y (Rl) = \u02c6Kl \u2208 G0. More specifically, we follow an induction based approach across iterations and establish one-step recursion similar to (25) and finally unroll the recursion to obtain something similar to (30). However, a key difference arises from the fact that the second term in the RHS of both (25) and (30) will now enjoy a variance reduction effect due to collaboration in light of Assumption 1 as shown in Corollary 8. In particular, following the induction approach from the proof of Lemma 9 in Appendix B.1, in the kth iteration, we have the following concentration with probability at least (1 \u2212 \u03b4\u2032) after conditioning on the event where the previous",
    "(30). However, a key difference arises from the fact that the second term in the RHS of both (25) and (30) will now enjoy a variance reduction effect due to collaboration in light of Assumption 1 as shown in Corollary 8. In particular, following the induction approach from the proof of Lemma 9 in Appendix B.1, in the kth iteration, we have the following concentration with probability at least (1 \u2212 \u03b4\u2032) after conditioning on the event where the previous iterations satisfy similar guarantees as in (25): \u2225G(Y (k)) \u2212 \u2207Cr(Y (k))\u2225 \u2264 c(p,8)D r(cid:112)Ml|M| (cid:115) log (cid:18) 2D \u03b4\u2032 (cid:19) . Conditioned on the event where the gradient estimate is concentrated as above, and defining ek := (cid:1) + \u03d5r following the arguments up to G(Y (k)) \u2212 \u2207C(Y (k)), we have \u2225ek\u2225 \u2264 c(p,8)D \u221a (cid:113) log (cid:0) 2D \u03b4\u2032 (26). Now, let us define cp = c(p,8)D \u221a Ml|M| r r (cid:113) Ml|M| log (cid:0) 2D \u03b4\u2032 (cid:1) + \u03d5r. Setting \u02dcr = (cid:19)1/2 (cid:18) c(p,8)D Ml \u221a \u03d5 (cid:113) (cid:1) log (cid:0) 2D \u03b4\u2032 (cid:113) log (cid:0) 2D \u03b4\u2032 c(p,8)D \u221a Ml \u02dcr r = min{\u03c1, which is ensured by our setting for Ml in Table 2. |M|1/4 }, we obtain the following bound on cp provided Z := cp = Z r(cid:112)|M| + \u03d5r (cid:26) Z \u02dcr|M|1/4 \u221a + \u03d5 \u02dcr |M|1/4 \u221a , Z \u03c1|M|1/2 (cid:27) + \u03d5\u03c1 (cid:41) \u02dcr |M|1/4 + \u03d5 \u2264 max \u2264 max (cid:40) 2 \u221a Z |M|1/4 (cid:26) \u2264 2 max \u2264 2 max , Z\u03d5 |M|1/4 (cid:26) Z \u03c1|M|1/4 (cid:27) (cid:112)\u03d5, 1 \u03c1 (cid:112)\u03d5, (cid:27) . 1 \u03c1 and (cid:1) \u2264 1 (32) (33) Based on the above, we choose \u03b7 = c(p,1) \u2264 (cid:111) to ensure that \u2225Y (k+1) \u2212 Y (k)\u2225 \u2264 \u03c1. \u03bb+2 max Defining Sk = C(Y (k)) \u2212 C(K\u2217) and following the analysis from Appendix B.1 up to (29) and using the bound on cp from (32), we obtain \u03c1 (cid:110)\u221a \u03d5, 1 \u03c1 Sk+1 \u2264 (cid:32) 1 \u2212 (cid:16) (cid:124) (cid:17) \u03b7\u00b5 4 (cid:123)(cid:122) s1 + Sk (cid:125) \u03b7\u00b5 4 (cid:124) (cid:115) log c(p,9)D (cid:112)Ml|M| (cid:123)(cid:122) s2 (cid:18) 2D \u03b4\u2032 (cid:19)(cid:33) , (cid:125) with probability at least 1 \u2212 \u03b4\u2032. Note that since |M| \u2265 1, the term s2 is not greater than the 4 10 \u02dc\u22060. Meanwhile, the term s1 \u2264 corresponding term from (25), and hence s2 \u2264 \u03b7\u00b5 4 \u22060 \u2264 \u03b7\u00b5 26 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS (cid:1) 10 \u02dc\u22060 as we have conditioned on the event where Y (k) \u2208 G0 similar to the proof of (cid:0)1 \u2212 \u03b7\u00b5 4 Lemma 9. This ensures that Y (k+1) \u2208 G0. Now, unrolling the recursion, we have with probability at least 1 \u2212 \u03b4\u2032Rl, Y (Rl) = \u02c6K(l) \u2208 G0 and the following: C( \u02c6K(l)) \u2212 C(K\u2217) \u2264 (cid:16) 1 \u2212 (cid:17)Rl \u03b7\u00b5 4 (C( \u02c6K(l\u22121)) \u2212 C(K\u2217)) + (cid:32) c(p,9)D (cid:112)Ml|M| (cid:19)(cid:33) (cid:115) log (cid:18) 2D \u03b4\u2032 . (34) Setting \u03b4\u2032 = \u03b4l/(4RlN ) and applying an",
    "where Y (k) \u2208 G0 similar to the proof of (cid:0)1 \u2212 \u03b7\u00b5 4 Lemma 9. This ensures that Y (k+1) \u2208 G0. Now, unrolling the recursion, we have with probability at least 1 \u2212 \u03b4\u2032Rl, Y (Rl) = \u02c6K(l) \u2208 G0 and the following: C( \u02c6K(l)) \u2212 C(K\u2217) \u2264 (cid:16) 1 \u2212 (cid:17)Rl \u03b7\u00b5 4 (C( \u02c6K(l\u22121)) \u2212 C(K\u2217)) + (cid:32) c(p,9)D (cid:112)Ml|M| (cid:19)(cid:33) (cid:115) log (cid:18) 2D \u03b4\u2032 . (34) Setting \u03b4\u2032 = \u03b4l/(4RlN ) and applying an union bound over all agents, we have the above guarantee for all agents with probability at least 1 \u2212 \u03b4l/4. Let us denote this event by \u02dcEl. Defining El = \u02dcEl \u2229 El\u22121, we have the following: P(El|EThm1) = P( \u02dcEl|El\u22121, EThm1)P(El\u22121|EThm1) \u2265 (1 \u2212 \u03b4l/4) \uf8ed1 \u2212 \uf8eb l\u22121 (cid:88) j=1 \u03b4j 4 \uf8f6 \uf8eb \uf8f8 \u2265 \uf8ed1 \u2212 \uf8f6 \uf8f8 . l (cid:88) j=1 \u03b4j 4 This completes the induction argument. Hence, we have established that \u02c6K(l\u22121) i (cid:16) (34) holds for all agents i \u2208 [N ] in every epoch l \u2265 \u02dcL with probability at least \u2208 G0 1 \u2212 (cid:80)l \u03c3(i) and that (cid:17) \u03b4j 4 j=1 . Next, we analyze the final convergence guarantee in the last epoch \u00afL. Conditioned on the event E \u00afL\u22121 \u2229 EThm1, we obtain (34) as shown in the following with probability at least 1 \u2212 \u03b4 \u00afL/4 for all agents i \u2208 [N ]: C\u03c3(i)( \u02c6K( \u00afL) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 1 \u2212 (cid:17)R \u00afL \u03b7\u00b5 4 (cid:16) (cid:124) ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i))) (cid:125) (C\u03c3(i)( \u02c6K( \u00afL\u22121) i (cid:123)(cid:122) s1 (cid:18) 8DN R \u00afL \u03b4 \u00afL log (cid:115) (cid:123)(cid:122) s2 (cid:19)(cid:33) . (cid:125) + (cid:32) (cid:124) c(p,9)D (cid:112)M \u00afL|M| In the above, the settings of R \u00afL and M \u00afL from Table 2 ensures the following: 4 \u03b7\u00b5 log , note that the term (cid:18) c(p,4)N 10 \u02dc\u22060 \u22062 \u00afL (cid:19) from R \u00afL \u2265 (cid:18) s1 \u2264 exp \u2212 (cid:19) \u03b7\u00b5R \u00afL 4 (cid:18) S0 \u2264 exp \u2212 (cid:19) \u03b7\u00b5R \u00afL 4 10 \u02dc\u22060 \u2264 \u22062 \u00afL c(p,4)N . Using M \u00afL = M \u00afL = c(p,4) \u22062 \u00afL log c(p,4) log \u22062 \u00afL (cid:16) 8DN R \u00afL \u03b4 \u00afL (cid:17) (cid:16) 8DN R \u00afL \u03b4 \u00afL (cid:17) \u2265 log in the above, we have s1 \u2264 (cid:16) 8DN R \u00afL \u03b4 \u00afL , we have (cid:17) log (cid:19) (cid:18) 8DN R \u00afL \u03b4 \u00afL M \u00afLN . Furthermore, since (cid:114) (cid:16) 8DN R \u00afL log \u03b4 \u00afL (cid:112)M \u00afLN (cid:17) \u2264 (cid:114) (cid:16) 8DN R \u00afL log \u03b4 \u00afL (cid:112)M \u00afLN (cid:17) \u2264 (cid:114) (cid:16) 8DN R \u00afL log \u03b4 \u00afL (cid:112)M \u00afL|M| (cid:17) . s1 \u2264 27 KANAKERI BAJAJ VERMA GUPTA MITRA Together with the term s2 we obtain the following with probability at least 1 \u2212 \u03b4 \u00afL/4 for all agents i \u2208 [N ]: C\u03c3(i)( \u02c6K( \u00afL) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 2 max{1, c(p,9)D} (cid:113) M \u00afL|M\u03c3(i)| (cid:115) log (cid:18) 8DN R \u00afL \u03b4 (cid:19) . (35) The above",
    "R \u00afL log \u03b4 \u00afL (cid:112)M \u00afLN (cid:17) \u2264 (cid:114) (cid:16) 8DN R \u00afL log \u03b4 \u00afL (cid:112)M \u00afL|M| (cid:17) . s1 \u2264 27 KANAKERI BAJAJ VERMA GUPTA MITRA Together with the term s2 we obtain the following with probability at least 1 \u2212 \u03b4 \u00afL/4 for all agents i \u2208 [N ]: C\u03c3(i)( \u02c6K( \u00afL) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 2 max{1, c(p,9)D} (cid:113) M \u00afL|M\u03c3(i)| (cid:115) log (cid:18) 8DN R \u00afL \u03b4 (cid:19) . (35) The above holds on the event E \u00afL conditioned on the event EThm1. We have P(E \u00afL|EThm1) \u2265 (cid:16) 1 \u2212 (cid:80) \u00afL j=1 \u03b4j 4 (cid:17) (cid:16) = 1 \u2212 (cid:80) \u00afL j=1 \u03b4 8j2 (cid:17) \u2265 1 \u2212 \u03b4/4. Therefore, P(EE \u00afL \u2229 EThm1) = P(EE \u00afL|EThm1)P(EThm1) \u2265 (1 \u2212 \u03b4/4)(1 \u2212 \u03b4/2) \u2265 1 \u2212 (\u03b4/4 + \u03b4/2) \u2265 1 \u2212 \u03b4. Note that the guarantee in (35) provides a rate \u02dcO . It remains to show that M \u00afL = \u02dc\u2126(T ). Since \u2206l = \u22060/4l and Rl \u2265 1 for all l \u2208 {1, 2, . . . , \u00afL}, consider the following as Ml \u2264 T M \u00afL|M\u03c3(i)| 1/ (cid:16) (cid:113) (cid:17) Ml \u2264 T =\u21d2 4l \u2264 (cid:1) T \u22062 0 c(p,10) log (cid:0) 8DN \u03b4 T \u22062 0 c(p,10) log (cid:0) 8DN (cid:32) \u03b4 (cid:33) (cid:1) =\u21d2 l \u2264 log = log(c(p,11)T ), (36) where we defined c(p,11) := follows: (cid:18) \u22062 0 c(p,10) log( 8DN \u03b4 ) (cid:19) . Now, we use the upper bound on l to bound Rl as Rl = 4 \u03b7\u00b5 (cid:32) (a) \u2264 l (cid:32) (cid:32) log (cid:32) 4 \u03b7\u00b5 log (cid:33) c(p,10)N \u02dc\u22062 0 \u22062 0 c(p,10)N \u02dc\u22062 0 \u22062 0 (cid:32) (cid:33) + l log(4) (cid:33) (cid:33)(cid:33) + log(4) (b) \u2264 c(p,12) log(c(p,11)T ), 28 HARNESSING DATA FROM CLUSTERED LQR SYSTEMS where (a) follows as l \u2265 1, and (b) follows from (36) with c(p,12) := 4 \u03b7\u00b5 Therefore, the overall sample complexity has the following bound: (cid:18) log (cid:18) c(p,10)N \u02dc\u22062 \u22062 0 0 (cid:19) + log(4) (cid:19) . \u00afL (cid:88) T = (2MlRl + Ml) l=1 \u00afL (cid:88) (3MlRl) (a) \u2264 (b) \u2264 l=1 \u00afL (cid:88) l=1 3c(p,12) log(c(p,11)T ) = 4c(p,12) log(c(p,11)T ) (cid:33) (cid:1) 4l (cid:32) \u03b4 c(p,10) log (cid:0) 8DN T \u22062 0 c(p,10) log (cid:0) 8DN T \u22062 0 (cid:33) (cid:1) \u03b4 (cid:32) \u00afL 4 =\u21d2 T 4c(p,12) log(c(p,11)T ) (cid:18) c(p,10) log( 8DN T \u22062 0 \u03b4 ) (cid:19) \u2264 4 \u00afL. \uf8fc \uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd \uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe (37) In the above, (a) follows as Rl \u2265 1 and (b) as we used Rl \u2264 T in Ml. Using the lower bound on 4 \u00afL as obtained above in M \u00afL, we obtain M \u00afL \u2265 (35), we have the following with probability 1 \u2212 \u03b4: T log (cid:16) 8DN R \u00afL \u03b4 (cid:17) 4c(p,12) log(c(p,11)T ) log( 8DN T \u03b4 . Using this bound in ) C\u03c3(i)( \u02c6K( \u00afL) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 = 4D max{1, c(p,9)} (cid:113) c(p,12)",
    "(a) follows as Rl \u2265 1 and (b) as we used Rl \u2264 T in Ml. Using the lower bound on 4 \u00afL as obtained above in M \u00afL, we obtain M \u00afL \u2265 (35), we have the following with probability 1 \u2212 \u03b4: T log (cid:16) 8DN R \u00afL \u03b4 (cid:17) 4c(p,12) log(c(p,11)T ) log( 8DN T \u03b4 . Using this bound in ) C\u03c3(i)( \u02c6K( \u00afL) i ) \u2212 C\u03c3(i)(K\u2217 \u03c3(i)) \u2264 = 4D max{1, c(p,9)} (cid:113) c(p,12) log(c(p,11)T ) log (cid:0) 8DN T (cid:113) \u03b4 T |M\u03c3(i)| Dc(p,13) (cid:113) (cid:113) log (cid:0) 8DN T \u03b4 (cid:1) , T |M\u03c3(i)| (cid:1) (38) where we defined c(p,13) := 4 max{1, c(p,9)} (cid:113) c(p,12) log(c(p,11)T ). Finally, it remains to show that \u00afL > L when T \u2265 \u02dcO(1/\u22062). To ensure, \u00afL > L, consider the number of rollouts required up to (L + 1)th epoch. From (37) with the summation from 1 to L + 1 we have: L+1 (cid:88) (2MlRl + Ml) \u2264 4c(p,12) log(c(p,11)T ) l=1 (cid:32) c(p,10) log (cid:0) 8DN T \u22062 0 \u03b4 (cid:33) (cid:1) 4L+1. In the above, setting T \u2265 RHS to ensure \u00afL > L, we have T \u2265 4c(p,12) log(c(p,11)T ) (cid:32) (a) \u2265 4c(p,12) log(c(p,11)T ) \u03b4 c(p,10) log (cid:0) 8DN T \u22062 0 c(p,10) log (cid:0) 8DN T \u22062 0 \u03b4 (cid:32) (cid:1) (cid:33) 4L+1 (cid:1) (cid:33) (cid:18) \u22060 \u2206 (cid:19)2 . 29 KANAKERI BAJAJ VERMA GUPTA MITRA In the above, since L = min{l \u2208 1, 2, . . . : \u2206l \u2264 \u2206/2}, (a) follows from the fact that \u2206L+1 = \u22060/(2L+1) \u2264 \u2206/2. Hence, when T \u2265 \u02dcO(1/\u22062), we have \u00afL > L. This completes the proof of Theorem 2. Appendix D. Proof of Corollary 3 In this section, we analyze the total communication complexity of the PCPO algorithm. In every epoch, each agent communicates with the server once in every iteration of the collaborative policy optimization subroutine, and once to send the local policy to update the neighborhood sets. Hence, the overall communication complexity is (cid:80) \u00afL l=1(Rl + 1) \u2264 (R \u00afL + 1) \u00afL since R \u00afL = c(p,2) log \u2265 c(p,2) log (cid:19) (cid:18) 2 \u00afLc(p,3)N \u22062 0 l=1(Rl + 1). Note that (cid:80) \u00afL (cid:17) (cid:16) 2lc(p,3)N \u22062 0 = Rl. From (37), \u00afL is logarithmic in T . Furthermore, R \u00afL = c(p,2) log (cid:18) 2 \u00afLc(p,3)N \u22062 0 (cid:19) is logarithmic in the number of agents N and T . Finally, since T \u2265 \u02dcO(1/\u22062), the overall communication complexity is logarithmic in T , N and 1/\u2206. 30"
  ]
}